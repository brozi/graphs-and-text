{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING IN GRAPH & TEXT DATA\n",
    "\n",
    "# LINK PREDICTION IN GRAPH\n",
    "\n",
    "\n",
    "## S. Calcagno, I. Koval, B. Rozi√®re\n",
    "\n",
    "This notebook is part of the report of the Machine Learning in graph and text data. The goal was to predict citations between articles based on their title, abstract, authors and year of publication.\n",
    "\n",
    "The notebook is structured as follow:\n",
    "- I. DATA IMPORTATION\n",
    "- II. FEATURE ENGINEERING\n",
    "- III. CLASSIFIER\n",
    "- IV. ASSEMBLING\n",
    "- V. SUBMISSION\n",
    "\n",
    "\n",
    "### Comments\n",
    "The code was fully optimized so that it takes less than 45 minutes to create all the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. DATA IMPORTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "import csv\n",
    "import pandas as pd\n",
    "from nltk.stem import SnowballStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sknn.mlp import Layer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sknn.mlp import Classifier as MLP\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "info = pd.read_csv(\n",
    "    \"node_information.csv\", \n",
    "    header= None, \n",
    "    names=[\"Id\", \"year\", \"title\", \"authors\", \"journal\", \"abstract\"],\n",
    "    sep=\",\",\n",
    "    index_col = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"training_set.txt\", sep=\" \", header=None)\n",
    "X_test = pd.read_csv(\"testing_set.txt\", sep=\" \", header=None)\n",
    "y_train = X_train[2]\n",
    "X_train.drop([2], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. FEATURE ENGINEERING\n",
    "\n",
    "This part includes :\n",
    "- Feature Preprocess\n",
    "- Topologic features (graph of the articles)\n",
    "- Semantic features\n",
    "- Attributes features \n",
    "- Topologic features (graph of the authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II.1 Feature Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- list_authors is the list of authors in the papers. They are presented as follow : \"Michalis Vazirgiannis, Xavier Le Pennec et al\" becomes [\"m. vazirgiannis\", \"x. pennec\"]\n",
    "- list_universities is the list where the authors are from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from authors_and_universities import authors_and_universities\n",
    "\n",
    "list_authors, list_universities = authors_and_universities(info)\n",
    "info['authors'] = list_authors\n",
    "info['universities'] = list_universities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. 2. Topologic features\n",
    "\n",
    "Creates a graph of the articles, then compute the following features : \n",
    "- Betweeness centrality\n",
    "- Number of common neighbours\n",
    "- Jaccard Coefficient\n",
    "- Difference in inlinks coefficient\n",
    "- Number of times \"to\" cites\n",
    "- Same cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from create_topologic_features import make_graph, create_topologic_features\n",
    "\n",
    "G = make_graph(X_train, y_train, X_test)  \n",
    "X_train = create_topologic_features(X_train, G)\n",
    "X_test = create_topologic_features(X_test, G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# II.3. NLP features\n",
    "\n",
    "\n",
    "For NLP features, we will:\n",
    "### 1. Train a word 2 vec model\n",
    "We concatenate the abstract and title texts as sentence inputs for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/bat/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "CPU times: user 11min 32s, sys: 16 s, total: 11min 48s\n",
      "Wall time: 9min 12s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bat/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "from semantic_features import *\n",
    "\n",
    "\n",
    "sentences = []  # Initialize an empty list of sentences\n",
    "word_list_abstract=pd.Series(index=info.index,dtype=np.str)\n",
    "word_list_title=pd.Series(index=info.index,dtype=np.str)\n",
    "\n",
    "for idx in info.index:\n",
    "    w_list_a = text_to_sentences(info.loc[idx,\"abstract\"], tokenizer, True)\n",
    "    sentences += w_list_a\n",
    "    word_list_abstract[idx]= w_list_a\n",
    "    w_list_t = text_to_sentences(info.loc[idx,\"title\"], tokenizer, True)\n",
    "    sentences += w_list_t\n",
    "    word_list_title[idx]= w_list_t\n",
    "\n",
    "num_features = 200\n",
    "model = load_model(\n",
    "    sentences, \n",
    "    train = False, \n",
    "    num_features = num_features, \n",
    "    min_word_count = 10, \n",
    "    num_workers = 4, \n",
    "    context = 10, \n",
    "    downsampling = 1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this model to transform the abstract and title into a list of word vectors and compute the centroid of those lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centroid_title = dict(zip(info.index, getAvgFeatureVecs(info.title.values, model, num_features)))\n",
    "centroid_abstract = dict(zip(info.index, getAvgFeatureVecs(info.abstract.values, model, num_features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also use it to make word clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an\n",
    "# average of 5 words per cluster\n",
    "word_vectors = model.syn0\n",
    "num_clusters = word_vectors.shape[0] / 5\n",
    "\n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "kmeans_clustering = KMeans( n_clusters = num_clusters )\n",
    "idx = kmeans_clustering.fit_predict( word_vectors )\n",
    "\n",
    "# Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "# a cluster number                                                                                            \n",
    "word_centroid_map = dict(zip( model.index2word, idx ))\n",
    "\n",
    "# Pre-allocate an array for the training set bags of centroids (for speed)\n",
    "bag_centroids_abstract = {}\n",
    "#centroids_title = {}\n",
    "# Transform the training set reviews into bags of centroids\n",
    "for review, idx in zip(word_list_abstract, word_list_abstract.index):\n",
    "    bag_centroids_abstract[idx] = create_bag_of_centroids(review[0], word_centroid_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also compute a list of words from title and abstract to compute the Jaccard similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_l_title = info.title.apply(lambda x: text_to_wordlist(x, True))\n",
    "w_l_abstract = info.abstract.apply(lambda x: text_to_wordlist(x, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call a function that computes all these variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 53s, sys: 2.28 s, total: 15min 56s\n",
      "Wall time: 15min 59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bat/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:31: FutureWarning: scalar indexers for index type Int64Index should be integers and not floating point\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = create_nlp_features(\n",
    "    X_train, centroid_title, centroid_abstract, bag_centroids_abstract, w_l_title, w_l_abstract)\n",
    "X_test = create_nlp_features(\n",
    "    X_test,centroid_title, centroid_abstract, bag_centroids_abstract, w_l_title, w_l_abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# II.4. Attribute features\n",
    "\n",
    "- Difference in publication year\n",
    "- Number of common authors\n",
    "- Self-citation\n",
    "- Same journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from create_attribute_features import create_attribute_features\n",
    "X_train = create_attribute_features(X_train,info)\n",
    "X_test = create_attribute_features(X_test,info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II.5. Author Graph features\n",
    "\n",
    "- TBC: AUTHORS NORMALIZED NUMBER SAME CLUSTER\n",
    "- TBC: AUTHORS SLUTER JACCARD\n",
    "- Authors betweeness centrality\n",
    "- Authors common neighbors\n",
    "- Authors Jaccard Coefficient\n",
    "- Authors max difference in inlinks\n",
    "- Authors sum difference in inlins\n",
    "- Authors max of times to cited\n",
    "- Authors sum of times to cited\n",
    "- Authors median of times to cited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating cluster features\n",
      "cluster features created\n",
      "creating cluster features\n",
      "cluster features created\n",
      "Wall time: 7min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from author_graph import make_graph_authors, create_topologic_features_authors\n",
    "\n",
    "G_authors = make_graph_authors(X_train, y_train, info)\n",
    "X_train = create_topologic_features_authors(X_train, G_authors, info, betweeness = True, common_neigh_and_jacc = True, inlinks = True)\n",
    "X_test = create_topologic_features_authors(X_test, G_authors, info,  betweeness = True, common_neigh_and_jacc = True, inlinks = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. CLASSIFIERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_light = X_train.drop([0,1,\"Number of times to cited\",\"Authors  of times to cited\"], axis = 1)\n",
    "X_test_light = X_test.drop([0,1,\"Number of times to cited\",\"Authors  of times to cited\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_light2 = X_train.drop([0,1,\"Number common neighbours\",\"Self citation\",\"Same journal\",\"Authors max difference in inlinks\",\n",
    "                                   \"Number of times to cited\",\"Authors  of times to cited\"], axis = 1)\n",
    "X_test_light2 = X_test.drop([0,1,\"Number common neighbours\",\"Self citation\",\"Same journal\",\"Authors max difference in inlinks\",\n",
    "                             \"Number of times to cited\",\"Authors  of times to cited\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III.1. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "rfc = RandomForestClassifier(n_estimators = 400,n_jobs=3)\n",
    "rfc.fit(X_train_light2, y_train)\n",
    "pred_rfc = rfc.predict(X_test_light2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "X_columns = X_train_light2.columns\n",
    "ordering = np.argsort(rfc.feature_importances_)[::-1]\n",
    "\n",
    "importances = rfc.feature_importances_[ordering]\n",
    "feature_names = X_columns[ordering]\n",
    "\n",
    "x = np.arange(len(feature_names))\n",
    "plt.bar(x, importances)\n",
    "plt.xticks(x + 0.5, feature_names, rotation=90, fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III.2. Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.976028087186\n"
     ]
    }
   ],
   "source": [
    "scaler1=StandardScaler()\n",
    "scaler = MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "nn = MLP(\n",
    "layers=[\n",
    "    Layer(\"Rectifier\", units=100),\n",
    "    Layer(\"Rectifier\", units=100),\n",
    "    Layer(\"Rectifier\", units=100),\n",
    "           Layer(\"Softmax\")],\n",
    "        learning_rule = 'momentum', learning_rate=0.05, batch_size = 100,dropout_rate =0.1,\n",
    "        n_iter=100,\n",
    "        verbose = 1, \n",
    "        valid_size = 0.1, \n",
    "        n_stable = 25,\n",
    "        debug = True,\n",
    "        random_state = 42\n",
    "        #    regularize = 'L2'\n",
    ")\n",
    "\n",
    "clfnn = Pipeline([\n",
    "    (\"scaler\", scaler1),\n",
    "    ('neural network', nn)\n",
    "    ])\n",
    "\n",
    "clfnn.fit(X_train.values, y_train.values)\n",
    "pred = clfnn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.974494404658\n",
      "CPU times: user 32min 36s, sys: 43.3 s, total: 33min 19s\n",
      "Wall time: 17min 29s\n"
     ]
    }
   ],
   "source": [
    "scaler1_light=StandardScaler()\n",
    "scaler = MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "nn_light = MLP(\n",
    "layers=[\n",
    "    Layer(\"Rectifier\", units=50),\n",
    "    Layer(\"Rectifier\", units=50),\n",
    "    Layer(\"Rectifier\", units=50),\n",
    "    Layer(\"Rectifier\", units=40),\n",
    "    Layer(\"Rectifier\", units=40),\n",
    "           Layer(\"Softmax\")],\n",
    "        learning_rule = 'momentum', learning_rate=0.05, batch_size = 100,dropout_rate =0.1,\n",
    "        n_iter=150,\n",
    "        verbose = 1, \n",
    "        valid_size = 0.1, \n",
    "        n_stable = 25,\n",
    "        debug = False,\n",
    "        random_state = 43\n",
    "        #    regularize = 'L2'\n",
    ")\n",
    "\n",
    "clfnn_light = Pipeline([\n",
    "    (\"scaler\", scaler1_light),\n",
    "    ('neural network', nn_light)\n",
    "    ])\n",
    "\n",
    "clfnn_light.fit(X_train_light.values, y_train.values)\n",
    "pred = clfnn_light.predict(X_test_light)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "scaler1=StandardScaler()\n",
    "scaler = MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "nn2 = MLP(\n",
    "layers=[\n",
    "    Layer(\"Rectifier\", units=100),\n",
    "    Layer(\"Rectifier\", units=100),\n",
    "           Layer(\"Softmax\")],\n",
    "        learning_rule = 'momentum', learning_rate=0.05, batch_size = 100,dropout_rate =0.1,\n",
    "        n_iter=100,\n",
    "        verbose = 1, \n",
    "        valid_size = 0.1, \n",
    "        n_stable = 25,\n",
    "        debug = True,\n",
    "        #    regularize = 'L2'\n",
    ")\n",
    "\n",
    "clfnn2 = Pipeline([\n",
    "    (\"scaler\", scaler1),\n",
    "    ('neural network', nn2)\n",
    "    ])\n",
    "\n",
    "clfnn2.fit(X_train_light.values, y_train.values)\n",
    "pred = clfnn2.predict(X_test_light)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "scaler1=StandardScaler()\n",
    "scaler = MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "nn = MLP(\n",
    "layers=[\n",
    "    Layer(\"Rectifier\", units=100),\n",
    "    Layer(\"Rectifier\", units=100),\n",
    "    Layer(\"Rectifier\", units=100),\n",
    "           Layer(\"Softmax\")],\n",
    "        learning_rule = 'momentum', learning_rate=0.05, batch_size = 100,dropout_rate =0.1,\n",
    "        n_iter=100,\n",
    "        verbose = 1, \n",
    "        valid_size = 0.1, \n",
    "        n_stable = 25,\n",
    "        debug = True,\n",
    "        #    regularize = 'L2'\n",
    ")\n",
    "\n",
    "clfnn = Pipeline([\n",
    "    (\"scaler\", scaler1),\n",
    "    ('neural network', nn)\n",
    "    ])\n",
    "\n",
    "clfnn.fit(X_train.values, y_train.values)\n",
    "pred = clfnn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "scaler1=StandardScaler()\n",
    "scaler = MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "nn = MLP(\n",
    "layers=[\n",
    "    Layer(\"Rectifier\", units=100),\n",
    "    Layer(\"Rectifier\", units=100),\n",
    "    Layer(\"Rectifier\", units=100),\n",
    "           Layer(\"Softmax\")],\n",
    "        learning_rule = 'momentum', learning_rate=0.05, batch_size = 100,dropout_rate =0.1,\n",
    "        n_iter=100,\n",
    "        verbose = 1, \n",
    "        valid_size = 0.1, \n",
    "        n_stable = 25,\n",
    "        debug = True,\n",
    "        #    regularize = 'L2'\n",
    ")\n",
    "\n",
    "clfnn = Pipeline([\n",
    "    (\"scaler\", scaler1),\n",
    "    ('neural network', nn)\n",
    "    ])\n",
    "\n",
    "clfnn.fit(X_train.values, y_train.values)\n",
    "pred = clfnn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III.3. Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier(n_estimators = 400, max_depth = 5)\n",
    "gbc.fit(X_train_light, y_train)\n",
    "pred_gbc1 = gbc.predict(X_test_light)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier(n_estimators = 400, max_depth = 7)\n",
    "gbc.fit(X_train_light, y_train)\n",
    "pred_gbc2 = gbc.predict(X_test_light)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III.4. Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "etc = ExtraTreesClassifier(n_estimators = 400,n_jobs=2)\n",
    "etc.fit(X_train.drop([0,1], axis = 1), y_train)\n",
    "pred_etc = etc.predict(X_test.drop([0,1], axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III.5. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf =  Pipeline([\n",
    "    (\"scaler\", MinMaxScaler()),\n",
    "    (\"regression\",LogisticRegression(C=200000,max_iter = 1000))\n",
    "    ])\n",
    "clf.fit(X_train.drop([0,1], axis = 1), y_train)\n",
    "pred_lr = clf.predict(X_test.drop([0,1], axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# IV. ASSEMBLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rounds(predict):\n",
    "    return [0 if pr<=0.5 else 1 for pr in predict]\n",
    "\n",
    "def make_pred(y_pred_array, weights = None):\n",
    "    if weights ==None:\n",
    "        weights = [1 for i in y_pred_array]\n",
    "    return np.array(rounds(1./sum(weights)*sum([weights[i]*y for i,y in enumerate(y_pred_array)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# V. SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_submission(predicted_label, name = 'submit.csv'):\n",
    "    submit_d = d = {'id' : pd.Series(np.arange(1,X_test.shape[0]+1).astype(int)),\n",
    "                    'category' : pd.Series(predicted_label).astype(int)}\n",
    "    submit = pd.DataFrame(submit_d)\n",
    "    submit.to_csv(name,index=False)\n",
    "    return submit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
