{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "import csv\n",
    "import pandas as pd\n",
    "from nltk.stem import SnowballStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import re\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "info = pd.read_csv(\n",
    "    \"node_information.csv\", \n",
    "    header= None, \n",
    "    names=[\"Id\", \"year\", \"title\", \"authors\", \"journal\", \"abstract\"],\n",
    "    sep=\",\",\n",
    "    index_col = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27770, 5) (615512, 2) (32648, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.read_csv(\"training_set.txt\", sep=\" \", header=None)\n",
    "X_test = pd.read_csv(\"testing_set.txt\", sep=\" \", header=None)\n",
    "y_train = X_train[2]\n",
    "X_train.drop([2], axis = 1, inplace = True)\n",
    "\n",
    "print info.shape, X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######################\n",
    "### FOR VALIDATION ###\n",
    "######################\n",
    "\n",
    "\n",
    "#####################\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.2)\n",
    "#####################\n",
    "\n",
    "#####################\n",
    "small_portion_to_train = 50000\n",
    "small_portion_to_test  = 5000\n",
    "#X_train = X_train[:small_portion_to_train]\n",
    "#y_train = y_train[:small_portion_to_train]\n",
    "\n",
    "#X_test  = X_test[:small_portion_to_test]\n",
    "#y_test = y_test[:small_portion_to_test]\n",
    "#####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- list_authors is the list of authors in the papers\n",
    "- list_universities is the list where the authors are from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def universities_to_keep(authors, universities):\n",
    "    while('(' in authors and ')' in authors):\n",
    "        universities.append( authors[authors.find('(')+1 : authors.find(')')] )\n",
    "        authors = authors[: authors.find('(')] + authors[ authors.find(')')+1 : ]\n",
    "            \n",
    "    if '(' in authors:\n",
    "        universities.append( authors[authors.find('(')+1 : ])\n",
    "        authors = authors[: authors.find('(')]\n",
    "    \n",
    "    return authors, universities\n",
    "\n",
    "\n",
    "def name_to_keep(author):\n",
    "    if len(author.split(' ')) <= 1:\n",
    "        return author\n",
    "    \n",
    "    while( author[0] == ' ' and len(author) > 0):\n",
    "        author = author[1:]\n",
    "    while( author[-1] == ' ' and len(author) > 0):\n",
    "        author = author[:-1]\n",
    "    \n",
    "    author = author.replace('.', '. ')\n",
    "    author = author.replace('.  ', '. ')\n",
    "    name_to_keep = author.split(' ')[0][0] + '. ' + author.split(' ')[-1]\n",
    "\n",
    "    return name_to_keep\n",
    "\n",
    "# Transform concatenated names of authors to a list of authors \n",
    "list_authors = []\n",
    "list_universities = []\n",
    "\n",
    "info['authors'] = info['authors'].replace(np.nan, 'missing')\n",
    "for authors in info['authors']:\n",
    "    if authors != 'missing':\n",
    "        ### split the different authors\n",
    "        authors = authors.lower()\n",
    "        \n",
    "        ### Find the universities included in the name\n",
    "        universities = []\n",
    "        authors, universities = universities_to_keep(authors, universities)\n",
    "        \n",
    "        ### Split the authors\n",
    "        authors = re.split(',|&', authors)\n",
    "        \n",
    "        ### For each author, check if university, and store it. Also, keep just the names (To be improved)\n",
    "        authors_in_article = []      \n",
    "        for author in authors:\n",
    "            if author != ' ':\n",
    "                authors_in_article.append(name_to_keep(author))\n",
    "            \n",
    "        list_universities.append(universities)\n",
    "        list_authors.append(authors_in_article)\n",
    "    else:\n",
    "        list_universities.append(['missing'])\n",
    "        list_authors.append(['missing'])   \n",
    "        \n",
    "info['authors'] = list_authors\n",
    "info['universities'] = list_universities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topologic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_graph(X_train, y_train, X_test):\n",
    "    X_train = pd.concat([X_train, y_train], axis = 1)\n",
    "    X_train = X_train.values\n",
    "    G = nx.DiGraph()\n",
    "    for i in range(X_train.shape[0]):\n",
    "        source = X_train[i,0]\n",
    "        target = X_train[i,1]\n",
    "        G.add_node(source)\n",
    "        G.add_node(target)\n",
    "        if X_train[i,-1] == 1:\n",
    "            G.add_edge(source,target)\n",
    "            \n",
    "    X_test = X_test.values\n",
    "    for i in range(X_test.shape[0]):\n",
    "        source = X_test[i,0]\n",
    "        target = X_test[i,1]\n",
    "        G.add_node(source)\n",
    "        G.add_node(target)\n",
    "        \n",
    "    return G  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G = make_graph(X_train, y_train, X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_topologic_features(X, G):\n",
    "    X_ = X.copy()\n",
    "    X = X.values\n",
    "    \n",
    "    X_['Betweeness centrality'] = compute_betweeness_array(X, G)\n",
    "    X_['Number common neighbours'] = make_common_neighbors(X, G)\n",
    "    X_['Jaccard coefficienf'] = make_jaccard(X, G)\n",
    "    diff_deg, to_deg = compute_diff_inlinks(X, G)\n",
    "    X_['Difference in inlinks coefficient'] = diff_deg\n",
    "    X_[\"Number of times to cited\"] = to_deg\n",
    "    X_['Same cluster'] = same_community(X,G)\n",
    "    return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from create_topologic_features import create_topologic_features\n",
    "X_train = create_topologic_features(X_train, G)\n",
    "X_test = create_topologic_features(X_test, G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Semantic features\n",
    "- Cosine similarity within the titles as tf-idf\n",
    "- Cosine similarity within the abstracts as tf-idf\n",
    "- Cosine similarity within the titles as word2vec\n",
    "- Cosine similarity within the abstracts as word2vec\n",
    "\n",
    "### To try\n",
    "- Difference cosine similarities?\n",
    "- Keep the stopwords or not?\n",
    "- Stemmise the words of not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Igor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from semantic_features import *\n",
    "\n",
    "\n",
    "sentences = []  # Initialize an empty list of sentences\n",
    "word_list_abstract=pd.Series(index=info.index,dtype=np.str)\n",
    "word_list_title=pd.Series(index=info.index,dtype=np.str)\n",
    "\n",
    "for idx in info.index:\n",
    "    w_list_a = text_to_sentences(info.loc[idx,\"abstract\"], tokenizer, True)\n",
    "    sentences += w_list_a\n",
    "    word_list_abstract[idx]= w_list_a\n",
    "    w_list_t = text_to_sentences(info.loc[idx,\"title\"], tokenizer, True)\n",
    "    sentences += w_list_t\n",
    "    word_list_title[idx]= w_list_t\n",
    "\n",
    "num_features = 200\n",
    "model = load_model(sentences, train = False, num_features = num_features, min_word_count = 10, num_workers = 4, context = 10, downsampling = 1e-3)\n",
    "\n",
    "\n",
    "centroid_title = dict(zip(info.index, getAvgFeatureVecs(info.title.values, model, num_features)))\n",
    "centroid_abstract = dict(zip(info.index, getAvgFeatureVecs(info.abstract.values, model, num_features)))\n",
    "\n",
    "\n",
    "# Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an\n",
    "# average of 5 words per cluster\n",
    "word_vectors = model.syn0\n",
    "num_clusters = word_vectors.shape[0] / 5\n",
    "\n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "kmeans_clustering = KMeans( n_clusters = num_clusters )\n",
    "idx = kmeans_clustering.fit_predict( word_vectors )\n",
    "\n",
    "# Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "# a cluster number                                                                                            \n",
    "word_centroid_map = dict(zip( model.index2word, idx ))\n",
    "\n",
    "# Pre-allocate an array for the training set bags of centroids (for speed)\n",
    "bag_centroids_abstract = {}\n",
    "#centroids_title = {}\n",
    "# Transform the training set reviews into bags of centroids\n",
    "for review, idx in zip(word_list_abstract, word_list_abstract.index):\n",
    "    bag_centroids_abstract[idx] = create_bag_of_centroids(review[0], word_centroid_map)\n",
    "#for review, idx in zip(word_list_title, word_list_title.index):\n",
    "#    centroids_title[idx] = create_bag_of_centroids( review[0], word_centroid_map )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_l_title = info.title.apply(lambda x: text_to_wordlist(x, True))\n",
    "w_l_abstract = info.abstract.apply(lambda x: text_to_wordlist(x, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "   \n",
    "def compute_cosines(centroids_dic, X):\n",
    "    bag = []\n",
    "    for idx, row in X.iterrows():\n",
    "        \n",
    "        condition = (np.isnan(centroids_dic[row[0]]).any() or np.isnan(centroids_dic[row[1]]).any())\n",
    "        if condition:\n",
    "            bag.append(0)\n",
    "        else:\n",
    "            bag.append(cosine(centroids_dic[row[0]], centroids_dic[row[1]]))\n",
    "\n",
    "    return np.array(bag)\n",
    "\n",
    "def create_nlp_features(X, centroid_title, centroid_abstract, bag_centroids_abstract, w_l_title, w_l_abstract):\n",
    "    X_ = X.copy()\n",
    "    \n",
    "    X_['CosineD_title_centroid'] = compute_cosines(centroid_title,X)\n",
    "    X_['CosineD_abstract_centroid'] = compute_cosines(centroid_abstract,X)\n",
    "    X_['CosineD_abstract_bag_centroids'] = compute_cosines(bag_centroids_abstract,X)\n",
    "    X_['Jaccard_title'] = compute_jaccard(w_l_title, X_)\n",
    "    X_['Jaccard_abstract'] = compute_jaccard(w_l_abstract, X_)\n",
    "    \n",
    "    return X_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = create_nlp_features(\n",
    "    X_train, centroid_title, centroid_abstract, bag_centroids_abstract, w_l_title, w_l_abstract)\n",
    "X_test = create_nlp_features(\n",
    "    X_test,centroid_title, centroid_abstract, bag_centroids_abstract, w_l_title, w_l_abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Attribute features\n",
    "\n",
    "- Difference in publication year\n",
    "- Number of common authors\n",
    "- Self-citation\n",
    "- Same journal\n",
    "- Number of times \"to\" cited (Attraction of the \"to\" paper)\n",
    "\n",
    "### To try\n",
    "- Number of times each author of \"to\" cited [Sum of these number of times] ?\n",
    "- Number of times each journal cited?\n",
    "- Number of same university??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from create_attribute_features import create_attribute_features\n",
    "X_train = create_attribute_features(X_train,info)\n",
    "X_test = create_attribute_features(X_test,info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author Graph features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from author_graph import make_graph_authors, create_topologic_features_authors\n",
    "\n",
    "G_authors = make_graph_authors(X_train, y_train, info)\n",
    "X_train = create_topologic_features_authors(X_train, G_authors, info, betweeness = True, common_neigh_and_jacc = True, inlinks = True)\n",
    "X_test = create_topologic_features_authors(X_test, G_authors, info,  betweeness = True, common_neigh_and_jacc = True, inlinks = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Impact factors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train=X_train.drop([0,1], axis = 1) \n",
    "X_test = X_test.drop([0,1], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = \"validation_set2/\"\n",
    "X_train.to_csv(path+\"X_train.csv\")\n",
    "X_test.to_csv(path+\"X_test.csv\")\n",
    "y_train.to_csv(path+\"y_train.csv\")\n",
    "y_test.to_csv(path+\"y_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(pred, real):\n",
    "    tot = 0\n",
    "    for i, val in enumerate(real):\n",
    "        if pred[i] == val:\n",
    "            tot += 1\n",
    "    return float(tot)/len(real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sknn.mlp import Layer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sknn.mlp import Classifier as MLP\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "scaler1=StandardScaler()\n",
    "scaler = MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "nn = MLP(\n",
    "layers=[\n",
    "Layer(\"Rectifier\", units=2000),\n",
    "        Layer(\"Softmax\")],\n",
    "        learning_rule = 'momentum', learning_rate=0.02, batch_size = 150,dropout_rate = 0.25,\n",
    "        n_iter=50,\n",
    "        verbose = 1, \n",
    "        valid_size = 0.1, \n",
    "        n_stable = 15,\n",
    "        debug = False,\n",
    "        #    regularize = 'L2'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = Pipeline([\n",
    "    (\"scaler\", scaler1),\n",
    "    ('neural network', nn)\n",
    "    ])\n",
    "\n",
    "clf.fit(X_train.drop([0,1], axis = 1), y_train)\n",
    "pred = clf.predict(X_test.drop([0,1], axis = 1))\n",
    "print score(pred, y_test)\n",
    "print score(clf.predict(X_train.drop([0,1], axis = 1)),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.957718333428\n",
      "1.0\n",
      "Wall time: 5min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators = 500,n_jobs=4)\n",
    "rfc.fit(X_train.drop([0,1], axis = 1), y_train)\n",
    "pred = rfc.predict(X_test.drop([0,1], axis = 1))\n",
    "print score(pred, y_test)\n",
    "print score(rfc.predict(X_train.drop([0,1], axis = 1)),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "etc = ExtraTreesClassifier(n_estimators = 400,n_jobs=2)\n",
    "etc.fit(X_train.drop([0,1], axis = 1), y_train)\n",
    "pred = etc.predict(X_test.drop([0,1], axis = 1))\n",
    "print score(pred, y_test)\n",
    "print score(etc.predict(X_train.drop([0,1], axis = 1)),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier(n_estimators = 200)\n",
    "gbc.fit(X_train.drop([0,1], axis = 1), y_train)\n",
    "pred = gbc.predict(X_test.drop([0,1], axis = 1))\n",
    "print score(pred, y_test)\n",
    "print score(gbc.predict(X_train.drop([0,1], axis = 1)),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf =  Pipeline([\n",
    "    (\"scaler\", MinMaxScaler()),\n",
    "    (\"regression\",LogisticRegression(C=200000,max_iter = 1000))\n",
    "    ])\n",
    "clf.fit(X_train.drop([0,1], axis = 1), y_train)\n",
    "pred = clf.predict(X_test.drop([0,1], axis = 1))\n",
    "print score(pred, y_test)\n",
    "print score(clf.predict(X_train.drop([0,1], axis = 1)),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_submission(predicted_label, name = 'submit.csv'):\n",
    "    submit_d = d = {'id' : pd.Series(np.arange(1,X_test.shape[0]+1).astype(int)),\n",
    "                    'category' : pd.Series(predicted_label).astype(int)}\n",
    "    submit = pd.DataFrame(submit_d)\n",
    "    submit.to_csv(name,index=False)\n",
    "    return submit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
