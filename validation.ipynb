{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "import csv\n",
    "import pandas as pd\n",
    "from nltk.stem import SnowballStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import re\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "info = pd.read_csv(\n",
    "    \"node_information.csv\", \n",
    "    header= None, \n",
    "    names=[\"Id\", \"year\", \"title\", \"authors\", \"journal\", \"abstract\"],\n",
    "    sep=\",\",\n",
    "    index_col = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27770, 5) (615512, 2) (32648, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.read_csv(\"training_set.txt\", sep=\" \", header=None)\n",
    "X_test = pd.read_csv(\"testing_set.txt\", sep=\" \", header=None)\n",
    "y_train = X_train[2]\n",
    "X_train.drop([2], axis = 1, inplace = True)\n",
    "\n",
    "print info.shape, X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######################\n",
    "### FOR VALIDATION ###\n",
    "######################\n",
    "\n",
    "\n",
    "#####################\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.2)\n",
    "#####################\n",
    "\n",
    "#####################\n",
    "small_portion_to_train = 50000\n",
    "small_portion_to_test  = 5000\n",
    "#X_train = X_train[:small_portion_to_train]\n",
    "#y_train = y_train[:small_portion_to_train]\n",
    "\n",
    "#X_test  = X_test[:small_portion_to_test]\n",
    "#y_test = y_test[:small_portion_to_test]\n",
    "#####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- list_authors is the list of authors in the papers\n",
    "- list_universities is the list where the authors are from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def universities_to_keep(authors, universities):\n",
    "    while('(' in authors and ')' in authors):\n",
    "        universities.append( authors[authors.find('(')+1 : authors.find(')')] )\n",
    "        authors = authors[: authors.find('(')] + authors[ authors.find(')')+1 : ]\n",
    "            \n",
    "    if '(' in authors:\n",
    "        universities.append( authors[authors.find('(')+1 : ])\n",
    "        authors = authors[: authors.find('(')]\n",
    "    \n",
    "    return authors, universities\n",
    "\n",
    "\n",
    "def name_to_keep(author):\n",
    "    if len(author.split(' ')) <= 1:\n",
    "        return author\n",
    "    \n",
    "    while( author[0] == ' ' and len(author) > 0):\n",
    "        author = author[1:]\n",
    "    while( author[-1] == ' ' and len(author) > 0):\n",
    "        author = author[:-1]\n",
    "    \n",
    "    author = author.replace('.', '. ')\n",
    "    author = author.replace('.  ', '. ')\n",
    "    name_to_keep = author.split(' ')[0][0] + '. ' + author.split(' ')[-1]\n",
    "\n",
    "    return name_to_keep\n",
    "\n",
    "# Transform concatenated names of authors to a list of authors \n",
    "list_authors = []\n",
    "list_universities = []\n",
    "\n",
    "info['authors'] = info['authors'].replace(np.nan, 'missing')\n",
    "for authors in info['authors']:\n",
    "    if authors != 'missing':\n",
    "        ### split the different authors\n",
    "        authors = authors.lower()\n",
    "        \n",
    "        ### Find the universities included in the name\n",
    "        universities = []\n",
    "        authors, universities = universities_to_keep(authors, universities)\n",
    "        \n",
    "        ### Split the authors\n",
    "        authors = re.split(',|&', authors)\n",
    "        \n",
    "        ### For each author, check if university, and store it. Also, keep just the names (To be improved)\n",
    "        authors_in_article = []      \n",
    "        for author in authors:\n",
    "            if author != ' ':\n",
    "                authors_in_article.append(name_to_keep(author))\n",
    "            \n",
    "        list_universities.append(universities)\n",
    "        list_authors.append(authors_in_article)\n",
    "    else:\n",
    "        list_universities.append(['missing'])\n",
    "        list_authors.append(['missing'])   \n",
    "        \n",
    "info['authors'] = list_authors\n",
    "info['universities'] = list_universities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topologic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_graph(X_train, y_train, X_test):\n",
    "    X_train = pd.concat([X_train, y_train], axis = 1)\n",
    "    X_train = X_train.values\n",
    "    G = nx.DiGraph()\n",
    "    for i in range(X_train.shape[0]):\n",
    "        source = X_train[i,0]\n",
    "        target = X_train[i,1]\n",
    "        G.add_node(source)\n",
    "        G.add_node(target)\n",
    "        if X_train[i,-1] == 1:\n",
    "            G.add_edge(source,target)\n",
    "            \n",
    "    X_test = X_test.values\n",
    "    for i in range(X_test.shape[0]):\n",
    "        source = X_test[i,0]\n",
    "        target = X_test[i,1]\n",
    "        G.add_node(source)\n",
    "        G.add_node(target)\n",
    "        \n",
    "    return G  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G = make_graph(X_train, y_train, X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_topologic_features(X, G):\n",
    "    X_ = X.copy()\n",
    "    X = X.values\n",
    "    \n",
    "    X_['Betweeness centrality'] = compute_betweeness_array(X, G)\n",
    "    X_['Number common neighbours'] = make_common_neighbors(X, G)\n",
    "    X_['Jaccard coefficienf'] = make_jaccard(X, G)\n",
    "    diff_deg, to_deg = compute_diff_inlinks(X, G)\n",
    "    X_['Difference in inlinks coefficient'] = diff_deg\n",
    "    X_[\"Number of times to cited\"] = to_deg\n",
    "    X_['Same cluster'] = same_community(X,G)\n",
    "    return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 41s, sys: 5.14 s, total: 2min 46s\n",
      "Wall time: 2min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from create_topologic_features import create_topologic_features\n",
    "X_train = create_topologic_features(X_train, G)\n",
    "X_test = create_topologic_features(X_test, G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Semantic features\n",
    "- Cosine similarity within the titles as tf-idf\n",
    "- Cosine similarity within the abstracts as tf-idf\n",
    "- Cosine similarity within the titles as word2vec\n",
    "- Cosine similarity within the abstracts as word2vec\n",
    "\n",
    "### To try\n",
    "- Difference cosine similarities?\n",
    "- Keep the stopwords or not?\n",
    "- Stemmise the words of not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import various modules for string cleaning\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def text_to_wordlist(text, remove_stopwords=False ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    text = BeautifulSoup(text).get_text()\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "    text = re.sub(\"[\\W]\",\" \", text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/Flukmacdesof/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download the punkt tokenizer for sentence splitting\n",
    "import nltk.data\n",
    "nltk.download(\"punkt\")   \n",
    "\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Define a function to split a review into parsed sentences\n",
    "def text_to_sentences(text, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a text into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(text.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append(text_to_wordlist(raw_sentence, remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "train = False\n",
    "sentences = []  # Initialize an empty list of sentences\n",
    "word_list_abstract=pd.Series(index=info.index,dtype=np.str)\n",
    "word_list_title=pd.Series(index=info.index,dtype=np.str)\n",
    "print \"Parsing sentences from training set\"\n",
    "for idx in info.index:\n",
    "    w_list_a = text_to_sentences(info.loc[idx,\"abstract\"], tokenizer, True)\n",
    "    sentences += w_list_a\n",
    "    word_list_abstract[idx]= w_list_a\n",
    "    w_list_t = text_to_sentences(info.loc[idx,\"title\"], tokenizer, True)\n",
    "    sentences += w_list_t\n",
    "    word_list_title[idx]= w_list_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim.models import Word2Vec\n",
    "num_features = 200    # Word vector dimensionality  \n",
    "if train:\n",
    "    # Import the built-in logging module and configure it so that Word2Vec \n",
    "    # creates nice output messages\n",
    "    import logging\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "    # Set values for various parameters\n",
    "                    \n",
    "    min_word_count = 10   # Minimum word count                        \n",
    "    num_workers = 4       # Number of threads to run in parallel\n",
    "    context = 10          # Context window size                                                                                    \n",
    "    downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "    # Initialize and train the model (this will take some time)\n",
    "\n",
    "    print \"Training model...\"\n",
    "    model = word2vec.Word2Vec(\n",
    "        sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling\n",
    "    )\n",
    "\n",
    "    # If you don't plan to train the model any further, calling \n",
    "    # init_sims will make the model much more memory-efficient.\n",
    "    model.init_sims(replace=True)\n",
    "\n",
    "    # It can be helpful to create a meaningful model name and \n",
    "    # save the model for later use. You can load it later using Word2Vec.load()\n",
    "    model_name = \"200features_10minwords_10context\"\n",
    "    model.save(model_name)\n",
    "else:\n",
    "    model = Word2Vec.load(\"200features_10minwords_10context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(list_of_words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in list_of_words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(text_list, model, num_features):\n",
    "    # Given a set of texts (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    textFeatureVecs = np.zeros((len(text_list),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for list_of_words in text_list:\n",
    "        # Print a status message every 1000th review\n",
    "        \"\"\"\n",
    "        if counter%1000. == 0.:\n",
    "            print \"Review %d of %d\" % (counter, len(text_list))\n",
    "        \"\"\"\n",
    "        # Call the function (defined above) that makes average feature vectors\n",
    "        textFeatureVecs[counter] = makeFeatureVec(list_of_words, model, num_features)\n",
    "\n",
    "        # Increment the counter\n",
    "        counter = counter + 1.\n",
    "    return textFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:44: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "centroid_title = dict(zip(info.index, getAvgFeatureVecs(info.title.values, model, num_features)))\n",
    "centroid_abstract = dict(zip(info.index, getAvgFeatureVecs(info.abstract.values, model, num_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for K Means clustering:  208.609315872 seconds.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "start = time.time() # Start time\n",
    "\n",
    "# Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an\n",
    "# average of 5 words per cluster\n",
    "word_vectors = model.syn0\n",
    "num_clusters = word_vectors.shape[0] / 5\n",
    "\n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "kmeans_clustering = KMeans( n_clusters = num_clusters )\n",
    "idx = kmeans_clustering.fit_predict( word_vectors )\n",
    "\n",
    "# Get the end time and print how long the process took\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print \"Time taken for K Means clustering: \", elapsed, \"seconds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "# a cluster number                                                                                            \n",
    "word_centroid_map = dict(zip( model.index2word, idx ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bag_of_centroids( wordlist, word_centroid_map ):\n",
    "    #\n",
    "    # The number of clusters is equal to the highest cluster index\n",
    "    # in the word / centroid map\n",
    "    num_centroids = max( word_centroid_map.values() ) + 1\n",
    "    #\n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
    "    #\n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count \n",
    "    # by one\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    #\n",
    "    # Return the \"bag of centroids\"\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-allocate an array for the training set bags of centroids (for speed)\n",
    "bag_centroids_abstract = {}\n",
    "#centroids_title = {}\n",
    "# Transform the training set reviews into bags of centroids\n",
    "for review, idx in zip(word_list_abstract, word_list_abstract.index):\n",
    "    bag_centroids_abstract[idx] = create_bag_of_centroids(review[0], word_centroid_map)\n",
    "#for review, idx in zip(word_list_title, word_list_title.index):\n",
    "#    centroids_title[idx] = create_bag_of_centroids( review[0], word_centroid_map )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "def compute_cosines(centroids_dic, X):\n",
    "    bag=[cosine(centroids_dic[x[0]], centroids_dic[x[1]]) for x in X]\n",
    "    return np.array(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_nlp_features(X, centroid_title, centroid_abstract, bag_centroids_abstract):\n",
    "    X_ = X.copy()\n",
    "    X = X.values\n",
    "    \n",
    "    X_['CosineD_title_centroid'] = compute_cosines(centroid_title,X)\n",
    "    X_['CosineD_abstract_centroid'] = compute_cosines(centroid_abstract,X)\n",
    "    X_['CosineD_abstract_bag_centroids'] = compute_cosines(bag_centroids_abstract,X)\n",
    "    \n",
    "    return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 20s, sys: 970 ms, total: 3min 21s\n",
      "Wall time: 3min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = create_nlp_features(X_train, centroid_title, centroid_abstract, bag_centroids_abstract)\n",
    "X_test = create_nlp_features(X_test,centroid_title, centroid_abstract, bag_centroids_abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Attribute features\n",
    "\n",
    "- Difference in publication year\n",
    "- Number of common authors\n",
    "- Self-citation\n",
    "- Same journal\n",
    "- Number of times \"to\" cited (Attraction of the \"to\" paper)\n",
    "\n",
    "### To try\n",
    "- Number of times each author of \"to\" cited [Sum of these number of times] ?\n",
    "- Number of times each journal cited?\n",
    "- Number of same university??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 31s, sys: 11.4 s, total: 7min 42s\n",
      "Wall time: 7min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from create_attribute_features import create_attribute_features\n",
    "X_train = create_attribute_features(X_train,info)\n",
    "X_test = create_attribute_features(X_test,info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author Graph features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 50s, sys: 30.6 s, total: 12min 20s\n",
      "Wall time: 12min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from author_graph import make_graph_authors, create_topologic_features_authors\n",
    "\n",
    "G_authors = make_graph_authors(X_train, y_train, info)\n",
    "X_train = create_topologic_features_authors(X_train, G_authors, info, betweeness = True, common_neigh_and_jacc = True, inlinks = True)\n",
    "X_test = create_topologic_features_authors(X_test, G_authors, info,  betweeness = True, common_neigh_and_jacc = True, inlinks = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>Betweeness centrality</th>\n",
       "      <th>Number common neighbours</th>\n",
       "      <th>Jaccard coefficienf</th>\n",
       "      <th>Difference in inlinks coefficient</th>\n",
       "      <th>Number of times to cited</th>\n",
       "      <th>Same cluster</th>\n",
       "      <th>CosineD_title_centroid</th>\n",
       "      <th>CosineD_abstract_centroid</th>\n",
       "      <th>...</th>\n",
       "      <th>Self citation</th>\n",
       "      <th>Same journal</th>\n",
       "      <th>Authors betweeness</th>\n",
       "      <th>Authors common neighbors</th>\n",
       "      <th>Authors jaccard</th>\n",
       "      <th>Authors max difference in inlinks</th>\n",
       "      <th>Authors sum difference in inlinks</th>\n",
       "      <th>Authors max of times to cited</th>\n",
       "      <th>Authors sum of times to cited</th>\n",
       "      <th>Authors  of times to cited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>407169</th>\n",
       "      <td>9709200</td>\n",
       "      <td>2062</td>\n",
       "      <td>-0.000216</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.017137</td>\n",
       "      <td>0.002474</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.039554</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>-179</td>\n",
       "      <td>-145</td>\n",
       "      <td>53</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88252</th>\n",
       "      <td>11033</td>\n",
       "      <td>5020</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005892</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.020927</td>\n",
       "      <td>431</td>\n",
       "      <td>0.068806</td>\n",
       "      <td>-4967</td>\n",
       "      <td>-4967</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518527</th>\n",
       "      <td>5239</td>\n",
       "      <td>5032</td>\n",
       "      <td>-0.000864</td>\n",
       "      <td>27</td>\n",
       "      <td>0.293478</td>\n",
       "      <td>16</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001229</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.054617</td>\n",
       "      <td>160</td>\n",
       "      <td>0.189798</td>\n",
       "      <td>331</td>\n",
       "      <td>876</td>\n",
       "      <td>1046</td>\n",
       "      <td>418</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290352</th>\n",
       "      <td>9711148</td>\n",
       "      <td>9304155</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014144</td>\n",
       "      <td>0.001625</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.861791</td>\n",
       "      <td>1619</td>\n",
       "      <td>0.257311</td>\n",
       "      <td>-3974</td>\n",
       "      <td>-3291</td>\n",
       "      <td>1882</td>\n",
       "      <td>1199</td>\n",
       "      <td>663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378559</th>\n",
       "      <td>9302101</td>\n",
       "      <td>9206071</td>\n",
       "      <td>-0.000360</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013147</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.044958</td>\n",
       "      <td>58</td>\n",
       "      <td>0.148718</td>\n",
       "      <td>-261</td>\n",
       "      <td>-286</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0        1  Betweeness centrality  Number common neighbours  \\\n",
       "407169  9709200     2062              -0.000216                         0   \n",
       "88252     11033     5020               0.002341                         0   \n",
       "518527     5239     5032              -0.000864                        27   \n",
       "290352  9711148  9304155               0.000216                         0   \n",
       "378559  9302101  9206071              -0.000360                         0   \n",
       "\n",
       "        Jaccard coefficienf  Difference in inlinks coefficient  \\\n",
       "407169             0.000000                                  3   \n",
       "88252              0.000000                                -56   \n",
       "518527             0.293478                                 16   \n",
       "290352             0.000000                                 12   \n",
       "378559             0.000000                                 15   \n",
       "\n",
       "        Number of times to cited  Same cluster  CosineD_title_centroid  \\\n",
       "407169                         6             0                0.017137   \n",
       "88252                          0             0                0.005892   \n",
       "518527                        49             1                0.001229   \n",
       "290352                        12             0                0.014144   \n",
       "378559                        15             1                0.013147   \n",
       "\n",
       "        CosineD_abstract_centroid             ...              Self citation  \\\n",
       "407169                   0.002474             ...                          0   \n",
       "88252                    0.000697             ...                          0   \n",
       "518527                   0.000882             ...                          0   \n",
       "290352                   0.001625             ...                          0   \n",
       "378559                   0.000798             ...                          1   \n",
       "\n",
       "        Same journal  Authors betweeness  Authors common neighbors  \\\n",
       "407169             0            0.039554                         1   \n",
       "88252              0            1.020927                       431   \n",
       "518527             1           -0.054617                       160   \n",
       "290352             0            0.861791                      1619   \n",
       "378559             0            0.044958                        58   \n",
       "\n",
       "        Authors jaccard  Authors max difference in inlinks  \\\n",
       "407169         0.002717                               -179   \n",
       "88252          0.068806                              -4967   \n",
       "518527         0.189798                                331   \n",
       "290352         0.257311                              -3974   \n",
       "378559         0.148718                               -261   \n",
       "\n",
       "        Authors sum difference in inlinks  Authors max of times to cited  \\\n",
       "407169                               -145                             53   \n",
       "88252                               -4967                            206   \n",
       "518527                                876                           1046   \n",
       "290352                              -3291                           1882   \n",
       "378559                               -286                             25   \n",
       "\n",
       "        Authors sum of times to cited  Authors  of times to cited  \n",
       "407169                             19                          19  \n",
       "88252                             206                         206  \n",
       "518527                            418                         211  \n",
       "290352                           1199                         663  \n",
       "378559                             25                          25  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>Betweeness centrality</th>\n",
       "      <th>Number common neighbours</th>\n",
       "      <th>Jaccard coefficienf</th>\n",
       "      <th>Difference in inlinks coefficient</th>\n",
       "      <th>Number of times to cited</th>\n",
       "      <th>Same cluster</th>\n",
       "      <th>CosineD_title_centroid</th>\n",
       "      <th>CosineD_abstract_centroid</th>\n",
       "      <th>...</th>\n",
       "      <th>Self citation</th>\n",
       "      <th>Same journal</th>\n",
       "      <th>Authors betweeness</th>\n",
       "      <th>Authors common neighbors</th>\n",
       "      <th>Authors jaccard</th>\n",
       "      <th>Authors max difference in inlinks</th>\n",
       "      <th>Authors sum difference in inlinks</th>\n",
       "      <th>Authors max of times to cited</th>\n",
       "      <th>Authors sum of times to cited</th>\n",
       "      <th>Authors  of times to cited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>260427</th>\n",
       "      <td>9709122</td>\n",
       "      <td>9610043</td>\n",
       "      <td>-0.031114</td>\n",
       "      <td>24</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>882</td>\n",
       "      <td>899</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016793</td>\n",
       "      <td>0.001427</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.062780</td>\n",
       "      <td>724</td>\n",
       "      <td>0.338951</td>\n",
       "      <td>675</td>\n",
       "      <td>3243</td>\n",
       "      <td>4336</td>\n",
       "      <td>1511</td>\n",
       "      <td>993.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453256</th>\n",
       "      <td>9801042</td>\n",
       "      <td>9212088</td>\n",
       "      <td>-0.000432</td>\n",
       "      <td>19</td>\n",
       "      <td>0.322034</td>\n",
       "      <td>27</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005844</td>\n",
       "      <td>0.001221</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.068299</td>\n",
       "      <td>78</td>\n",
       "      <td>0.012470</td>\n",
       "      <td>-5114</td>\n",
       "      <td>-5083</td>\n",
       "      <td>90</td>\n",
       "      <td>59</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82909</th>\n",
       "      <td>9809073</td>\n",
       "      <td>304026</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>1</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003912</td>\n",
       "      <td>0.001189</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.007014</td>\n",
       "      <td>2</td>\n",
       "      <td>0.010989</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>70</td>\n",
       "      <td>65</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600257</th>\n",
       "      <td>205174</td>\n",
       "      <td>9701025</td>\n",
       "      <td>-0.004249</td>\n",
       "      <td>9</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>129</td>\n",
       "      <td>153</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003792</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.970565</td>\n",
       "      <td>752</td>\n",
       "      <td>0.119936</td>\n",
       "      <td>4509</td>\n",
       "      <td>4509</td>\n",
       "      <td>5173</td>\n",
       "      <td>5173</td>\n",
       "      <td>5173.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593540</th>\n",
       "      <td>9805027</td>\n",
       "      <td>9611182</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004324</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.957342</td>\n",
       "      <td>826</td>\n",
       "      <td>0.131508</td>\n",
       "      <td>4788</td>\n",
       "      <td>4784</td>\n",
       "      <td>5173</td>\n",
       "      <td>5173</td>\n",
       "      <td>5173.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0        1  Betweeness centrality  Number common neighbours  \\\n",
       "260427  9709122  9610043              -0.031114                        24   \n",
       "453256  9801042  9212088              -0.000432                        19   \n",
       "82909   9809073   304026               0.000180                         1   \n",
       "600257   205174  9701025              -0.004249                         9   \n",
       "593540  9805027  9611182               0.000144                         0   \n",
       "\n",
       "        Jaccard coefficienf  Difference in inlinks coefficient  \\\n",
       "260427             0.025641                                882   \n",
       "453256             0.322034                                 27   \n",
       "82909              0.062500                                 -1   \n",
       "600257             0.047619                                129   \n",
       "593540             0.000000                                  0   \n",
       "\n",
       "        Number of times to cited  Same cluster  CosineD_title_centroid  \\\n",
       "260427                       899             1                0.016793   \n",
       "453256                        45             1                0.005844   \n",
       "82909                          0             1                0.003912   \n",
       "600257                       153             0                0.003792   \n",
       "593540                         1             0                0.004324   \n",
       "\n",
       "        CosineD_abstract_centroid             ...              Self citation  \\\n",
       "260427                   0.001427             ...                          0   \n",
       "453256                   0.001221             ...                          0   \n",
       "82909                    0.001189             ...                          0   \n",
       "600257                   0.000737             ...                          0   \n",
       "593540                   0.000499             ...                          0   \n",
       "\n",
       "        Same journal  Authors betweeness  Authors common neighbors  \\\n",
       "260427             0           -0.062780                       724   \n",
       "453256             0            1.068299                        78   \n",
       "82909              0           -0.007014                         2   \n",
       "600257             0           -0.970565                       752   \n",
       "593540             0           -0.957342                       826   \n",
       "\n",
       "        Authors jaccard  Authors max difference in inlinks  \\\n",
       "260427         0.338951                                675   \n",
       "453256         0.012470                              -5114   \n",
       "82909          0.010989                                 12   \n",
       "600257         0.119936                               4509   \n",
       "593540         0.131508                               4788   \n",
       "\n",
       "        Authors sum difference in inlinks  Authors max of times to cited  \\\n",
       "260427                               3243                           4336   \n",
       "453256                              -5083                             90   \n",
       "82909                                  17                             70   \n",
       "600257                               4509                           5173   \n",
       "593540                               4784                           5173   \n",
       "\n",
       "        Authors sum of times to cited  Authors  of times to cited  \n",
       "260427                           1511                       993.5  \n",
       "453256                             59                        45.0  \n",
       "82909                              65                        35.0  \n",
       "600257                           5173                      5173.0  \n",
       "593540                           5173                      5173.0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(pred, real):\n",
    "    tot = 0\n",
    "    for i, val in enumerate(real):\n",
    "        if pred[i] == val:\n",
    "            tot += 1\n",
    "    return float(tot)/len(real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(492409,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.958092004257\n",
      "0.999983753343\n",
      "CPU times: user 7min 52s, sys: 7.37 s, total: 7min 59s\n",
      "Wall time: 4min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators = 100,n_jobs=2)\n",
    "rfc.fit(X_train.drop([0,1], axis = 1), y_train)\n",
    "pred = rfc.predict(X_test.drop([0,1], axis = 1))\n",
    "print score(pred, y_test)\n",
    "print score(rfc.predict(X_train.drop([0,1], axis = 1)),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.961666246964\n",
      "1.0\n",
      "CPU times: user 3min 46s, sys: 8.76 s, total: 3min 55s\n",
      "Wall time: 2min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "etc = ExtraTreesClassifier(n_estimators = 100,n_jobs=2)\n",
    "etc.fit(X_train.drop([0,1], axis = 1), y_train)\n",
    "pred = etc.predict(X_test.drop([0,1], axis = 1))\n",
    "print score(pred, y_test)\n",
    "print score(etc.predict(X_train.drop([0,1], axis = 1)),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier(n_estimators = 200)\n",
    "gbc.fit(X_train.drop([0,1], axis = 1), y_train)\n",
    "pred = gbc.predict(X_test.drop([0,1], axis = 1))\n",
    "print score(pred, y_test)\n",
    "print score(gbc.predict(X_train.drop([0,1], axis = 1)),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = reg = Pipeline([\n",
    "    (\"scaler\", MinMaxScaler()),\n",
    "    (\"regression\",LogisticRegression(C=200000,max_iter = 1000))\n",
    "    ])\n",
    "clf.fit(X_train.drop([0,1], axis = 1), y_train)\n",
    "pred = clf.predict(X_test.drop([0,1], axis = 1))\n",
    "print score(pred, y_test)\n",
    "print score(clf.predict(X_train.drop([0,1], axis = 1)),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_submission(predicted_label, name = 'submit.csv'):\n",
    "    submit_d = d = {'id' : pd.Series(np.arange(1,X_test.shape[0]+1).astype(int)),\n",
    "                    'category' : pd.Series(predicted_label).astype(int)}\n",
    "    submit = pd.DataFrame(submit_d)\n",
    "    submit.to_csv(name,index=False)\n",
    "    return submit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
