{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "import csv\n",
    "import pandas as pd\n",
    "from nltk.stem import SnowballStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import re\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "info = pd.read_csv(\n",
    "    \"node_information.csv\", \n",
    "    header= None, \n",
    "    names=[\"Id\", \"year\", \"title\", \"authors\", \"journal\", \"abstract\"],\n",
    "    sep=\",\",\n",
    "    index_col = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27770, 5) (615512, 2) (32648, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.read_csv(\"training_set.txt\", sep=\" \", header=None)\n",
    "X_test = pd.read_csv(\"testing_set.txt\", sep=\" \", header=None)\n",
    "y_train = X_train[2]\n",
    "X_train.drop([2], axis = 1, inplace = True)\n",
    "\n",
    "print info.shape, X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######################\n",
    "### FOR VALIDATION ###\n",
    "######################\n",
    "\n",
    "\n",
    "#####################\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.2)\n",
    "#####################\n",
    "\n",
    "#####################\n",
    "small_portion_to_train = 50000\n",
    "small_portion_to_test  = 5000\n",
    "#X_train = X_train[:small_portion_to_train]\n",
    "#y_train = y_train[:small_portion_to_train]\n",
    "\n",
    "#X_test  = X_test[:small_portion_to_test]\n",
    "#y_test = y_test[:small_portion_to_test]\n",
    "#####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- list_authors is the list of authors in the papers\n",
    "- list_universities is the list where the authors are from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def universities_to_keep(authors, universities):\n",
    "    while('(' in authors and ')' in authors):\n",
    "        universities.append( authors[authors.find('(')+1 : authors.find(')')] )\n",
    "        authors = authors[: authors.find('(')] + authors[ authors.find(')')+1 : ]\n",
    "            \n",
    "    if '(' in authors:\n",
    "        universities.append( authors[authors.find('(')+1 : ])\n",
    "        authors = authors[: authors.find('(')]\n",
    "    \n",
    "    return authors, universities\n",
    "\n",
    "\n",
    "def name_to_keep(author):\n",
    "    if len(author.split(' ')) <= 1:\n",
    "        return author\n",
    "    \n",
    "    while( author[0] == ' ' and len(author) > 0):\n",
    "        author = author[1:]\n",
    "    while( author[-1] == ' ' and len(author) > 0):\n",
    "        author = author[:-1]\n",
    "    \n",
    "    author = author.replace('.', '. ')\n",
    "    author = author.replace('.  ', '. ')\n",
    "    name_to_keep = author.split(' ')[0][0] + '. ' + author.split(' ')[-1]\n",
    "\n",
    "    return name_to_keep\n",
    "\n",
    "# Transform concatenated names of authors to a list of authors \n",
    "list_authors = []\n",
    "list_universities = []\n",
    "\n",
    "info['authors'] = info['authors'].replace(np.nan, 'missing')\n",
    "for authors in info['authors']:\n",
    "    if authors != 'missing':\n",
    "        ### split the different authors\n",
    "        authors = authors.lower()\n",
    "        \n",
    "        ### Find the universities included in the name\n",
    "        universities = []\n",
    "        authors, universities = universities_to_keep(authors, universities)\n",
    "        \n",
    "        ### Split the authors\n",
    "        authors = re.split(',|&', authors)\n",
    "        \n",
    "        ### For each author, check if university, and store it. Also, keep just the names (To be improved)\n",
    "        authors_in_article = []      \n",
    "        for author in authors:\n",
    "            if author != ' ':\n",
    "                authors_in_article.append(name_to_keep(author))\n",
    "            \n",
    "        list_universities.append(universities)\n",
    "        list_authors.append(authors_in_article)\n",
    "    else:\n",
    "        list_universities.append(['missing'])\n",
    "        list_authors.append(['missing'])   \n",
    "        \n",
    "info['authors'] = list_authors\n",
    "info['universities'] = list_universities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topologic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_graph(X_train, y_train, X_test):\n",
    "    X_train = pd.concat([X_train, y_train], axis = 1)\n",
    "    X_train = X_train.values\n",
    "    G = nx.DiGraph()\n",
    "    for i in range(X_train.shape[0]):\n",
    "        source = X_train[i,0]\n",
    "        target = X_train[i,1]\n",
    "        G.add_node(source)\n",
    "        G.add_node(target)\n",
    "        if X_train[i,-1] == 1:\n",
    "            G.add_edge(source,target)\n",
    "            \n",
    "    X_test = X_test.values\n",
    "    for i in range(X_test.shape[0]):\n",
    "        source = X_test[i,0]\n",
    "        target = X_test[i,1]\n",
    "        G.add_node(source)\n",
    "        G.add_node(target)\n",
    "        \n",
    "    return G  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G = make_graph(X_train, y_train, X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_topologic_features(X, G):\n",
    "    X_ = X.copy()\n",
    "    X = X.values\n",
    "    \n",
    "    X_['Betweeness centrality'] = compute_betweeness_array(X, G)\n",
    "    X_['Number common neighbours'] = make_common_neighbors(X, G)\n",
    "    X_['Jaccard coefficienf'] = make_jaccard(X, G)\n",
    "    diff_deg, to_deg = compute_diff_inlinks(X, G)\n",
    "    X_['Difference in inlinks coefficient'] = diff_deg\n",
    "    X_[\"Number of times to cited\"] = to_deg\n",
    "    X_['Same cluster'] = same_community(X,G)\n",
    "    return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 22s, sys: 752 ms, total: 1min 23s\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from create_topologic_features import create_topologic_features\n",
    "X_train = create_topologic_features(X_train, G)\n",
    "X_test = create_topologic_features(X_test, G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Semantic features\n",
    "- Cosine similarity within the titles as tf-idf\n",
    "- Cosine similarity within the abstracts as tf-idf\n",
    "- Cosine similarity within the titles as word2vec\n",
    "- Cosine similarity within the abstracts as word2vec\n",
    "\n",
    "### To try\n",
    "- Difference cosine similarities?\n",
    "- Keep the stopwords or not?\n",
    "- Stemmise the words of not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/bat/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Parsing sentences from training set"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bat/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "/home/bat/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:146: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time taken for K Means clustering:  57.7536728382 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Import various modules for string cleaning\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def text_to_wordlist(text, remove_stopwords=False ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    text = BeautifulSoup(text).get_text()\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "    text = re.sub(\"[\\W]\",\" \", text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)\n",
    "\n",
    "# Download the punkt tokenizer for sentence splitting\n",
    "import nltk.data\n",
    "nltk.download(\"punkt\")   \n",
    "\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Define a function to split a review into parsed sentences\n",
    "def text_to_sentences(text, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a text into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(text.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append(text_to_wordlist(raw_sentence, remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences\n",
    "\n",
    "train = False\n",
    "sentences = []  # Initialize an empty list of sentences\n",
    "word_list_abstract=pd.Series(index=info.index,dtype=np.str)\n",
    "word_list_title=pd.Series(index=info.index,dtype=np.str)\n",
    "print \"Parsing sentences from training set\"\n",
    "for idx in info.index:\n",
    "    w_list_a = text_to_sentences(info.loc[idx,\"abstract\"], tokenizer, True)\n",
    "    sentences += w_list_a\n",
    "    word_list_abstract[idx]= w_list_a\n",
    "    w_list_t = text_to_sentences(info.loc[idx,\"title\"], tokenizer, True)\n",
    "    sentences += w_list_t\n",
    "    word_list_title[idx]= w_list_t\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Word2Vec\n",
    "num_features = 200    # Word vector dimensionality  \n",
    "if train:\n",
    "    # Import the built-in logging module and configure it so that Word2Vec \n",
    "    # creates nice output messages\n",
    "    import logging\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "    # Set values for various parameters\n",
    "                    \n",
    "    min_word_count = 10   # Minimum word count                        \n",
    "    num_workers = 4       # Number of threads to run in parallel\n",
    "    context = 10          # Context window size                                                                                    \n",
    "    downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "    # Initialize and train the model (this will take some time)\n",
    "\n",
    "    print \"Training model...\"\n",
    "    model = word2vec.Word2Vec(\n",
    "        sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling\n",
    "    )\n",
    "\n",
    "    # If you don't plan to train the model any further, calling \n",
    "    # init_sims will make the model much more memory-efficient.\n",
    "    model.init_sims(replace=True)\n",
    "\n",
    "    # It can be helpful to create a meaningful model name and \n",
    "    # save the model for later use. You can load it later using Word2Vec.load()\n",
    "    model_name = \"200features_10minwords_10context\"\n",
    "    model.save(model_name)\n",
    "else:\n",
    "    model = Word2Vec.load(\"200features_10minwords_10context\")\n",
    "\n",
    "def makeFeatureVec(list_of_words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in list_of_words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(text_list, model, num_features):\n",
    "    # Given a set of texts (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    textFeatureVecs = np.zeros((len(text_list),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for list_of_words in text_list:\n",
    "        # Print a status message every 1000th review\n",
    "        \"\"\"\n",
    "        if counter%1000. == 0.:\n",
    "            print \"Review %d of %d\" % (counter, len(text_list))\n",
    "        \"\"\"\n",
    "        # Call the function (defined above) that makes average feature vectors\n",
    "        textFeatureVecs[counter] = makeFeatureVec(list_of_words, model, num_features)\n",
    "\n",
    "        # Increment the counter\n",
    "        counter = counter + 1.\n",
    "    return textFeatureVecs\n",
    "\n",
    "centroid_title = dict(zip(info.index, getAvgFeatureVecs(info.title.values, model, num_features)))\n",
    "centroid_abstract = dict(zip(info.index, getAvgFeatureVecs(info.abstract.values, model, num_features)))\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "start = time.time() # Start time\n",
    "\n",
    "# Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an\n",
    "# average of 5 words per cluster\n",
    "word_vectors = model.syn0\n",
    "num_clusters = word_vectors.shape[0] / 5\n",
    "\n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "kmeans_clustering = KMeans( n_clusters = num_clusters )\n",
    "idx = kmeans_clustering.fit_predict( word_vectors )\n",
    "\n",
    "# Get the end time and print how long the process took\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print \"Time taken for K Means clustering: \", elapsed, \"seconds.\"\n",
    "\n",
    "# Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "# a cluster number                                                                                            \n",
    "word_centroid_map = dict(zip( model.index2word, idx ))\n",
    "\n",
    "def create_bag_of_centroids( wordlist, word_centroid_map ):\n",
    "    #\n",
    "    # The number of clusters is equal to the highest cluster index\n",
    "    # in the word / centroid map\n",
    "    num_centroids = max( word_centroid_map.values() ) + 1\n",
    "    #\n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
    "    #\n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count \n",
    "    # by one\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    #\n",
    "    # Return the \"bag of centroids\"\n",
    "    return bag_of_centroids\n",
    "\n",
    "# Pre-allocate an array for the training set bags of centroids (for speed)\n",
    "bag_centroids_abstract = {}\n",
    "#centroids_title = {}\n",
    "# Transform the training set reviews into bags of centroids\n",
    "for review, idx in zip(word_list_abstract, word_list_abstract.index):\n",
    "    bag_centroids_abstract[idx] = create_bag_of_centroids(review[0], word_centroid_map)\n",
    "#for review, idx in zip(word_list_title, word_list_title.index):\n",
    "#    centroids_title[idx] = create_bag_of_centroids( review[0], word_centroid_map )\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "def compute_cosines(centroids_dic, X):\n",
    "    bag=[cosine(centroids_dic[x[0]], centroids_dic[x[1]]) for x in X]\n",
    "    return np.array(bag)\n",
    "\n",
    "def create_nlp_features(X, centroid_title, centroid_abstract, bag_centroids_abstract):\n",
    "    X_ = X.copy()\n",
    "    X = X.values\n",
    "    \n",
    "    X_['CosineD_title_centroid'] = compute_cosines(centroid_title,X)\n",
    "    X_['CosineD_abstract_centroid'] = compute_cosines(centroid_abstract,X)\n",
    "    X_['CosineD_abstract_bag_centroids'] = compute_cosines(bag_centroids_abstract,X)\n",
    "    \n",
    "    return X_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 22s, sys: 54.7 ms, total: 1min 22s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = create_nlp_features(X_train, centroid_title, centroid_abstract, bag_centroids_abstract)\n",
    "X_test = create_nlp_features(X_test,centroid_title, centroid_abstract, bag_centroids_abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Attribute features\n",
    "\n",
    "- Difference in publication year\n",
    "- Number of common authors\n",
    "- Self-citation\n",
    "- Same journal\n",
    "- Number of times \"to\" cited (Attraction of the \"to\" paper)\n",
    "\n",
    "### To try\n",
    "- Number of times each author of \"to\" cited [Sum of these number of times] ?\n",
    "- Number of times each journal cited?\n",
    "- Number of same university??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/bat/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Parsing sentences from training set\n",
      "Time taken for K Means clustering:  81.2030808926 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bat/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:146: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "# Import various modules for string cleaning\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def text_to_wordlist(text, remove_stopwords=False ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    text = BeautifulSoup(text).get_text()\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "    text = re.sub(\"[\\W]\",\" \", text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)\n",
    "\n",
    "# Download the punkt tokenizer for sentence splitting\n",
    "import nltk.data\n",
    "nltk.download(\"punkt\")   \n",
    "\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Define a function to split a review into parsed sentences\n",
    "def text_to_sentences(text, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a text into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(text.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append(text_to_wordlist(raw_sentence, remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences\n",
    "\n",
    "train = False\n",
    "sentences = []  # Initialize an empty list of sentences\n",
    "word_list_abstract=pd.Series(index=info.index,dtype=np.str)\n",
    "word_list_title=pd.Series(index=info.index,dtype=np.str)\n",
    "print \"Parsing sentences from training set\"\n",
    "for idx in info.index:\n",
    "    w_list_a = text_to_sentences(info.loc[idx,\"abstract\"], tokenizer, True)\n",
    "    sentences += w_list_a\n",
    "    word_list_abstract[idx]= w_list_a\n",
    "    w_list_t = text_to_sentences(info.loc[idx,\"title\"], tokenizer, True)\n",
    "    sentences += w_list_t\n",
    "    word_list_title[idx]= w_list_t\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Word2Vec\n",
    "num_features = 200    # Word vector dimensionality  \n",
    "if train:\n",
    "    # Import the built-in logging module and configure it so that Word2Vec \n",
    "    # creates nice output messages\n",
    "    import logging\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "    # Set values for various parameters\n",
    "                    \n",
    "    min_word_count = 10   # Minimum word count                        \n",
    "    num_workers = 4       # Number of threads to run in parallel\n",
    "    context = 10          # Context window size                                                                                    \n",
    "    downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "    # Initialize and train the model (this will take some time)\n",
    "\n",
    "    print \"Training model...\"\n",
    "    model = word2vec.Word2Vec(\n",
    "        sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling\n",
    "    )\n",
    "\n",
    "    # If you don't plan to train the model any further, calling \n",
    "    # init_sims will make the model much more memory-efficient.\n",
    "    model.init_sims(replace=True)\n",
    "\n",
    "    # It can be helpful to create a meaningful model name and \n",
    "    # save the model for later use. You can load it later using Word2Vec.load()\n",
    "    model_name = \"200features_10minwords_10context\"\n",
    "    model.save(model_name)\n",
    "else:\n",
    "    model = Word2Vec.load(\"200features_10minwords_10context\")\n",
    "\n",
    "def makeFeatureVec(list_of_words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in list_of_words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(text_list, model, num_features):\n",
    "    # Given a set of texts (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    textFeatureVecs = np.zeros((len(text_list),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for list_of_words in text_list:\n",
    "        # Print a status message every 1000th review\n",
    "        \"\"\"\n",
    "        if counter%1000. == 0.:\n",
    "            print \"Review %d of %d\" % (counter, len(text_list))\n",
    "        \"\"\"\n",
    "        # Call the function (defined above) that makes average feature vectors\n",
    "        textFeatureVecs[counter] = makeFeatureVec(list_of_words, model, num_features)\n",
    "\n",
    "        # Increment the counter\n",
    "        counter = counter + 1.\n",
    "    return textFeatureVecs\n",
    "\n",
    "centroid_title = dict(zip(info.index, getAvgFeatureVecs(info.title.values, model, num_features)))\n",
    "centroid_abstract = dict(zip(info.index, getAvgFeatureVecs(info.abstract.values, model, num_features)))\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "start = time.time() # Start time\n",
    "\n",
    "# Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an\n",
    "# average of 5 words per cluster\n",
    "word_vectors = model.syn0\n",
    "num_clusters = word_vectors.shape[0] / 5\n",
    "\n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "kmeans_clustering = KMeans( n_clusters = num_clusters )\n",
    "idx = kmeans_clustering.fit_predict( word_vectors )\n",
    "\n",
    "# Get the end time and print how long the process took\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print \"Time taken for K Means clustering: \", elapsed, \"seconds.\"\n",
    "\n",
    "# Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "# a cluster number                                                                                            \n",
    "word_centroid_map = dict(zip( model.index2word, idx ))\n",
    "\n",
    "def create_bag_of_centroids( wordlist, word_centroid_map ):\n",
    "    #\n",
    "    # The number of clusters is equal to the highest cluster index\n",
    "    # in the word / centroid map\n",
    "    num_centroids = max( word_centroid_map.values() ) + 1\n",
    "    #\n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
    "    #\n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count \n",
    "    # by one\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    #\n",
    "    # Return the \"bag of centroids\"\n",
    "    return bag_of_centroids\n",
    "\n",
    "# Pre-allocate an array for the training set bags of centroids (for speed)\n",
    "bag_centroids_abstract = {}\n",
    "#centroids_title = {}\n",
    "# Transform the training set reviews into bags of centroids\n",
    "for review, idx in zip(word_list_abstract, word_list_abstract.index):\n",
    "    bag_centroids_abstract[idx] = create_bag_of_centroids(review[0], word_centroid_map)\n",
    "#for review, idx in zip(word_list_title, word_list_title.index):\n",
    "#    centroids_title[idx] = create_bag_of_centroids( review[0], word_centroid_map )\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "def compute_cosines(centroids_dic, X):\n",
    "    bag=[cosine(centroids_dic[x[0]], centroids_dic[x[1]]) for x in X]\n",
    "    return np.array(bag)\n",
    "\n",
    "def create_nlp_features(X, centroid_title, centroid_abstract, bag_centroids_abstract):\n",
    "    X_ = X.copy()\n",
    "    X = X.values\n",
    "    \n",
    "    X_['CosineD_title_centroid'] = compute_cosines(centroid_title,X)\n",
    "    X_['CosineD_abstract_centroid'] = compute_cosines(centroid_abstract,X)\n",
    "    X_['CosineD_abstract_bag_centroids'] = compute_cosines(bag_centroids_abstract,X)\n",
    "    \n",
    "    return X_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 17s, sys: 63.2 ms, total: 1min 17s\n",
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = create_nlp_features(X_train, centroid_title, centroid_abstract, bag_centroids_abstract)\n",
    "X_test = create_nlp_features(X_test,centroid_title, centroid_abstract, bag_centroids_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 59s, sys: 1.41 s, total: 4min 1s\n",
      "Wall time: 4min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from create_attribute_features import create_attribute_features\n",
    "X_train = create_attribute_features(X_train,info)\n",
    "X_test = create_attribute_features(X_test,info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author Graph features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 55s, sys: 1.72 s, total: 9min 56s\n",
      "Wall time: 10min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from author_graph import make_graph_authors, create_topologic_features_authors\n",
    "\n",
    "G_authors = make_graph_authors(X_train, y_train, info)\n",
    "X_train = create_topologic_features_authors(X_train, G_authors, info, betweeness = True, common_neigh_and_jacc = True, inlinks = True)\n",
    "X_test = create_topologic_features_authors(X_test, G_authors, info,  betweeness = True, common_neigh_and_jacc = True, inlinks = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>Betweeness centrality</th>\n",
       "      <th>Number common neighbours</th>\n",
       "      <th>Jaccard coefficienf</th>\n",
       "      <th>Difference in inlinks coefficient</th>\n",
       "      <th>Number of times to cited</th>\n",
       "      <th>Same cluster</th>\n",
       "      <th>CosineD_title_centroid</th>\n",
       "      <th>CosineD_abstract_centroid</th>\n",
       "      <th>...</th>\n",
       "      <th>Self citation</th>\n",
       "      <th>Same journal</th>\n",
       "      <th>Authors betweeness</th>\n",
       "      <th>Authors common neighbors</th>\n",
       "      <th>Authors jaccard</th>\n",
       "      <th>Authors max difference in inlinks</th>\n",
       "      <th>Authors sum difference in inlinks</th>\n",
       "      <th>Authors max of times to cited</th>\n",
       "      <th>Authors sum of times to cited</th>\n",
       "      <th>Authors  of times to cited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3432</th>\n",
       "      <td>9119</td>\n",
       "      <td>9810126</td>\n",
       "      <td>-0.005150</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006098</td>\n",
       "      <td>143</td>\n",
       "      <td>143</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012299</td>\n",
       "      <td>0.001006</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.105209</td>\n",
       "      <td>200</td>\n",
       "      <td>0.130890</td>\n",
       "      <td>720</td>\n",
       "      <td>1910</td>\n",
       "      <td>2258</td>\n",
       "      <td>872</td>\n",
       "      <td>522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116401</th>\n",
       "      <td>4115</td>\n",
       "      <td>9912072</td>\n",
       "      <td>-0.008391</td>\n",
       "      <td>32</td>\n",
       "      <td>0.105611</td>\n",
       "      <td>254</td>\n",
       "      <td>275</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003831</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.139588</td>\n",
       "      <td>978</td>\n",
       "      <td>0.397399</td>\n",
       "      <td>1270</td>\n",
       "      <td>1749</td>\n",
       "      <td>3376</td>\n",
       "      <td>2095</td>\n",
       "      <td>749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244993</th>\n",
       "      <td>11079</td>\n",
       "      <td>9802068</td>\n",
       "      <td>-0.005690</td>\n",
       "      <td>7</td>\n",
       "      <td>0.028926</td>\n",
       "      <td>158</td>\n",
       "      <td>186</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003094</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.099575</td>\n",
       "      <td>424</td>\n",
       "      <td>0.268695</td>\n",
       "      <td>685</td>\n",
       "      <td>1137</td>\n",
       "      <td>1688</td>\n",
       "      <td>893</td>\n",
       "      <td>844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485454</th>\n",
       "      <td>201004</td>\n",
       "      <td>108119</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>10</td>\n",
       "      <td>0.109890</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005318</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.001035</td>\n",
       "      <td>151</td>\n",
       "      <td>0.177022</td>\n",
       "      <td>64</td>\n",
       "      <td>-246</td>\n",
       "      <td>326</td>\n",
       "      <td>258</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470066</th>\n",
       "      <td>9210040</td>\n",
       "      <td>112110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007626</td>\n",
       "      <td>0.000821</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.023226</td>\n",
       "      <td>362</td>\n",
       "      <td>0.058125</td>\n",
       "      <td>-4972</td>\n",
       "      <td>-4972</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0        1  Betweeness centrality  Number common neighbours  \\\n",
       "3432       9119  9810126              -0.005150                         1   \n",
       "116401     4115  9912072              -0.008391                        32   \n",
       "244993    11079  9802068              -0.005690                         7   \n",
       "485454   201004   108119               0.000360                        10   \n",
       "470066  9210040   112110               0.000000                         0   \n",
       "\n",
       "        Jaccard coefficienf  Difference in inlinks coefficient  \\\n",
       "3432               0.006098                                143   \n",
       "116401             0.105611                                254   \n",
       "244993             0.028926                                158   \n",
       "485454             0.109890                                 10   \n",
       "470066             0.000000                                 -1   \n",
       "\n",
       "        Number of times to cited  Same cluster  CosineD_title_centroid  \\\n",
       "3432                         143             1                0.012299   \n",
       "116401                       275             1                0.003831   \n",
       "244993                       186             1                0.003094   \n",
       "485454                        25             1                0.005318   \n",
       "470066                         4             0                0.007626   \n",
       "\n",
       "        CosineD_abstract_centroid             ...              Self citation  \\\n",
       "3432                     0.001006             ...                          0   \n",
       "116401                   0.000732             ...                          0   \n",
       "244993                   0.000562             ...                          0   \n",
       "485454                   0.000895             ...                          0   \n",
       "470066                   0.000821             ...                          0   \n",
       "\n",
       "        Same journal  Authors betweeness  Authors common neighbors  \\\n",
       "3432               0           -0.105209                       200   \n",
       "116401             0           -0.139588                       978   \n",
       "244993             0           -0.099575                       424   \n",
       "485454             0           -0.001035                       151   \n",
       "470066             0            1.023226                       362   \n",
       "\n",
       "        Authors jaccard  Authors max difference in inlinks  \\\n",
       "3432           0.130890                                720   \n",
       "116401         0.397399                               1270   \n",
       "244993         0.268695                                685   \n",
       "485454         0.177022                                 64   \n",
       "470066         0.058125                              -4972   \n",
       "\n",
       "        Authors sum difference in inlinks  Authors max of times to cited  \\\n",
       "3432                                 1910                           2258   \n",
       "116401                               1749                           3376   \n",
       "244993                               1137                           1688   \n",
       "485454                               -246                            326   \n",
       "470066                              -4972                            180   \n",
       "\n",
       "        Authors sum of times to cited  Authors  of times to cited  \n",
       "3432                              872                         522  \n",
       "116401                           2095                         749  \n",
       "244993                            893                         844  \n",
       "485454                            258                         163  \n",
       "470066                            180                         180  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>Betweeness centrality</th>\n",
       "      <th>Number common neighbours</th>\n",
       "      <th>Jaccard coefficienf</th>\n",
       "      <th>Difference in inlinks coefficient</th>\n",
       "      <th>Number of times to cited</th>\n",
       "      <th>Same cluster</th>\n",
       "      <th>CosineD_title_centroid</th>\n",
       "      <th>CosineD_abstract_centroid</th>\n",
       "      <th>...</th>\n",
       "      <th>Self citation</th>\n",
       "      <th>Same journal</th>\n",
       "      <th>Authors betweeness</th>\n",
       "      <th>Authors common neighbors</th>\n",
       "      <th>Authors jaccard</th>\n",
       "      <th>Authors max difference in inlinks</th>\n",
       "      <th>Authors sum difference in inlinks</th>\n",
       "      <th>Authors max of times to cited</th>\n",
       "      <th>Authors sum of times to cited</th>\n",
       "      <th>Authors  of times to cited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>152793</th>\n",
       "      <td>211086</td>\n",
       "      <td>9504102</td>\n",
       "      <td>-0.002521</td>\n",
       "      <td>9</td>\n",
       "      <td>0.079646</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012404</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.095550</td>\n",
       "      <td>40</td>\n",
       "      <td>0.053333</td>\n",
       "      <td>479</td>\n",
       "      <td>700</td>\n",
       "      <td>703</td>\n",
       "      <td>482</td>\n",
       "      <td>351.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325283</th>\n",
       "      <td>7236</td>\n",
       "      <td>9606017</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>5</td>\n",
       "      <td>0.036765</td>\n",
       "      <td>75</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012277</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.994251</td>\n",
       "      <td>725</td>\n",
       "      <td>0.116634</td>\n",
       "      <td>4744</td>\n",
       "      <td>4518</td>\n",
       "      <td>5152</td>\n",
       "      <td>5152</td>\n",
       "      <td>5152.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54796</th>\n",
       "      <td>9305090</td>\n",
       "      <td>9312034</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010361</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.949638</td>\n",
       "      <td>1185</td>\n",
       "      <td>0.189904</td>\n",
       "      <td>-4351</td>\n",
       "      <td>-3947</td>\n",
       "      <td>1205</td>\n",
       "      <td>801</td>\n",
       "      <td>201.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48423</th>\n",
       "      <td>10216</td>\n",
       "      <td>9301100</td>\n",
       "      <td>-0.000504</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009030</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.064275</td>\n",
       "      <td>21</td>\n",
       "      <td>0.032159</td>\n",
       "      <td>382</td>\n",
       "      <td>579</td>\n",
       "      <td>599</td>\n",
       "      <td>402</td>\n",
       "      <td>299.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465770</th>\n",
       "      <td>9806193</td>\n",
       "      <td>9909222</td>\n",
       "      <td>-0.000324</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011871</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.035989</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010667</td>\n",
       "      <td>187</td>\n",
       "      <td>173</td>\n",
       "      <td>232</td>\n",
       "      <td>232</td>\n",
       "      <td>232.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0        1  Betweeness centrality  Number common neighbours  \\\n",
       "152793   211086  9504102              -0.002521                         9   \n",
       "325283     7236  9606017              -0.001621                         5   \n",
       "54796   9305090  9312034               0.000396                         0   \n",
       "48423     10216  9301100              -0.000504                         0   \n",
       "465770  9806193  9909222              -0.000324                         0   \n",
       "\n",
       "        Jaccard coefficienf  Difference in inlinks coefficient  \\\n",
       "152793             0.079646                                 90   \n",
       "325283             0.036765                                 75   \n",
       "54796              0.000000                                -11   \n",
       "48423              0.000000                                 17   \n",
       "465770             0.000000                                 -2   \n",
       "\n",
       "        Number of times to cited  Same cluster  CosineD_title_centroid  \\\n",
       "152793                        90             1                0.012404   \n",
       "325283                        80             0                0.012277   \n",
       "54796                          1             0                0.010361   \n",
       "48423                         17             0                0.009030   \n",
       "465770                         7             0                0.011871   \n",
       "\n",
       "        CosineD_abstract_centroid             ...              Self citation  \\\n",
       "152793                   0.000968             ...                          0   \n",
       "325283                   0.000500             ...                          0   \n",
       "54796                    0.000580             ...                          0   \n",
       "48423                    0.001801             ...                          0   \n",
       "465770                   0.000610             ...                          0   \n",
       "\n",
       "        Same journal  Authors betweeness  Authors common neighbors  \\\n",
       "152793             0           -0.095550                        40   \n",
       "325283             0           -0.994251                       725   \n",
       "54796              0            0.949638                      1185   \n",
       "48423              0           -0.064275                        21   \n",
       "465770             1           -0.035989                         4   \n",
       "\n",
       "        Authors jaccard  Authors max difference in inlinks  \\\n",
       "152793         0.053333                                479   \n",
       "325283         0.116634                               4744   \n",
       "54796          0.189904                              -4351   \n",
       "48423          0.032159                                382   \n",
       "465770         0.010667                                187   \n",
       "\n",
       "        Authors sum difference in inlinks  Authors max of times to cited  \\\n",
       "152793                                700                            703   \n",
       "325283                               4518                           5152   \n",
       "54796                               -3947                           1205   \n",
       "48423                                 579                            599   \n",
       "465770                                173                            232   \n",
       "\n",
       "        Authors sum of times to cited  Authors  of times to cited  \n",
       "152793                            482                       351.5  \n",
       "325283                           5152                      5152.0  \n",
       "54796                             801                       201.0  \n",
       "48423                             402                       299.5  \n",
       "465770                            232                       232.0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train=X_train.drop([0,1], axis = 1) \n",
    "X_test = X_test.drop([0,1], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = \"validation_set2/\"\n",
    "X_train.to_csv(path+\"X_train.csv\")\n",
    "X_test.to_csv(path+\"X_test.csv\")\n",
    "y_train.to_csv(path+\"y_train.csv\")\n",
    "y_test.to_csv(path+\"y_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(pred, real):\n",
    "    tot = 0\n",
    "    for i, val in enumerate(real):\n",
    "        if pred[i] == val:\n",
    "            tot += 1\n",
    "    return float(tot)/len(real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sknn.mlp import Layer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sknn.mlp import Classifier as MLP\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "scaler1=StandardScaler()\n",
    "scaler = MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "nn = MLP(\n",
    "layers=[\n",
    "Layer(\"Rectifier\", units=2000),\n",
    "        Layer(\"Softmax\")],\n",
    "        learning_rule = 'momentum', learning_rate=0.02, batch_size = 150,dropout_rate = 0.25,\n",
    "        n_iter=50,\n",
    "        verbose = 1, \n",
    "        valid_size = 0.1, \n",
    "        n_stable = 15,\n",
    "        debug = False,\n",
    "        #    regularize = 'L2'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-f0b96fcfd1f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     ])\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bat/anaconda2/lib/python2.7/site-packages/sklearn/pipeline.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    163\u001b[0m         \"\"\"\n\u001b[0;32m    164\u001b[0m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pre_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bat/anaconda2/lib/python2.7/site-packages/sknn/mlp.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, w)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;31m# Now train based on a problem transformed into regression.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bat/anaconda2/lib/python2.7/site-packages/sknn/mlp.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, w)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m             log.error(\"\\n{}{}{}\\n\\n{}\\n\".format(\n",
      "\u001b[1;32m/home/bat/anaconda2/lib/python2.7/site-packages/sknn/mlp.pyc\u001b[0m in \u001b[0;36m_train\u001b[1;34m(self, X, y, w)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m             \u001b[0mis_best_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m             \u001b[0mavg_train_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mavg_train_error\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mavg_train_error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bat/anaconda2/lib/python2.7/site-packages/sknn/backend/lasagne/mlp.pyc\u001b[0m in \u001b[0;36m_train_impl\u001b[1;34m(self, X, y, w)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_train_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_valid_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bat/anaconda2/lib/python2.7/site-packages/sknn/backend/lasagne/mlp.pyc\u001b[0m in \u001b[0;36m_batch_impl\u001b[1;34m(self, X, y, w, processor, mode, output, shuffle)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwb\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mwb\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bat/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    (\"scaler\", scaler1),\n",
    "    ('neural network', nn)\n",
    "    ])\n",
    "\n",
    "clf.fit(X_train.drop([0,1], axis = 1), y_train)\n",
    "pred = clf.predict(X_test.drop([0,1], axis = 1))\n",
    "print score(pred, y_test)\n",
    "print score(clf.predict(X_train.drop([0,1], axis = 1)),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.958546907874\n",
      "0.999987815007\n",
      "CPU times: user 3min 13s, sys: 999 ms, total: 3min 14s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators = 100,n_jobs=2)\n",
    "rfc.fit(X_train.drop([0,1], axis = 1), y_train)\n",
    "pred = rfc.predict(X_test.drop([0,1], axis = 1))\n",
    "print score(pred, y_test)\n",
    "print score(rfc.predict(X_train.drop([0,1], axis = 1)),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.961536274502\n",
      "1.0\n",
      "CPU times: user 9min 33s, sys: 4.21 s, total: 9min 38s\n",
      "Wall time: 4min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "etc = ExtraTreesClassifier(n_estimators = 400,n_jobs=2)\n",
    "etc.fit(X_train.drop([0,1], axis = 1), y_train)\n",
    "pred = etc.predict(X_test.drop([0,1], axis = 1))\n",
    "print score(pred, y_test)\n",
    "print score(etc.predict(X_train.drop([0,1], axis = 1)),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier(n_estimators = 200)\n",
    "gbc.fit(X_train.drop([0,1], axis = 1), y_train)\n",
    "pred = gbc.predict(X_test.drop([0,1], axis = 1))\n",
    "print score(pred, y_test)\n",
    "print score(gbc.predict(X_train.drop([0,1], axis = 1)),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.952941845446\n",
      "0.956489422411\n",
      "CPU times: user 41 s, sys: 848 ms, total: 41.8 s\n",
      "Wall time: 41.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf =  Pipeline([\n",
    "    (\"scaler\", MinMaxScaler()),\n",
    "    (\"regression\",LogisticRegression(C=200000,max_iter = 1000))\n",
    "    ])\n",
    "clf.fit(X_train.drop([0,1], axis = 1), y_train)\n",
    "pred = clf.predict(X_test.drop([0,1], axis = 1))\n",
    "print score(pred, y_test)\n",
    "print score(clf.predict(X_train.drop([0,1], axis = 1)),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_submission(predicted_label, name = 'submit.csv'):\n",
    "    submit_d = d = {'id' : pd.Series(np.arange(1,X_test.shape[0]+1).astype(int)),\n",
    "                    'category' : pd.Series(predicted_label).astype(int)}\n",
    "    submit = pd.DataFrame(submit_d)\n",
    "    submit.to_csv(name,index=False)\n",
    "    return submit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
