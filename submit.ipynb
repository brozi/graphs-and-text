{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "import csv\n",
    "import pandas as pd\n",
    "from nltk.stem import SnowballStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import re\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "info = pd.read_csv(\n",
    "    \"node_information.csv\", \n",
    "    header= None, \n",
    "    names=[\"Id\", \"year\", \"title\", \"authors\", \"journal\", \"abstract\"],\n",
    "    sep=\",\",\n",
    "    index_col = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27770, 5) (615512, 2) (32648, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.read_csv(\"training_set.txt\", sep=\" \", header=None)\n",
    "X_test = pd.read_csv(\"testing_set.txt\", sep=\" \", header=None)\n",
    "y_train = X_train[2]\n",
    "X_train.drop([2], axis = 1, inplace = True)\n",
    "\n",
    "print info.shape, X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######################\n",
    "### FOR VALIDATION ###\n",
    "######################\n",
    "\n",
    "\n",
    "#####################\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.2)\n",
    "#####################\n",
    "\n",
    "#####################\n",
    "small_portion_to_train = 50000\n",
    "small_portion_to_test  = 5000\n",
    "#X_train = X_train[:small_portion_to_train]\n",
    "#y_train = y_train[:small_portion_to_train]\n",
    "\n",
    "#X_test  = X_test[:small_portion_to_test]\n",
    "#y_test = y_test[:small_portion_to_test]\n",
    "#####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- list_authors is the list of authors in the papers\n",
    "- list_universities is the list where the authors are from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def universities_to_keep(authors, universities):\n",
    "    while('(' in authors and ')' in authors):\n",
    "        universities.append( authors[authors.find('(')+1 : authors.find(')')] )\n",
    "        authors = authors[: authors.find('(')] + authors[ authors.find(')')+1 : ]\n",
    "            \n",
    "    if '(' in authors:\n",
    "        universities.append( authors[authors.find('(')+1 : ])\n",
    "        authors = authors[: authors.find('(')]\n",
    "    \n",
    "    return authors, universities\n",
    "\n",
    "\n",
    "def name_to_keep(author):\n",
    "    if len(author.split(' ')) <= 1:\n",
    "        return author\n",
    "    \n",
    "    while( author[0] == ' ' and len(author) > 0):\n",
    "        author = author[1:]\n",
    "    while( author[-1] == ' ' and len(author) > 0):\n",
    "        author = author[:-1]\n",
    "    \n",
    "    author = author.replace('.', '. ')\n",
    "    author = author.replace('.  ', '. ')\n",
    "    name_to_keep = author.split(' ')[0][0] + '. ' + author.split(' ')[-1]\n",
    "\n",
    "    return name_to_keep\n",
    "\n",
    "# Transform concatenated names of authors to a list of authors \n",
    "list_authors = []\n",
    "list_universities = []\n",
    "\n",
    "info['authors'] = info['authors'].replace(np.nan, 'missing')\n",
    "for authors in info['authors']:\n",
    "    if authors != 'missing':\n",
    "        ### split the different authors\n",
    "        authors = authors.lower()\n",
    "        \n",
    "        ### Find the universities included in the name\n",
    "        universities = []\n",
    "        authors, universities = universities_to_keep(authors, universities)\n",
    "        \n",
    "        ### Split the authors\n",
    "        authors = re.split(',|&', authors)\n",
    "        \n",
    "        ### For each author, check if university, and store it. Also, keep just the names (To be improved)\n",
    "        authors_in_article = []      \n",
    "        for author in authors:\n",
    "            if author != ' ':\n",
    "                authors_in_article.append(name_to_keep(author))\n",
    "            \n",
    "        list_universities.append(universities)\n",
    "        list_authors.append(authors_in_article)\n",
    "    else:\n",
    "        list_universities.append(['missing'])\n",
    "        list_authors.append(['missing'])   \n",
    "        \n",
    "info['authors'] = list_authors\n",
    "info['universities'] = list_universities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topologic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_graph(X_train, y_train, X_test):\n",
    "    X_train = pd.concat([X_train, y_train], axis = 1)\n",
    "    X_train = X_train.values\n",
    "    G = nx.DiGraph()\n",
    "    for i in range(X_train.shape[0]):\n",
    "        source = X_train[i,0]\n",
    "        target = X_train[i,1]\n",
    "        G.add_node(source)\n",
    "        G.add_node(target)\n",
    "        if X_train[i,2] == 1:\n",
    "            G.add_edge(source,target)\n",
    "            \n",
    "    X_test = X_test.values\n",
    "    for i in range(X_test.shape[0]):\n",
    "        source = X_test[i,0]\n",
    "        target = X_test[i,1]\n",
    "        G.add_node(source)\n",
    "        G.add_node(target)\n",
    "        \n",
    "    return G  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G = make_graph(X_train, y_train, X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 25s, sys: 804 ms, total: 1min 26s\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from create_topologic_features import create_topologic_features\n",
    "X_train = create_topologic_features(X_train, G)\n",
    "X_test = create_topologic_features(X_test, G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Semantic features\n",
    "- Cosine similarity within the titles as tf-idf\n",
    "- Cosine similarity within the abstracts as tf-idf\n",
    "- Cosine similarity within the titles as word2vec\n",
    "- Cosine similarity within the abstracts as word2vec\n",
    "\n",
    "### To try\n",
    "- Difference cosine similarities?\n",
    "- Keep the stopwords or not?\n",
    "- Stemmise the words of not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/bat/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Parsing sentences from training set"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bat/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "/home/bat/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:146: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time taken for K Means clustering:  59.4925282001 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Import various modules for string cleaning\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def text_to_wordlist(text, remove_stopwords=False ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    text = BeautifulSoup(text).get_text()\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "    text = re.sub(\"[\\W]\",\" \", text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)\n",
    "\n",
    "# Download the punkt tokenizer for sentence splitting\n",
    "import nltk.data\n",
    "nltk.download(\"punkt\")   \n",
    "\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Define a function to split a review into parsed sentences\n",
    "def text_to_sentences(text, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a text into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(text.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append(text_to_wordlist(raw_sentence, remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences\n",
    "\n",
    "train = False\n",
    "sentences = []  # Initialize an empty list of sentences\n",
    "word_list_abstract=pd.Series(index=info.index,dtype=np.str)\n",
    "word_list_title=pd.Series(index=info.index,dtype=np.str)\n",
    "print \"Parsing sentences from training set\"\n",
    "for idx in info.index:\n",
    "    w_list_a = text_to_sentences(info.loc[idx,\"abstract\"], tokenizer, True)\n",
    "    sentences += w_list_a\n",
    "    word_list_abstract[idx]= w_list_a\n",
    "    w_list_t = text_to_sentences(info.loc[idx,\"title\"], tokenizer, True)\n",
    "    sentences += w_list_t\n",
    "    word_list_title[idx]= w_list_t\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Word2Vec\n",
    "num_features = 200    # Word vector dimensionality  \n",
    "if train:\n",
    "    # Import the built-in logging module and configure it so that Word2Vec \n",
    "    # creates nice output messages\n",
    "    import logging\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "    # Set values for various parameters\n",
    "                    \n",
    "    min_word_count = 10   # Minimum word count                        \n",
    "    num_workers = 4       # Number of threads to run in parallel\n",
    "    context = 10          # Context window size                                                                                    \n",
    "    downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "    # Initialize and train the model (this will take some time)\n",
    "\n",
    "    print \"Training model...\"\n",
    "    model = word2vec.Word2Vec(\n",
    "        sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling\n",
    "    )\n",
    "\n",
    "    # If you don't plan to train the model any further, calling \n",
    "    # init_sims will make the model much more memory-efficient.\n",
    "    model.init_sims(replace=True)\n",
    "\n",
    "    # It can be helpful to create a meaningful model name and \n",
    "    # save the model for later use. You can load it later using Word2Vec.load()\n",
    "    model_name = \"200features_10minwords_10context\"\n",
    "    model.save(model_name)\n",
    "else:\n",
    "    model = Word2Vec.load(\"200features_10minwords_10context\")\n",
    "\n",
    "def makeFeatureVec(list_of_words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in list_of_words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(text_list, model, num_features):\n",
    "    # Given a set of texts (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    textFeatureVecs = np.zeros((len(text_list),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for list_of_words in text_list:\n",
    "        # Print a status message every 1000th review\n",
    "        \"\"\"\n",
    "        if counter%1000. == 0.:\n",
    "            print \"Review %d of %d\" % (counter, len(text_list))\n",
    "        \"\"\"\n",
    "        # Call the function (defined above) that makes average feature vectors\n",
    "        textFeatureVecs[counter] = makeFeatureVec(list_of_words, model, num_features)\n",
    "\n",
    "        # Increment the counter\n",
    "        counter = counter + 1.\n",
    "    return textFeatureVecs\n",
    "\n",
    "centroid_title = dict(zip(info.index, getAvgFeatureVecs(info.title.values, model, num_features)))\n",
    "centroid_abstract = dict(zip(info.index, getAvgFeatureVecs(info.abstract.values, model, num_features)))\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "start = time.time() # Start time\n",
    "\n",
    "# Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an\n",
    "# average of 5 words per cluster\n",
    "word_vectors = model.syn0\n",
    "num_clusters = word_vectors.shape[0] / 5\n",
    "\n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "kmeans_clustering = KMeans( n_clusters = num_clusters )\n",
    "idx = kmeans_clustering.fit_predict( word_vectors )\n",
    "\n",
    "# Get the end time and print how long the process took\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print \"Time taken for K Means clustering: \", elapsed, \"seconds.\"\n",
    "\n",
    "# Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "# a cluster number                                                                                            \n",
    "word_centroid_map = dict(zip( model.index2word, idx ))\n",
    "\n",
    "def create_bag_of_centroids( wordlist, word_centroid_map ):\n",
    "    #\n",
    "    # The number of clusters is equal to the highest cluster index\n",
    "    # in the word / centroid map\n",
    "    num_centroids = max( word_centroid_map.values() ) + 1\n",
    "    #\n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
    "    #\n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count \n",
    "    # by one\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    #\n",
    "    # Return the \"bag of centroids\"\n",
    "    return bag_of_centroids\n",
    "\n",
    "# Pre-allocate an array for the training set bags of centroids (for speed)\n",
    "bag_centroids_abstract = {}\n",
    "#centroids_title = {}\n",
    "# Transform the training set reviews into bags of centroids\n",
    "for review, idx in zip(word_list_abstract, word_list_abstract.index):\n",
    "    bag_centroids_abstract[idx] = create_bag_of_centroids(review[0], word_centroid_map)\n",
    "#for review, idx in zip(word_list_title, word_list_title.index):\n",
    "#    centroids_title[idx] = create_bag_of_centroids( review[0], word_centroid_map )\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "def compute_cosines(centroids_dic, X):\n",
    "    bag=[cosine(centroids_dic[x[0]], centroids_dic[x[1]]) for x in X]\n",
    "    return np.array(bag)\n",
    "\n",
    "def create_nlp_features(X, centroid_title, centroid_abstract, bag_centroids_abstract):\n",
    "    X_ = X.copy()\n",
    "    X = X.values\n",
    "    \n",
    "    X_['CosineD_title_centroid'] = compute_cosines(centroid_title,X)\n",
    "    X_['CosineD_abstract_centroid'] = compute_cosines(centroid_abstract,X)\n",
    "    X_['CosineD_abstract_bag_centroids'] = compute_cosines(bag_centroids_abstract,X)\n",
    "    \n",
    "    return X_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 18s, sys: 78.1 ms, total: 1min 18s\n",
      "Wall time: 1min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = create_nlp_features(X_train, centroid_title, centroid_abstract, bag_centroids_abstract)\n",
    "X_test = create_nlp_features(X_test,centroid_title, centroid_abstract, bag_centroids_abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Attribute features\n",
    "\n",
    "- Difference in publication year\n",
    "- Number of common authors\n",
    "- Self-citation\n",
    "- Same journal\n",
    "- Number of times \"to\" cited (Attraction of the \"to\" paper)\n",
    "\n",
    "### To try\n",
    "- Number of times each author of \"to\" cited [Sum of these number of times] ?\n",
    "- Number of times each journal cited?\n",
    "- Number of same university??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 45s, sys: 1.49 s, total: 3min 47s\n",
      "Wall time: 3min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from create_attribute_features import create_attribute_features\n",
    "X_train = create_attribute_features(X_train,info)\n",
    "X_test = create_attribute_features(X_test,info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author Graph Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 4s, sys: 2.34 s, total: 8min 7s\n",
      "Wall time: 8min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from author_graph import make_graph_authors, create_topologic_features_authors\n",
    "\n",
    "G_authors = make_graph_authors(X_train, y_train, info)\n",
    "X_train = create_topologic_features_authors(X_train, G_authors, info, betweeness = True, common_neigh_and_jacc = True, inlinks = True)\n",
    "X_test = create_topologic_features_authors(X_test, G_authors, info,  betweeness = True, common_neigh_and_jacc = True, inlinks = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>Betweeness centrality</th>\n",
       "      <th>Number common neighbours</th>\n",
       "      <th>Jaccard coefficienf</th>\n",
       "      <th>Difference in inlinks coefficient</th>\n",
       "      <th>Number of times to cited</th>\n",
       "      <th>Same cluster</th>\n",
       "      <th>CosineD_title_centroid</th>\n",
       "      <th>CosineD_abstract_centroid</th>\n",
       "      <th>...</th>\n",
       "      <th>Self citation</th>\n",
       "      <th>Same journal</th>\n",
       "      <th>Authors betweeness</th>\n",
       "      <th>Authors common neighbors</th>\n",
       "      <th>Authors jaccard</th>\n",
       "      <th>Authors max difference in inlinks</th>\n",
       "      <th>Authors sum difference in inlinks</th>\n",
       "      <th>Authors max of times to cited</th>\n",
       "      <th>Authors sum of times to cited</th>\n",
       "      <th>Authors  of times to cited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9510123</td>\n",
       "      <td>9502114</td>\n",
       "      <td>-0.000216</td>\n",
       "      <td>1</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003766</td>\n",
       "      <td>0.001423</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.080947</td>\n",
       "      <td>636</td>\n",
       "      <td>0.097952</td>\n",
       "      <td>-5247</td>\n",
       "      <td>-4982</td>\n",
       "      <td>426</td>\n",
       "      <td>161</td>\n",
       "      <td>154.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9707075</td>\n",
       "      <td>9604178</td>\n",
       "      <td>-0.002449</td>\n",
       "      <td>20</td>\n",
       "      <td>0.097087</td>\n",
       "      <td>113</td>\n",
       "      <td>124</td>\n",
       "      <td>1</td>\n",
       "      <td>0.026033</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.063470</td>\n",
       "      <td>413</td>\n",
       "      <td>0.324430</td>\n",
       "      <td>363</td>\n",
       "      <td>682</td>\n",
       "      <td>1086</td>\n",
       "      <td>758</td>\n",
       "      <td>543.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9312155</td>\n",
       "      <td>9506142</td>\n",
       "      <td>-0.000144</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009869</td>\n",
       "      <td>0.001369</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.005749</td>\n",
       "      <td>1</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9911255</td>\n",
       "      <td>302165</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004979</td>\n",
       "      <td>0.000687</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.056456</td>\n",
       "      <td>291</td>\n",
       "      <td>0.266728</td>\n",
       "      <td>222</td>\n",
       "      <td>214</td>\n",
       "      <td>667</td>\n",
       "      <td>667</td>\n",
       "      <td>667.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9701033</td>\n",
       "      <td>209076</td>\n",
       "      <td>-0.000612</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005345</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.116592</td>\n",
       "      <td>134</td>\n",
       "      <td>0.020685</td>\n",
       "      <td>-5393</td>\n",
       "      <td>-5389</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0        1  Betweeness centrality  Number common neighbours  \\\n",
       "0  9510123  9502114              -0.000216                         1   \n",
       "1  9707075  9604178              -0.002449                        20   \n",
       "2  9312155  9506142              -0.000144                         0   \n",
       "3  9911255   302165               0.000216                         0   \n",
       "4  9701033   209076              -0.000612                         0   \n",
       "\n",
       "   Jaccard coefficienf  Difference in inlinks coefficient  \\\n",
       "0             0.058824                                  5   \n",
       "1             0.097087                                113   \n",
       "2             0.000000                                  1   \n",
       "3             0.000000                                 -2   \n",
       "4             0.000000                                 -5   \n",
       "\n",
       "   Number of times to cited  Same cluster  CosineD_title_centroid  \\\n",
       "0                         8             0                0.003766   \n",
       "1                       124             1                0.026033   \n",
       "2                         2             0                0.009869   \n",
       "3                         2             1                0.004979   \n",
       "4                         2             0                0.005345   \n",
       "\n",
       "   CosineD_abstract_centroid             ...              Self citation  \\\n",
       "0                   0.001423             ...                          0   \n",
       "1                   0.001301             ...                          0   \n",
       "2                   0.001369             ...                          0   \n",
       "3                   0.000687             ...                          0   \n",
       "4                   0.000681             ...                          0   \n",
       "\n",
       "   Same journal  Authors betweeness  Authors common neighbors  \\\n",
       "0             1            1.080947                       636   \n",
       "1             0           -0.063470                       413   \n",
       "2             0           -0.005749                         1   \n",
       "3             0           -0.056456                       291   \n",
       "4             0            1.116592                       134   \n",
       "\n",
       "   Authors jaccard  Authors max difference in inlinks  \\\n",
       "0         0.097952                              -5247   \n",
       "1         0.324430                                363   \n",
       "2         0.023256                                 14   \n",
       "3         0.266728                                222   \n",
       "4         0.020685                              -5393   \n",
       "\n",
       "   Authors sum difference in inlinks  Authors max of times to cited  \\\n",
       "0                              -4982                            426   \n",
       "1                                682                           1086   \n",
       "2                                 13                             15   \n",
       "3                                214                            667   \n",
       "4                              -5389                             19   \n",
       "\n",
       "   Authors sum of times to cited  Authors  of times to cited  \n",
       "0                            161                       154.0  \n",
       "1                            758                       543.0  \n",
       "2                             15                        15.0  \n",
       "3                            667                       667.0  \n",
       "4                             15                         9.5  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>Betweeness centrality</th>\n",
       "      <th>Number common neighbours</th>\n",
       "      <th>Jaccard coefficienf</th>\n",
       "      <th>Difference in inlinks coefficient</th>\n",
       "      <th>Number of times to cited</th>\n",
       "      <th>Same cluster</th>\n",
       "      <th>CosineD_title_centroid</th>\n",
       "      <th>CosineD_abstract_centroid</th>\n",
       "      <th>...</th>\n",
       "      <th>Self citation</th>\n",
       "      <th>Same journal</th>\n",
       "      <th>Authors betweeness</th>\n",
       "      <th>Authors common neighbors</th>\n",
       "      <th>Authors jaccard</th>\n",
       "      <th>Authors max difference in inlinks</th>\n",
       "      <th>Authors sum difference in inlinks</th>\n",
       "      <th>Authors max of times to cited</th>\n",
       "      <th>Authors sum of times to cited</th>\n",
       "      <th>Authors  of times to cited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9807076</td>\n",
       "      <td>9807139</td>\n",
       "      <td>0.001476</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-46</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011031</td>\n",
       "      <td>0.001181</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.069564</td>\n",
       "      <td>424</td>\n",
       "      <td>0.275325</td>\n",
       "      <td>-383</td>\n",
       "      <td>-1352</td>\n",
       "      <td>678</td>\n",
       "      <td>539</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>109162</td>\n",
       "      <td>1182</td>\n",
       "      <td>0.009291</td>\n",
       "      <td>24</td>\n",
       "      <td>0.074303</td>\n",
       "      <td>-61</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003334</td>\n",
       "      <td>0.001054</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.082327</td>\n",
       "      <td>302</td>\n",
       "      <td>0.258120</td>\n",
       "      <td>-145</td>\n",
       "      <td>277</td>\n",
       "      <td>719</td>\n",
       "      <td>297</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9702187</td>\n",
       "      <td>9510135</td>\n",
       "      <td>-0.018582</td>\n",
       "      <td>59</td>\n",
       "      <td>0.065338</td>\n",
       "      <td>517</td>\n",
       "      <td>726</td>\n",
       "      <td>1</td>\n",
       "      <td>0.008947</td>\n",
       "      <td>0.001919</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.819018</td>\n",
       "      <td>2465</td>\n",
       "      <td>0.374677</td>\n",
       "      <td>3183</td>\n",
       "      <td>2003</td>\n",
       "      <td>5408</td>\n",
       "      <td>5408</td>\n",
       "      <td>5408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>111048</td>\n",
       "      <td>110115</td>\n",
       "      <td>-0.000504</td>\n",
       "      <td>21</td>\n",
       "      <td>0.221053</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009270</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.070829</td>\n",
       "      <td>358</td>\n",
       "      <td>0.229487</td>\n",
       "      <td>577</td>\n",
       "      <td>1275</td>\n",
       "      <td>1680</td>\n",
       "      <td>872</td>\n",
       "      <td>532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9910176</td>\n",
       "      <td>9410073</td>\n",
       "      <td>-0.005150</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>143</td>\n",
       "      <td>144</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001355</td>\n",
       "      <td>0.001571</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.143498</td>\n",
       "      <td>289</td>\n",
       "      <td>0.135681</td>\n",
       "      <td>1108</td>\n",
       "      <td>3146</td>\n",
       "      <td>3367</td>\n",
       "      <td>1329</td>\n",
       "      <td>1171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0        1  Betweeness centrality  Number common neighbours  \\\n",
       "0  9807076  9807139               0.001476                         0   \n",
       "1   109162     1182               0.009291                        24   \n",
       "2  9702187  9510135              -0.018582                        59   \n",
       "3   111048   110115              -0.000504                        21   \n",
       "4  9910176  9410073              -0.005150                         0   \n",
       "\n",
       "   Jaccard coefficienf  Difference in inlinks coefficient  \\\n",
       "0             0.000000                                -46   \n",
       "1             0.074303                                -61   \n",
       "2             0.065338                                517   \n",
       "3             0.221053                                  5   \n",
       "4             0.000000                                143   \n",
       "\n",
       "   Number of times to cited  Same cluster  CosineD_title_centroid  \\\n",
       "0                         3             1                0.011031   \n",
       "1                        39             1                0.003334   \n",
       "2                       726             1                0.008947   \n",
       "3                        16             1                0.009270   \n",
       "4                       144             1                0.001355   \n",
       "\n",
       "   CosineD_abstract_centroid             ...              Self citation  \\\n",
       "0                   0.001181             ...                          0   \n",
       "1                   0.001054             ...                          0   \n",
       "2                   0.001919             ...                          0   \n",
       "3                   0.000746             ...                          0   \n",
       "4                   0.001571             ...                          0   \n",
       "\n",
       "   Same journal  Authors betweeness  Authors common neighbors  \\\n",
       "0             0            0.069564                       424   \n",
       "1             0            0.082327                       302   \n",
       "2             1           -0.819018                      2465   \n",
       "3             1           -0.070829                       358   \n",
       "4             0           -0.143498                       289   \n",
       "\n",
       "   Authors jaccard  Authors max difference in inlinks  \\\n",
       "0         0.275325                               -383   \n",
       "1         0.258120                               -145   \n",
       "2         0.374677                               3183   \n",
       "3         0.229487                                577   \n",
       "4         0.135681                               1108   \n",
       "\n",
       "   Authors sum difference in inlinks  Authors max of times to cited  \\\n",
       "0                              -1352                            678   \n",
       "1                                277                            719   \n",
       "2                               2003                           5408   \n",
       "3                               1275                           1680   \n",
       "4                               3146                           3367   \n",
       "\n",
       "   Authors sum of times to cited  Authors  of times to cited  \n",
       "0                            539                         339  \n",
       "1                            297                         215  \n",
       "2                           5408                        5408  \n",
       "3                            872                         532  \n",
       "4                           1329                        1171  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(pred, real):\n",
    "    tot = 0\n",
    "    for i, val in enumerate(real):\n",
    "        if pred[i] == val:\n",
    "            tot += 1\n",
    "    return float(tot)/len(real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-77a49a6ddfed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train_set/X_test.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train_set/y_train.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train_set/y_test.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "X_train.to_csv(\"train_set/X_train.csv\")\n",
    "X_test.to_csv(\"train_set/X_test.csv\")\n",
    "y_train.to_csv(\"train_set/y_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run from here if the data are already created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "import csv\n",
    "import pandas as pd\n",
    "from nltk.stem import SnowballStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "def score(pred, real):\n",
    "    tot = 0\n",
    "    for i, val in enumerate(real):\n",
    "        if pred[i] == val:\n",
    "            tot += 1\n",
    "    return float(tot)/len(real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"train_set/\"\n",
    "\n",
    "X_train = pd.read_csv(path+\"X_train.csv\",index_col=0)\n",
    "X_test = pd.read_csv(path+\"X_test.csv\",index_col=0)\n",
    "y_train = pd.Series.from_csv(path+\"y_train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_light = X_train.drop([\"Number common neighbours\",\"Self citation\",\"Same journal\",\"Authors betweeness\",\"Authors max difference in inlinks\",\"Number of times to cited\"], axis = 1)\n",
    "X_test_light = X_test.drop([\"Number common neighbours\",\"Self citation\",\"Same journal\",\"Authors betweeness\",\"Authors max difference in inlinks\",\"Number of times to cited\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_light = X_train.drop([\"Number of times to cited\"], axis = 1)\n",
    "X_test_light = X_test.drop([\"Number of times to cited\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sknn.mlp import Layer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sknn.mlp import Classifier as MLP\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "scaler1=StandardScaler()\n",
    "scaler = MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "val_percent = 0.1\n",
    "nn = MLP(\n",
    "layers=[\n",
    "    Layer(\"Rectifier\", units=100),\n",
    "    Layer(\"Rectifier\", units=100),\n",
    "    Layer(\"Rectifier\", units=100),\n",
    "        Layer(\"Softmax\")],\n",
    "        learning_rule = 'momentum', learning_rate=0.05, batch_size = 100,dropout_rate =0.1,\n",
    "        n_iter=100,\n",
    "        verbose = 1, \n",
    "        valid_size = val_percent, \n",
    "        n_stable = 30,\n",
    "        debug = True,\n",
    "        #    regularize = 'L2'\n",
    ")\n",
    "\n",
    "clfnn1 = Pipeline([\n",
    "    (\"scaler\", scaler1),\n",
    "    ('neural network', nn)\n",
    "    ])\n",
    "\n",
    "clfnn1.fit(X_train.values, y_train.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn2 = MLP(\n",
    "layers=[\n",
    "    Layer(\"Rectifier\", units=100),\n",
    "    Layer(\"Rectifier\", units=100),\n",
    "    Layer(\"Rectifier\", units=100),\n",
    "        Layer(\"Softmax\")],\n",
    "        learning_rule = 'momentum', learning_rate=0.05, batch_size = 100,dropout_rate =0.1,\n",
    "        n_iter=100,\n",
    "        verbose = 1, \n",
    "        valid_size = val_percent, \n",
    "        n_stable = 30,\n",
    "        debug = True,\n",
    "        #    regularize = 'L2'\n",
    ")\n",
    "\n",
    "clfnn2 = Pipeline([\n",
    "    (\"scaler\", scaler),\n",
    "    ('neural network', nn2)\n",
    "    ])\n",
    "\n",
    "clfnn2.fit(X_train.values, y_train.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "etc = ExtraTreesClassifier(n_estimators = 700,n_jobs=1)\n",
    "etc.fit(X_train_light, y_train)\n",
    "#pred = clf.predict(X_test.drop([0,1], axis = 1))\n",
    "#print score(pred, y_test)\n",
    "#print score(clf.predict(X_train.drop([0,1], axis = 1)),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier(n_estimators = 700)\n",
    "gbc.fit(X_train_light, y_train)\n",
    "#pred = gbc.predict(X_test_light)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "X_columns = X_train.drop([0,1], axis = 1).columns\n",
    "ordering = np.argsort(gbc.feature_importances_)[::-1]\n",
    "\n",
    "importances = gbc.feature_importances_[ordering]\n",
    "feature_names = X_columns[ordering]\n",
    "\n",
    "x = np.arange(len(feature_names))\n",
    "plt.bar(x, importances)\n",
    "plt.xticks(x + 0.5, feature_names, rotation=90, fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASSEMBLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_array = [gbc.predict_proba(X_test_light)[:,1],rfc.predict_proba(X_test_light)[:,1], clfnn1.predict_proba(X_test)[:,1].flatten(),clfnn2.predict_proba(X_test)[:,1].flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_pred(y_pred_array, weights = None):\n",
    "    if weights =None:\n",
    "        weights = [1 for i in y_pred_array]\n",
    "    return rounds(1./sum(weights)*sum([weights[i]*y for i,y in enumerate(y_pred_array)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = make_pred(y_pred_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_submission(predicted_label, name = 'submit.csv'):\n",
    "    submit_d = d = {'id' : pd.Series(np.arange(predicted_label.shape[0]).astype(int)),\n",
    "                    'category' : pd.Series(predicted_label).astype(int)}\n",
    "    submit = pd.DataFrame(submit_d, columns=[\"id\",\"category\"])\n",
    "    submit.to_csv(name,index=False)\n",
    "    return submit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submit = make_submission(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32648, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[l for i,l in enumerate(pred)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
