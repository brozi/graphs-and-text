{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "import csv\n",
    "import pandas as pd\n",
    "from nltk.stem import SnowballStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import re\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "info = pd.read_csv(\n",
    "    \"node_information.csv\", \n",
    "    header= None, \n",
    "    names=[\"Id\", \"year\", \"title\", \"authors\", \"journal\", \"abstract\"],\n",
    "    sep=\",\",\n",
    "    index_col = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27770, 5) (615512, 2) (32648, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.read_csv(\"training_set.txt\", sep=\" \", header=None)\n",
    "X_test = pd.read_csv(\"testing_set.txt\", sep=\" \", header=None)\n",
    "y_train = X_train[2]\n",
    "X_train.drop([2], axis = 1, inplace = True)\n",
    "\n",
    "print info.shape, X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######################\n",
    "### FOR VALIDATION ###\n",
    "######################\n",
    "\n",
    "\n",
    "#####################\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.2)\n",
    "#####################\n",
    "\n",
    "#####################\n",
    "small_portion_to_train = 50000\n",
    "small_portion_to_test  = 5000\n",
    "X_train = X_train[:small_portion_to_train]\n",
    "y_train = y_train[:small_portion_to_train]\n",
    "\n",
    "X_test  = X_test[:small_portion_to_test]\n",
    "y_train = y_test[:small_portion_to_test]\n",
    "#####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- list_authors is the list of authors in the papers\n",
    "- list_universities is the list where the authors are from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def universities_to_keep(authors, universities):\n",
    "    while('(' in authors and ')' in authors):\n",
    "        universities.append( authors[authors.find('(')+1 : authors.find(')')] )\n",
    "        authors = authors[: authors.find('(')] + authors[ authors.find(')')+1 : ]\n",
    "            \n",
    "    if '(' in authors:\n",
    "        universities.append( authors[authors.find('(')+1 : ])\n",
    "        authors = authors[: authors.find('(')]\n",
    "    \n",
    "    return authors, universities\n",
    "\n",
    "\n",
    "def name_to_keep(author):\n",
    "    if len(author.split(' ')) <= 1:\n",
    "        return author\n",
    "    \n",
    "    while( author[0] == ' ' and len(author) > 0):\n",
    "        author = author[1:]\n",
    "    while( author[-1] == ' ' and len(author) > 0):\n",
    "        author = author[:-1]\n",
    "    \n",
    "    author = author.replace('.', '. ')\n",
    "    author = author.replace('.  ', '. ')\n",
    "    name_to_keep = author.split(' ')[0][0] + '. ' + author.split(' ')[-1]\n",
    "\n",
    "    return name_to_keep\n",
    "\n",
    "# Transform concatenated names of authors to a list of authors \n",
    "list_authors = []\n",
    "list_universities = []\n",
    "\n",
    "info['authors'] = info['authors'].replace(np.nan, 'missing')\n",
    "for authors in info['authors']:\n",
    "    if authors != 'missing':\n",
    "        ### split the different authors\n",
    "        authors = authors.lower()\n",
    "        \n",
    "        ### Find the universities included in the name\n",
    "        universities = []\n",
    "        authors, universities = universities_to_keep(authors, universities)\n",
    "        \n",
    "        ### Split the authors\n",
    "        authors = re.split(',|&', authors)\n",
    "        \n",
    "        ### For each author, check if university, and store it. Also, keep just the names (To be improved)\n",
    "        authors_in_article = []      \n",
    "        for author in authors:\n",
    "            if author != ' ':\n",
    "                authors_in_article.append(name_to_keep(author))\n",
    "            \n",
    "        list_universities.append(universities)\n",
    "        list_authors.append(authors_in_article)\n",
    "    else:\n",
    "        list_universities.append(['missing'])\n",
    "        list_authors.append(['missing'])   \n",
    "        \n",
    "info['authors'] = list_authors\n",
    "info['universities'] = list_universities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topologic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_graph(X_train, y_train, X_test):\n",
    "    X_train = pd.concat([X_train, y_train], axis = 1)\n",
    "    X_train = X_train.values\n",
    "    G = nx.DiGraph()\n",
    "    for i in range(X_train.shape[0]):\n",
    "        source = X_train[i,0]\n",
    "        target = X_train[i,1]\n",
    "        G.add_node(source)\n",
    "        G.add_node(target)\n",
    "        if X_train[i,2] == 1:\n",
    "            G.add_edge(source,target)\n",
    "            \n",
    "    X_test = X_test.values\n",
    "    for i in range(X_test.shape[0]):\n",
    "        source = X_test[i,0]\n",
    "        target = X_test[i,1]\n",
    "        G.add_node(source)\n",
    "        G.add_node(target)\n",
    "        \n",
    "    return G  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G = make_graph(X_train, y_train, X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_betweeness_array(X, graph):    \n",
    "    centrality = nx.degree_centrality(graph)  \n",
    "    centr = [centrality[x[0]] - centrality[x[1]] for x in X[:]]\n",
    "    return np.array(centr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_common_neighbors(X, G): \n",
    "    G2 = G.to_undirected()\n",
    "    common_neighbors = [len(set(G2.neighbors(x[0])).intersection(G2.neighbors(x[1]))) for x in X[:]]\n",
    "    return np.array(common_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_jaccard(X, G):   \n",
    "    total_jaccard = nx.jaccard_coefficient(G.to_undirected(), [(x[0],x[1]) for x in X[:]])\n",
    "    jaccard = [jac for u, v, jac in total_jaccard]\n",
    "    return np.array(jaccard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Difference in inlinks between papers\n",
    "def compute_diff_inlinks(X, graph):\n",
    "    in_degrees=graph.in_degree()\n",
    "    diff_deg = [in_degrees[x[1]] - in_degrees[x[0]] for x in X[:]]\n",
    "    to_deg = [in_degrees[x[1]] for x in X[:]]\n",
    "    return diff_deg, to_deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_topologic_features(X, G):\n",
    "    X_ = X.copy()\n",
    "    X = X.values\n",
    "    \n",
    "    X_['Betweeness centrality'] = compute_betweeness_array(X, G)\n",
    "    X_['Number common neighbours'] = make_common_neighbors(X, G)\n",
    "    X_['Jaccard coefficienf'] = make_jaccard(X, G)\n",
    "    diff_deg, to_deg = compute_diff_inlinks(X, G)\n",
    "    X_['Difference in inlinks coefficient'] = diff_deg\n",
    "    X_[\"Number of times to cited\"] = to_deg\n",
    "    \n",
    "    return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_topologic_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9f6fd45bba81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'X_train = create_topologic_features(X_train, G)\\nX_test = create_topologic_features(X_test, G)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2118\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2120\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2121\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m             \u001b[0;32mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_topologic_features' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = create_topologic_features(X_train, G)\n",
    "X_test = create_topologic_features(X_test, G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Semantic features\n",
    "- Cosine similarity within the titles as tf-idf\n",
    "- Cosine similarity within the abstracts as tf-idf\n",
    "- Cosine similarity within the titles as word2vec\n",
    "- Cosine similarity within the abstracts as word2vec\n",
    "\n",
    "### To try\n",
    "- Difference cosine similarities?\n",
    "- Keep the stopwords or not?\n",
    "- Stemmise the words of not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import various modules for string cleaning\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def text_to_wordlist(text, remove_stopwords=False ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    text = BeautifulSoup(text).get_text()\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "    text = re.sub(\"[\\W]\",\" \", text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/Flukmacdesof/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download the punkt tokenizer for sentence splitting\n",
    "import nltk.data\n",
    "nltk.download(\"punkt\")   \n",
    "\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Define a function to split a review into parsed sentences\n",
    "def text_to_sentences(text, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a text into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(text.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append(text_to_wordlist(raw_sentence, remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    }
   ],
   "source": [
    "train = False\n",
    "sentences = []  # Initialize an empty list of sentences\n",
    "word_list_abstract=pd.Series(index=info.index,dtype=np.str)\n",
    "word_list_title=pd.Series(index=info.index,dtype=np.str)\n",
    "print \"Parsing sentences from training set\"\n",
    "for idx in info.index:\n",
    "    w_list_a = text_to_sentences(info.loc[idx,\"abstract\"], tokenizer)\n",
    "    sentences += w_list_a\n",
    "    word_list_abstract[idx]= w_list_a\n",
    "    w_list_t = text_to_sentences(info.loc[idx,\"title\"], tokenizer)\n",
    "    sentences += w_list_t\n",
    "    word_list_title[idx]= w_list_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim.models import Word2Vec\n",
    "num_features = 200    # Word vector dimensionality  \n",
    "if train:\n",
    "    # Import the built-in logging module and configure it so that Word2Vec \n",
    "    # creates nice output messages\n",
    "    import logging\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "    # Set values for various parameters\n",
    "                    \n",
    "    min_word_count = 10   # Minimum word count                        \n",
    "    num_workers = 4       # Number of threads to run in parallel\n",
    "    context = 10          # Context window size                                                                                    \n",
    "    downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "    # Initialize and train the model (this will take some time)\n",
    "\n",
    "    print \"Training model...\"\n",
    "    model = word2vec.Word2Vec(\n",
    "        sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling\n",
    "    )\n",
    "\n",
    "    # If you don't plan to train the model any further, calling \n",
    "    # init_sims will make the model much more memory-efficient.\n",
    "    model.init_sims(replace=True)\n",
    "\n",
    "    # It can be helpful to create a meaningful model name and \n",
    "    # save the model for later use. You can load it later using Word2Vec.load()\n",
    "    model_name = \"200features_10minwords_10context\"\n",
    "    model.save(model_name)\n",
    "else:\n",
    "    model = Word2Vec.load(\"200features_10minwords_10context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(list_of_words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in list_of_words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(text_list, model, num_features):\n",
    "    # Given a set of texts (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    textFeatureVecs = np.zeros((len(text_list),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for list_of_words in text_list:\n",
    "        # Print a status message every 1000th review\n",
    "        \"\"\"\n",
    "        if counter%1000. == 0.:\n",
    "            print \"Review %d of %d\" % (counter, len(text_list))\n",
    "        \"\"\"\n",
    "        # Call the function (defined above) that makes average feature vectors\n",
    "        textFeatureVecs[counter] = makeFeatureVec(list_of_words, model, num_features)\n",
    "\n",
    "        # Increment the counter\n",
    "        counter = counter + 1.\n",
    "    return textFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:44: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "centroid_title = pd.DataFrame(getAvgFeatureVecs(info.title.values, model, num_features), index= info.index)\n",
    "centroid_abstract = pd.DataFrame(getAvgFeatureVecs(info.abstract.values, model, num_features), index= info.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for K Means clustering:  218.334144831 seconds.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "start = time.time() # Start time\n",
    "\n",
    "# Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an\n",
    "# average of 5 words per cluster\n",
    "word_vectors = model.syn0\n",
    "num_clusters = word_vectors.shape[0] / 5\n",
    "\n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "kmeans_clustering = KMeans( n_clusters = num_clusters )\n",
    "idx = kmeans_clustering.fit_predict( word_vectors )\n",
    "\n",
    "# Get the end time and print how long the process took\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print \"Time taken for K Means clustering: \", elapsed, \"seconds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "# a cluster number                                                                                            \n",
    "word_centroid_map = dict(zip( model.index2word, idx ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "[u'electro', u'dipole', u'magnetic', u'moment', u'flux', u'electric']\n",
      "\n",
      "Cluster 1\n",
      "[u'final', u'submitted', u'unnecessary', u'adds', u'accepted']\n",
      "\n",
      "Cluster 2\n",
      "[u'parametrisation', u'vielbeins', u'span', u'tri', u'biconformal', u'sheaf', u'cartesian', u'encoding', u'uller', u'restricting', u'finsler', u'connectivity', u'teichm', u'hkt', u'foliation']\n",
      "\n",
      "Cluster 3\n",
      "[u'equality', u'lack', u'violations']\n",
      "\n",
      "Cluster 4\n",
      "[u'diverges', u'stays', u'dominates', u'independently', u'hagedorn', u'falls']\n",
      "\n",
      "Cluster 5\n",
      "[u'clarifies', u'motivate', u'emphasizes', u'discusses']\n",
      "\n",
      "Cluster 6\n",
      "[u'unavoidable', u'nor', u'neither', u'merely', u'consequently', u'identically']\n",
      "\n",
      "Cluster 7\n",
      "[u'matroid', u'enumerative', u'facilitate', u'older', u'he', u'theorists', u'companion', u'twenty', u'reverse', u'exploration', u'contradict', u'aims', u'mathematicians']\n",
      "\n",
      "Cluster 8\n",
      "[u'summed', u'checking', u'computable', u'remarkably']\n",
      "\n",
      "Cluster 9\n",
      "[u'canonical', u'bogoliubov', u'hamiltonian']\n",
      "\n",
      "Cluster 10\n",
      "[u'past', u'distortion', u'wormholes', u'universes', u'horizons', u'godel']\n",
      "\n",
      "Cluster 11\n",
      "[u'leea', u'un', u'da']\n",
      "\n",
      "Cluster 12\n",
      "[u'sutherland', u'trigonometric', u'calogero', u'integrable', u'gaudin', u'moser']\n",
      "\n",
      "Cluster 13\n",
      "[u'kikkawa', u'city', u'grossmann', u'november', u'seoul', u'erice', u'torino', u'pascos', u'karpacz', u'southern', u'nov', u'gravitation', u'brussels', u'beijing', u'villa', u'reviews', u'marcel', u'portugal', u'minnesota', u'mumbai', u'como', u'minneapolis', u'usc']\n",
      "\n",
      "Cluster 14\n",
      "[u'treatments', u'approaches']\n",
      "\n",
      "Cluster 15\n",
      "[u'leq', u'ge', u'perp', u'geq', u'le']\n",
      "\n",
      "Cluster 16\n",
      "[u'obeying', u'satisfy', u'satisfying', u'obey']\n",
      "\n",
      "Cluster 17\n",
      "[u'elimination', u'assignment']\n",
      "\n",
      "Cluster 18\n",
      "[u'allowed', u'restricted', u'each']\n",
      "\n",
      "Cluster 19\n",
      "[u'presumably']\n",
      "\n",
      "Cluster 20\n",
      "[u'fourfolds']\n",
      "\n",
      "Cluster 21\n",
      "[u'nonstandard', u'molecule', u'mkdv', u'volterra', u'dispersionless', u'submodel', u'bernard', u'isomonodromic', u'hirota', u'skp']\n",
      "\n",
      "Cluster 22\n",
      "[u'patches', u'let', u'could', u'should', u'can', u'cannot', u'may', u'would', u'must', u'might', u'will']\n",
      "\n",
      "Cluster 23\n",
      "[u'roll', u'wavelengths', u'hot', u'slow', u'typical', u'rolling', u'during', u'thermally', u'reheating', u'cosmic', u'against', u'microwave', u'acceptable', u'amplified', u'inflaton', u'interbrane', u'drastically', u'primordial']\n",
      "\n",
      "Cluster 24\n",
      "[u'structure', u'structures', u'doubles']\n",
      "\n",
      "Cluster 25\n",
      "[u'twisting', u'wilczek', u'schwimmer']\n",
      "\n",
      "Cluster 26\n",
      "[u'wrap', u'collapsed', u'intersections', u'lifts', u'multiply', u'intersect']\n",
      "\n",
      "Cluster 27\n",
      "[u'characterization']\n",
      "\n",
      "Cluster 28\n",
      "[u'composed', u'consisting']\n",
      "\n",
      "Cluster 29\n",
      "[u'prove', u'show', u'find', u'demonstrate']\n",
      "\n",
      "Cluster 30\n",
      "[u'studying', u'analyzing', u'examining', u'considering', u'investigating', u'identifying', u'comparing']\n",
      "\n",
      "Cluster 31\n",
      "[u'average', u'trajectory', u'probability', u'distribution', u'ensemble', u'frequencies', u'wavefunction']\n",
      "\n",
      "Cluster 32\n",
      "[u'displays', u'predicts', u'takes', u'exhibits', u'possesses', u'acquires']\n",
      "\n",
      "Cluster 33\n",
      "[u'solved', u'studied', u'analyzed', u'investigated', u'examined', u'reconsidered', u'considered', u'analysed']\n",
      "\n",
      "Cluster 34\n",
      "[u'multiplets', u'superfields', u'compensator', u'superfield', u'supermultiplet', u'multiplet']\n",
      "\n",
      "Cluster 35\n",
      "[u'easier', u'easy', u'challenge', u'transparent', u'impossible', u'clear', u'useful', u'designed', u'promising', u'difficult']\n",
      "\n",
      "Cluster 36\n",
      "[u'played', u'raised', u'motivated', u'inspired', u'covered']\n",
      "\n",
      "Cluster 37\n",
      "[u'wiechert', u'sd', u'polarisation', u'producing', u'dynamic', u'simultaneous', u'abnormal', u'antiparticles', u'aforementioned', u'conjugated', u'valence', u'screens', u'pct', u'fulfilling', u'hbox']\n",
      "\n",
      "Cluster 38\n",
      "[u'experimentally', u'neutrino', u'experiments', u'measurements', u'exciting', u'experiment', u'nucleosynthesis', u'thresholds', u'lifetime', u'significant', u'limited', u'distinctive']\n",
      "\n",
      "Cluster 39\n",
      "[u'regularize', u'simplify', u'incorporate', u'avoid', u'render', u'modify', u'control', u'maintain', u'isolate', u'cure', u'add', u'keep', u'improve']\n",
      "\n",
      "Cluster 40\n",
      "[u'combinatorial', u'direct']\n",
      "\n",
      "Cluster 41\n",
      "[u'figs', u'inserted', u'uuencoded', u'espcrc2', u'package', u'ftp', u'sprocl', u'format', u'uu', u'bibliography', u'compressed', u'anonymous', u'ams', u'psfig', u'mathematica', u'1980', u'phyzzx', u'tables', u'notation', u'1987', u'cls', u'deleted', u'65', u'epsfig', u'latex2e', u'tar']\n",
      "\n",
      "Cluster 42\n",
      "[u'veneziano', u'oppenheimer', u'yankielowicz']\n",
      "\n",
      "Cluster 43\n",
      "[u'rm', u'tilde', u'vec', u'prime', u'ab', u'exp', u'vp', u'rangle', u'omega', u'lambda', u'xi', u'psi', u'rho', u'bar', u'ij', u'det', u'chi', u'4x', u'mn', u'mu', u'nu', u'cdot', u'pm', u'gamma']\n",
      "\n",
      "Cluster 44\n",
      "[u'resulted', u'aside', u'standpoint', u'inferred', u'viewpoints', u'extracting', u'retrieved', u'proceeding', u'originated', u'stemming']\n",
      "\n",
      "Cluster 45\n",
      "[u'locally', u'globally', u'symmetric', u'ricci', u'maximally']\n",
      "\n",
      "Cluster 46\n",
      "[u'kastler', u'celebrated', u'hurwitz', u'dr', u'markov', u'schmidt', u'par']\n",
      "\n",
      "Cluster 47\n",
      "[u'vacua', u'domain', u'walls', u'wall']\n",
      "\n",
      "Cluster 48\n",
      "[u'pion', u'meson', u'masses', u'glueball', u'lightest', u'gluonic']\n",
      "\n",
      "Cluster 49\n",
      "[u'orientable', u'differentiable', u'perturbatively', u'summable', u'compact', u'decomposable', u'planar', u'contractible', u'factorizable', u'kahlerian', u'normalizable']\n"
     ]
    }
   ],
   "source": [
    "# For the first 10 clusters\n",
    "for cluster in xrange(0,50):\n",
    "    #\n",
    "    # Print the cluster number  \n",
    "    print \"\\nCluster %d\" % cluster\n",
    "    #\n",
    "    # Find all of the words for that cluster number, and print them out\n",
    "    words = []\n",
    "    for i in xrange(0,len(word_centroid_map.values())):\n",
    "        if( word_centroid_map.values()[i] == cluster ):\n",
    "            words.append(word_centroid_map.keys()[i])\n",
    "    print words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bag_of_centroids( wordlist, word_centroid_map ):\n",
    "    #\n",
    "    # The number of clusters is equal to the highest cluster index\n",
    "    # in the word / centroid map\n",
    "    num_centroids = max( word_centroid_map.values() ) + 1\n",
    "    #\n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
    "    #\n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count \n",
    "    # by one\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    #\n",
    "    # Return the \"bag of centroids\"\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pre-allocate an array for the training set bags of centroids (for speed)\n",
    "centroids_abstract = {}\n",
    "\n",
    "# Transform the training set reviews into bags of centroids\n",
    "for review, idx in zip(word_list_abstract, word_list_abstract.index):\n",
    "    centroids_abstract[idx] = create_bag_of_centroids( review[0], word_centroid_map )/len(review[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def compute_cosine_bag_centroids(centroids_dic, X):\n",
    "    bag=[cosine_similarity(centroids_dic[x[0]], centroids_dic[x[1]]) for x in X.iterrows()]\n",
    "    return np.array(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "449286",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-5814be5ba259>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompute_cosine_bag_centroids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcentroids_abstract\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-103-d31d814fc1fb>\u001b[0m in \u001b[0;36mcompute_cosine_bag_centroids\u001b[0;34m(centroids_dic, X)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_cosine_bag_centroids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcentroids_dic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mbag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcentroids_dic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcentroids_dic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 449286"
     ]
    }
   ],
   "source": [
    "compute_cosine_bag_centroids(centroids_abstract, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.55450016]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity(centroids_abstract[0],centroids_abstract[9191]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def cosine_sim_centroid(X, centroid_array):\n",
    "    cos_sim=[cosine_similarity(\n",
    "            centroid_array.loc[int(row[0])].reshape(-1,1), \n",
    "            centroid_array.loc[int(row[1])].reshape(-1,1)\n",
    "        ) for idx, row in X[[0,1]].iterrows()]\n",
    "    return np.array(cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cos_sim=cosine_sim_centroid(X_train, centroid_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-60b725f10c9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'bang', 0.9764775037765503),\n",
       " (u'pre', 0.9163153171539307),\n",
       " (u'crunch', 0.8864502906799316),\n",
       " (u'inflationary', 0.8362299799919128),\n",
       " (u'inflation', 0.8288743495941162),\n",
       " (u'exit', 0.8192932605743408),\n",
       " (u'graceful', 0.8099927306175232),\n",
       " (u'ekpyrotic', 0.8094025254249573),\n",
       " (u'epoch', 0.8060178756713867),\n",
       " (u'accelerating', 0.796191394329071)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"big\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Attribute features\n",
    "\n",
    "- Difference in publication year\n",
    "- Number of common authors\n",
    "- Self-citation\n",
    "- Same journal\n",
    "- Number of times \"to\" cited (Attraction of the \"to\" paper)\n",
    "\n",
    "### To try\n",
    "- Number of times each author of \"to\" cited [Sum of these number of times] ?\n",
    "- Number of times each journal cited?\n",
    "- Number of same university??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def numb_same_authors(authors1, authors2):\n",
    "    total = 0\n",
    "    for author in authors1:\n",
    "        if author in authors2:\n",
    "            total += 1\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_attribute_features(X):\n",
    "    length = X.shape[0]\n",
    "    difference_publication_year = np.zeros(length)\n",
    "    number_same_authors = np.zeros(length)\n",
    "    self_citation = [False for x in range(length)]\n",
    "    same_journal = [False for x in range(length)]\n",
    "    \n",
    "    ### NOT TO OVERFIT WHEN COUNTING : THE INFORMATION ARE AVAILABLE ONLY FOR X_TRAIN ###\n",
    "    number_times = X_train.groupby(1).count()\n",
    "    \n",
    "    i=-1\n",
    "    for idx, row in X.iterrows():\n",
    "        i += 1\n",
    "        ID1 = int(row[0])\n",
    "        ID2 = int(row[1])\n",
    "        \n",
    "        source = info.loc[ID1]\n",
    "        target = info.loc[ID2]\n",
    "        \n",
    "        ### Difference in publication year\n",
    "        difference_publication_year[i] = source['year'] - target['year']\n",
    "        \n",
    "        ### Number of same authors\n",
    "        common_authors = numb_same_authors(source['authors'], target['authors'])\n",
    "        number_same_authors[i] = common_authors\n",
    "        \n",
    "        ### Self citation ###\n",
    "        if common_authors >= 1:\n",
    "            self_citation[i] = True\n",
    "            \n",
    "        ### Same journal ###\n",
    "        if source['journal'] == target['journal']:\n",
    "            same_journal[i] = True\n",
    "            \n",
    "        \n",
    "    X_ = X.copy()\n",
    "    X_['Diff publication'] = difference_publication_year\n",
    "    X_['Number same authors'] = number_same_authors\n",
    "    X_['Self citation'] = self_citation\n",
    "    X_['Same journal'] = same_journal\n",
    "    return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = create_attribute_features(X_train)\n",
    "X_test = create_attribute_features(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>Betweeness centrality</th>\n",
       "      <th>Number common neighbours</th>\n",
       "      <th>Jaccard coefficienf</th>\n",
       "      <th>Difference in inlinks coefficient</th>\n",
       "      <th>Number of times to cited</th>\n",
       "      <th>Diff publication</th>\n",
       "      <th>Number same authors</th>\n",
       "      <th>Self citation</th>\n",
       "      <th>Same journal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>450485</th>\n",
       "      <td>3299</td>\n",
       "      <td>3256</td>\n",
       "      <td>-0.000648</td>\n",
       "      <td>2</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408211</th>\n",
       "      <td>105023</td>\n",
       "      <td>9601070</td>\n",
       "      <td>0.001837</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-29</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218450</th>\n",
       "      <td>105302</td>\n",
       "      <td>9511204</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20350</th>\n",
       "      <td>202022</td>\n",
       "      <td>9309075</td>\n",
       "      <td>-0.000972</td>\n",
       "      <td>11</td>\n",
       "      <td>0.134146</td>\n",
       "      <td>55</td>\n",
       "      <td>57</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481477</th>\n",
       "      <td>9807077</td>\n",
       "      <td>9510025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0        1  Betweeness centrality  Number common neighbours  \\\n",
       "450485     3299     3256              -0.000648                         2   \n",
       "408211   105023  9601070               0.001837                         0   \n",
       "218450   105302  9511204               0.000216                         0   \n",
       "20350    202022  9309075              -0.000972                        11   \n",
       "481477  9807077  9510025               0.000000                         0   \n",
       "\n",
       "        Jaccard coefficienf  Difference in inlinks coefficient  \\\n",
       "450485             0.040000                                  9   \n",
       "408211             0.000000                                -29   \n",
       "218450             0.000000                                 15   \n",
       "20350              0.134146                                 55   \n",
       "481477             0.000000                                  0   \n",
       "\n",
       "        Number of times to cited  Diff publication  Number same authors  \\\n",
       "450485                        12                 0                    0   \n",
       "408211                         5                 5                    0   \n",
       "218450                        16                 6                    0   \n",
       "20350                         57                 9                    0   \n",
       "481477                         0                 3                    0   \n",
       "\n",
       "       Self citation Same journal  \n",
       "450485         False        False  \n",
       "408211         False        False  \n",
       "218450         False        False  \n",
       "20350          False        False  \n",
       "481477         False        False  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>Betweeness centrality</th>\n",
       "      <th>Number common neighbours</th>\n",
       "      <th>Jaccard coefficienf</th>\n",
       "      <th>Difference in inlinks coefficient</th>\n",
       "      <th>Number of times to cited</th>\n",
       "      <th>Diff publication</th>\n",
       "      <th>Number same authors</th>\n",
       "      <th>Self citation</th>\n",
       "      <th>Same journal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>416513</th>\n",
       "      <td>101064</td>\n",
       "      <td>5163</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>4</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517498</th>\n",
       "      <td>12135</td>\n",
       "      <td>9802109</td>\n",
       "      <td>-0.044258</td>\n",
       "      <td>19</td>\n",
       "      <td>0.014961</td>\n",
       "      <td>1245</td>\n",
       "      <td>1249</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315011</th>\n",
       "      <td>9903196</td>\n",
       "      <td>9806074</td>\n",
       "      <td>-0.002377</td>\n",
       "      <td>37</td>\n",
       "      <td>0.213873</td>\n",
       "      <td>79</td>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614966</th>\n",
       "      <td>9607008</td>\n",
       "      <td>9308136</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185036</th>\n",
       "      <td>2028</td>\n",
       "      <td>9805105</td>\n",
       "      <td>-0.000612</td>\n",
       "      <td>6</td>\n",
       "      <td>0.070588</td>\n",
       "      <td>32</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0        1  Betweeness centrality  Number common neighbours  \\\n",
       "416513   101064     5163               0.000072                         4   \n",
       "517498    12135  9802109              -0.044258                        19   \n",
       "315011  9903196  9806074              -0.002377                        37   \n",
       "614966  9607008  9308136              -0.000036                         1   \n",
       "185036     2028  9805105              -0.000612                         6   \n",
       "\n",
       "        Jaccard coefficienf  Difference in inlinks coefficient  \\\n",
       "416513             0.111111                                 13   \n",
       "517498             0.014961                               1245   \n",
       "315011             0.213873                                 79   \n",
       "614966             0.100000                                  5   \n",
       "185036             0.070588                                 32   \n",
       "\n",
       "        Number of times to cited  Diff publication  Number same authors  \\\n",
       "416513                        13                 1                    1   \n",
       "517498                      1249                 3                    0   \n",
       "315011                       125                 1                    0   \n",
       "614966                         6                 3                    0   \n",
       "185036                        46                 2                    1   \n",
       "\n",
       "       Self citation Same journal  \n",
       "416513          True        False  \n",
       "517498         False        False  \n",
       "315011         False        False  \n",
       "614966         False        False  \n",
       "185036          True         True  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(pred, real):\n",
    "    tot = 0\n",
    "    for i, val in enumerate(real):\n",
    "        if pred[i] == val:\n",
    "            tot += 1\n",
    "    return float(tot)/len(real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.939603421525\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators = 100)\n",
    "rfc.fit(X_train.drop([0,1], axis = 1), y_train)\n",
    "pred = rfc.predict(X_test.drop([0,1], axis = 1))\n",
    "\n",
    "print score(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
