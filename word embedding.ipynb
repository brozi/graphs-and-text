{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/bat/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import random\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cosine\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "my_p = re.compile(r\"(\\b[-']\\b)|[\\W_]\")\n",
    "\n",
    "######################################\n",
    "# experimenting with word embeddings #\n",
    "######################################\n",
    "\n",
    "with open(\"word_vectors_lab5.csv\", \"r\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    word_vectors  = list(reader)\n",
    "\n",
    "# the first entry of each sublist is the word\n",
    "# the 300 following entries are the coordinates of the word in the embedding space\n",
    "\n",
    "# separate the words from their coordinates\n",
    "words = []\n",
    "# coordinates is a list of list representation of observations/dimensions data frame\n",
    "coordinates = []\n",
    "\n",
    "for vector in word_vectors:\n",
    "    words.append(vector[0])\n",
    "    coordinates.append(vector[1:301])\n",
    "\n",
    "coordinates = np.array(coordinates).astype(np.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7664\n",
      "0.0166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bat/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "/home/bat/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "/home/bat/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "/home/bat/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# function that returns the word vector as numpy array\n",
    "def my_vector_getter(word, my_coordinates):\n",
    "    index = words.index(word)\n",
    "    word_array = my_coordinates[index].ravel()\n",
    "    return (word_array)\n",
    "\n",
    "# function that returns cosine similarity between two word vectors\n",
    "def my_cos_similarity(word1, word2, my_coordinates):\n",
    "    sim = cosine(my_vector_getter(word1, my_coordinates),my_vector_getter(word2, my_coordinates)) \n",
    "    return (round(sim, 4))\n",
    "\n",
    "# two similar words\n",
    "print my_cos_similarity(\"man\",\"woman\", coordinates)\n",
    "\n",
    "# two dissimilar words\n",
    "print my_cos_similarity(\"man\",\"however\", coordinates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24559, 300)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coordinates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bat/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "/home/bat/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components= 2)\n",
    "new_coordinates = pca.fit_transform(coordinates)\n",
    "# for visualization, project the words into a lower-dimensional space using PCA\n",
    "# store the results in an array called new_coordinates\n",
    "\n",
    "# examples of concepts captured in the embedding space:\n",
    "\n",
    "# country-capital\n",
    "France = my_vector_getter(\"France\", new_coordinates)\n",
    "Paris = my_vector_getter(\"Paris\", new_coordinates)\n",
    "Germany = my_vector_getter(\"Germany\", new_coordinates)\n",
    "Berlin = my_vector_getter(\"Berlin\", new_coordinates)\n",
    "\n",
    "operation = France - Paris + Berlin\n",
    "round(cosine(operation, Germany),5)\n",
    "\n",
    "# visual inspection\n",
    "dim_1_coords = [element[0] for element in [France, Paris, Germany, Berlin]]\n",
    "dim_2_coords = [element[1] for element in [France, Paris, Germany, Berlin]]\n",
    "\n",
    "names = [\"France\", \"Paris\", \"Germany\", \"Berlin\"]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(dim_1_coords, dim_2_coords, \"ro\")\n",
    "plt.axis([-0.4, 0, -0.3, 0])\n",
    "\n",
    "for x, y, name in zip(dim_1_coords , dim_2_coords, names):                                                \n",
    "    ax.annotate(name, xy=(x, y))\n",
    "\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "representation of ' ['Kennedy', 'shot', 'dead', 'Dallas'] ' : \n",
      "[('shot', 1), ('Texas', 0), ('Kennedy', 1), ('President', 0), ('Dallas', 1), ('killed', 0), ('dead', 1)]\n",
      "representation of ' ['President', 'killed', 'Texas'] ' : \n",
      "[('shot', 0), ('Texas', 1), ('Kennedy', 0), ('President', 1), ('Dallas', 0), ('killed', 1), ('dead', 0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bat/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "/home/bat/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# adjective-superlative\n",
    "biggest = my_vector_getter(\"biggest\", new_coordinates)\n",
    "big = my_vector_getter(\"big\", new_coordinates)\n",
    "small = my_vector_getter(\"small\", new_coordinates)\n",
    "smallest = my_vector_getter(\"smallest\", new_coordinates)\n",
    "\n",
    "operation = biggest - big + small\n",
    "round(cosine(operation, smallest),5)\n",
    "\n",
    "# visual inspection\n",
    "# repeat the same steps as for the country-capital task\n",
    "\n",
    "s_1 = \"Kennedy was shot dead in Dallas\"\n",
    "s_2 = \"The President was killed in Texas\"\n",
    "\n",
    "# compute the features of the BOW space (unique common non-stopwords)\n",
    "\n",
    "# remove stopwords\n",
    "s_1 = [word for word in s_1.split(\" \") if word.lower() not in stpwds]\n",
    "s_2 = [word for word in s_2.split(\" \") if word.lower() not in stpwds]\n",
    "\n",
    "# the features are all the unique remaining words\n",
    "features = list(set(s_1).union(set(s_2)))\n",
    "\n",
    "# project the two sentences in the BOW space\n",
    "p_1 = []\n",
    "for feature in features:\n",
    "    if feature in s_1:\n",
    "        p_1.append(1)\n",
    "    else:\n",
    "        p_1.append(0)\n",
    "\n",
    "# repeat the steps above for the second sentence\n",
    "p_2 = []\n",
    "for feature in features:\n",
    "    if feature in s_2:\n",
    "        p_2.append(1)\n",
    "    else:\n",
    "        p_2.append(0)\n",
    "\n",
    "p_1_bow = zip(features, p_1)\n",
    "p_2_bow = zip(features, p_2)\n",
    "\n",
    "print \"representation of '\", s_1, \"' : \\n\",\n",
    "print p_1_bow\n",
    "\n",
    "print \"representation of '\", s_2, \"' : \\n\",\n",
    "print p_2_bow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bat/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "/home/bat/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) compute the similarity of these two sentences in the BOW space\n",
    "# of course, the two sentences have zero similarity since the dot product of the two BOW vectors is equal to zero (the two vectors are orthogonal)\n",
    "\n",
    "round(cosine(p_1,p_2), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bat/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "/home/bat/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5175"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we use the word embedding space\n",
    "\n",
    "p_1_words = [element[0] for element in p_1_bow if element[1]==1]\n",
    "\n",
    "# the sentence can be represented as a two-dimensional array (words as rows, dimensions as columns)\n",
    "p_1_embeddings = np.array([my_vector_getter(word, coordinates) for word in p_1_words])\n",
    "\n",
    "# compute centroid of cloud of words\n",
    "centroid_1 = np.mean(p_1_embeddings, axis=0)\n",
    "\n",
    "# repeat the same steps for the second sentence\n",
    "p_2_words = [element[0] for element in p_2_bow if element[1]==1]\n",
    "p_2_embeddings = np.array([my_vector_getter(word, coordinates) for word in p_2_words])\n",
    "centroid_2 = np.mean(p_2_embeddings, axis=0)\n",
    "# 2) compute cosine similarity between sentence centroids\n",
    "# this time we can see that the semantic similarity between the two sentences is captured\n",
    "round(cosine(centroid_1, centroid_2),4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named pyemd",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-e55d331c2dd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyemd\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0memd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m##################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# classification #\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m##################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named pyemd"
     ]
    }
   ],
   "source": [
    "from pyemd import emd\n",
    "##################\n",
    "# classification #\n",
    "##################\n",
    "\n",
    "# *** preliminary ***\n",
    "# to get familiar with the WMD, compute it for the two previous sentences\n",
    "\n",
    "# each sentence is viewed as a normalized BOW vector\n",
    "p_1_norm = np.true_divide(p_1, np.sum(p_1))\n",
    "p_2_norm = np.true_divide(p_2, np.sum(p_2))\n",
    "\n",
    "# select the embeddings of the features\n",
    "index_union_words = np.array([words.index(word_temp) for word_temp in features])\n",
    "coordinates_reduced = coordinates[index_union_words]\n",
    "\n",
    "# compute euclidean distances between the remaining words\n",
    "D = euclidean_distances(coordinates_reduced)\n",
    "\n",
    "# compute WMD\n",
    "emd(p_1_norm,p_2_norm,D)\n",
    "\n",
    "# *** end preliminary ***\n",
    "\n",
    "# select two categories\n",
    "categories = ['comp.graphics', 'sci.space']\n",
    "\n",
    "# load data set\n",
    "newsgroups = fetch_20newsgroups(subset='test',remove=('headers', 'footers', 'quotes'),categories=categories)\n",
    "documents, labels = newsgroups.data, newsgroups.target\n",
    "\n",
    "# split into very small training and testing sets\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(documents, labels,\n",
    "                                                          train_size=50,\n",
    "                                                          test_size=10)\n",
    "\n",
    "# clean documents\n",
    "\n",
    "clean_docs_train = []\n",
    "\n",
    "for doc in docs_train:\n",
    "\t# remove formatting\n",
    "\tdoc = re.sub(\"\\s+\", \" \", doc)\n",
    "\t# remove all punctuation except intra-word dashes\n",
    "\tdoc = my_p.sub(lambda m: (m.group(1) if m.group(1) else \" \"), doc)\n",
    "\t# remove extra whitespace\n",
    "\tdoc = re.sub(\" +\",\" \", doc)   \n",
    "\t# remove leading and trailing whitespace\n",
    "\tdoc = doc.strip()\n",
    "\t# tokenize\n",
    "\tdoc = doc.split(\" \")\n",
    "\t# remove stopwords\n",
    "\tdoc = [token for token in doc if token not in stpwds]\n",
    "\t# remove tokens less than 2 character in size\n",
    "\tdoc = [token for token in doc if len(token)>2]\n",
    "\tclean_docs_train.append(doc)\n",
    "\n",
    "# remove empty elements\n",
    "clean_docs_train = [element for element in clean_docs_train if len(element)>0]\n",
    "\n",
    "# repeat the same steps for docs_test\n",
    "\n",
    "# nearest neighbors-based predictions of the labels of the documents\n",
    "# in the test set based on word mover and centroid distance\n",
    "\n",
    "# get the list of features (i.e., the unique non-stopwords in the training set)\n",
    "# compute euclidean distances between embeddings corresponding to these features\n",
    "\n",
    "predictions_emd = []\n",
    "predictions_cos = []\n",
    "counter = 0\n",
    "\n",
    "for words_doc_1 in clean_docs_test:\n",
    "    emd_distances_list = []\n",
    "    cosine_distances_list = []\n",
    "    \n",
    "    # only retain the words that have an embedding available\n",
    "    words_doc_1 = [w for w in words_doc_1 if w in words]\n",
    "\t# pull up the word vectors of the words in the document\n",
    "    index_words_doc_1 = [words.index(w) for w in words_doc_1]\n",
    "    doc_1_embeddings = coordinates[index_words_doc_1]\n",
    "\t# compute centroid of word vectors\n",
    "    centroid_1 = np.mean(doc_1_embeddings, axis=0)\n",
    "    \n",
    "\t# project doc_1 in the BOW space\n",
    "    doc_1_bow = []\n",
    "    for feature in features:\n",
    "        if feature in words_doc_1:\n",
    "            doc_1_bow.append(1)\n",
    "        else:\n",
    "            doc_1_bow.append(0)\n",
    "    \n",
    "\t# normalize\n",
    "    doc_1_norm = np.true_divide(doc_1_bow, np.sum(doc_1_bow))  \t\t\t\t\n",
    "    \n",
    "    # compute distances to all the documents in the training set\n",
    "#    for words_doc_2 in clean_docs_train:\n",
    "        \n",
    "\t\t# project doc_2 in the BOW space and normalize it\n",
    "\t\t# compute its centroid\n",
    "\n",
    "# compute emd and cosine centroid distance\n",
    "        # append the values to the \"emd_distances_list\" and \"cosine_distances_list\" lists\n",
    "        \n",
    "# rank lists in ascending or descending order (based on whether it is distance or similarity)\n",
    "# select the top k elements (k nearest neighbors), k is a tuning parameter\n",
    "\t\n",
    "# get predictions as the most common label of the neighbors\n",
    "# store predictions in the \"predictions_emd\" and \"predictions_cos\" lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
