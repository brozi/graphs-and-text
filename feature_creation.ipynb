{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "import csv\n",
    "import pandas as pd\n",
    "from nltk.stem import SnowballStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import re\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "info = pd.read_csv(\n",
    "    \"node_information.csv\", \n",
    "    header= None, \n",
    "    names=[\"Id\", \"year\", \"title\", \"authors\", \"journal\", \"abstract\"],\n",
    "    sep=\",\",\n",
    "    index_col = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27770, 5) (615512, 2) (32648, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.read_csv(\"training_set.txt\", sep=\" \", header=None)\n",
    "X_test = pd.read_csv(\"testing_set.txt\", sep=\" \", header=None)\n",
    "y_train = X_train[2]\n",
    "X_train.drop([2], axis = 1, inplace = True)\n",
    "\n",
    "print info.shape, X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######################\n",
    "### FOR VALIDATION ###\n",
    "######################\n",
    "\n",
    "\n",
    "#####################\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.2)\n",
    "#####################\n",
    "\n",
    "#####################\n",
    "small_portion_to_train = 50000\n",
    "small_portion_to_test  = 5000\n",
    "X_train = X_train[:small_portion_to_train]\n",
    "y_train = y_train[:small_portion_to_train]\n",
    "\n",
    "X_test  = X_test[:small_portion_to_test]\n",
    "y_train = y_test[:small_portion_to_test]\n",
    "#####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- list_authors is the list of authors in the papers\n",
    "- list_universities is the list where the authors are from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def universities_to_keep(authors, universities):\n",
    "    while('(' in authors and ')' in authors):\n",
    "        universities.append( authors[authors.find('(')+1 : authors.find(')')] )\n",
    "        authors = authors[: authors.find('(')] + authors[ authors.find(')')+1 : ]\n",
    "            \n",
    "    if '(' in authors:\n",
    "        universities.append( authors[authors.find('(')+1 : ])\n",
    "        authors = authors[: authors.find('(')]\n",
    "    \n",
    "    return authors, universities\n",
    "\n",
    "\n",
    "def name_to_keep(author):\n",
    "    if len(author.split(' ')) <= 1:\n",
    "        return author\n",
    "    \n",
    "    while( author[0] == ' ' and len(author) > 0):\n",
    "        author = author[1:]\n",
    "    while( author[-1] == ' ' and len(author) > 0):\n",
    "        author = author[:-1]\n",
    "    \n",
    "    author = author.replace('.', '. ')\n",
    "    author = author.replace('.  ', '. ')\n",
    "    name_to_keep = author.split(' ')[0][0] + '. ' + author.split(' ')[-1]\n",
    "\n",
    "    return name_to_keep\n",
    "\n",
    "# Transform concatenated names of authors to a list of authors \n",
    "list_authors = []\n",
    "list_universities = []\n",
    "\n",
    "info['authors'] = info['authors'].replace(np.nan, 'missing')\n",
    "for authors in info['authors']:\n",
    "    if authors != 'missing':\n",
    "        ### split the different authors\n",
    "        authors = authors.lower()\n",
    "        \n",
    "        ### Find the universities included in the name\n",
    "        universities = []\n",
    "        authors, universities = universities_to_keep(authors, universities)\n",
    "        \n",
    "        ### Split the authors\n",
    "        authors = re.split(',|&', authors)\n",
    "        \n",
    "        ### For each author, check if university, and store it. Also, keep just the names (To be improved)\n",
    "        authors_in_article = []      \n",
    "        for author in authors:\n",
    "            if author != ' ':\n",
    "                authors_in_article.append(name_to_keep(author))\n",
    "            \n",
    "        list_universities.append(universities)\n",
    "        list_authors.append(authors_in_article)\n",
    "    else:\n",
    "        list_universities.append(['missing'])\n",
    "        list_authors.append(['missing'])   \n",
    "        \n",
    "info['authors'] = list_authors\n",
    "info['universities'] = list_universities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topologic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_graph(X_train, y_train, X_test):\n",
    "    X_train = pd.concat([X_train, y_train], axis = 1)\n",
    "    X_train = X_train.values\n",
    "    G = nx.DiGraph()\n",
    "    for i in range(X_train.shape[0]):\n",
    "        source = X_train[i,0]\n",
    "        target = X_train[i,1]\n",
    "        G.add_node(source)\n",
    "        G.add_node(target)\n",
    "        if X_train[i,2] == 1:\n",
    "            G.add_edge(source,target)\n",
    "            \n",
    "    X_test = X_test.values\n",
    "    for i in range(X_test.shape[0]):\n",
    "        source = X_test[i,0]\n",
    "        target = X_test[i,1]\n",
    "        G.add_node(source)\n",
    "        G.add_node(target)\n",
    "        \n",
    "    return G  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G = make_graph(X_train, y_train, X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_betweeness_array(X, graph):    \n",
    "    centrality = nx.degree_centrality(graph)  \n",
    "    centr = [centrality[x[0]] - centrality[x[1]] for x in X[:]]\n",
    "    return np.array(centr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_common_neighbors(X, G): \n",
    "    G2 = G.to_undirected()\n",
    "    common_neighbors = [len(set(G2.neighbors(x[0])).intersection(G2.neighbors(x[1]))) for x in X[:]]\n",
    "    return np.array(common_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_jaccard(X, G):   \n",
    "    total_jaccard = nx.jaccard_coefficient(G.to_undirected(), [(x[0],x[1]) for x in X[:]])\n",
    "    jaccard = [jac for u, v, jac in total_jaccard]\n",
    "    return np.array(jaccard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Difference in inlinks between papers\n",
    "def compute_diff_inlinks(X, graph):\n",
    "    in_degrees=graph.in_degree()\n",
    "    diff_deg = [in_degrees[x[1]] - in_degrees[x[0]] for x in X[:]]\n",
    "    to_deg = [in_degrees[x[1]] for x in X[:]]\n",
    "    return diff_deg, to_deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_topologic_features(X, G):\n",
    "    X_ = X.copy()\n",
    "    X = X.values\n",
    "    \n",
    "    X_['Betweeness centrality'] = compute_betweeness_array(X, G)\n",
    "    X_['Number common neighbours'] = make_common_neighbors(X, G)\n",
    "    X_['Jaccard coefficienf'] = make_jaccard(X, G)\n",
    "    diff_deg, to_deg = compute_diff_inlinks(X, G)\n",
    "    X_['Difference in inlinks coefficient'] = diff_deg\n",
    "    X_[\"Number of times to cited\"] = to_deg\n",
    "    \n",
    "    return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = create_topologic_features(X_train, G)\n",
    "X_test = create_topologic_features(X_test, G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Semantic features\n",
    "- Cosine similarity within the titles as tf-idf\n",
    "- Cosine similarity within the abstracts as tf-idf\n",
    "- Cosine similarity within the titles as word2vec\n",
    "- Cosine similarity within the abstracts as word2vec\n",
    "\n",
    "### To try\n",
    "- Difference cosine similarities?\n",
    "- Keep the stopwords or not?\n",
    "- Stemmise the words of not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import various modules for string cleaning\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def text_to_wordlist(text, remove_stopwords=False ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    text = BeautifulSoup(text).get_text()\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "    text = re.sub(\"[\\W]\",\" \", text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/Flukmacdesof/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download the punkt tokenizer for sentence splitting\n",
    "import nltk.data\n",
    "nltk.download(\"punkt\")   \n",
    "\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Define a function to split a review into parsed sentences\n",
    "def text_to_sentences(text, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a text into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(text.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append(text_to_wordlist(raw_sentence, remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "word_list_abstract=pd.Series(index=info.index,dtype=np.str)\n",
    "word_list_title=pd.Series(index=info.index,dtype=np.str)\n",
    "print \"Parsing sentences from training set\"\n",
    "for i, abstract in enumerate(info.abstract):\n",
    "    w_list = text_to_sentences(abstract, tokenizer)\n",
    "    sentences += w_list[0]\n",
    "    word_list_abstract[i]= w_list[0]\n",
    "for i, title in enumerate(info.title):\n",
    "    w_list=text_to_sentences(title, tokenizer)\n",
    "    sentences += w_list[0]\n",
    "    word_list_title[i]= w_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 200    # Word vector dimensionality                      \n",
    "min_word_count = 10   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print \"Training model...\"\n",
    "model = word2vec.Word2Vec(\n",
    "    sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling\n",
    ")\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"200features_10minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(list_of_words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in list_of_words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(text_list, model, num_features):\n",
    "    # Given a set of texts (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    textFeatureVecs = np.zeros((len(text_list),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for list_of_words in text_list:\n",
    "        # Print a status message every 1000th review\n",
    "        if counter%1000. == 0.:\n",
    "            print \"Review %d of %d\" % (counter, len(text_list))\n",
    "        # Call the function (defined above) that makes average feature vectors\n",
    "        textFeatureVecs[counter] = makeFeatureVec(list_of_words, model, num_features)\n",
    "\n",
    "        # Increment the counter\n",
    "        counter = counter + 1.\n",
    "    return textFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:42: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 62\n",
      "Review 2000 of 62\n",
      "Review 3000 of 41\n",
      "Review 4000 of 51\n",
      "Review 5000 of 70\n",
      "Review 6000 of 62\n",
      "Review 7000 of 70\n",
      "Review 8000 of 45\n",
      "Review 9000 of 27\n",
      "Review 10000 of 47\n",
      "Review 11000 of 61\n",
      "Review 12000 of 40\n",
      "Review 13000 of 66\n",
      "Review 14000 of 71\n",
      "Review 15000 of 52\n",
      "Review 16000 of 47\n",
      "Review 17000 of 67\n",
      "Review 18000 of 65\n",
      "Review 19000 of 39\n",
      "Review 20000 of 46\n",
      "Review 21000 of 70\n",
      "Review 22000 of 52\n",
      "Review 23000 of 40\n",
      "Review 24000 of 67\n",
      "Review 25000 of 46\n",
      "Review 26000 of 61\n",
      "Review 27000 of 17\n",
      "Review 0 of 728\n",
      "Review 1000 of 562\n",
      "Review 2000 of 478\n",
      "Review 3000 of 896\n",
      "Review 4000 of 790\n",
      "Review 5000 of 1166\n",
      "Review 6000 of 1022\n",
      "Review 7000 of 894\n",
      "Review 8000 of 224\n",
      "Review 9000 of 1384\n"
     ]
    }
   ],
   "source": [
    "centroid_title = getAvgFeatureVecs(info.title.values, model, num_features)\n",
    "centroid_abstract = getAvgFeatureVecs(info.abstract.values, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import spacial\n",
    "def cosine_sim_centroid(X, centroid_array):\n",
    "    vect_from=centroid_array[x[0]]\n",
    "    vect_to=centroid_array[x[1]]\n",
    "    cos_sim=[spatial.distance.cosine(vect_from[i], vect_to[i]) for i in range(X.shape[0])]\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cosine_sim_centroid(X, centroid_title.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'bang', 0.9764775037765503),\n",
       " (u'pre', 0.9163153171539307),\n",
       " (u'crunch', 0.8864502906799316),\n",
       " (u'inflationary', 0.8362299799919128),\n",
       " (u'inflation', 0.8288743495941162),\n",
       " (u'exit', 0.8192932605743408),\n",
       " (u'graceful', 0.8099927306175232),\n",
       " (u'ekpyrotic', 0.8094025254249573),\n",
       " (u'epoch', 0.8060178756713867),\n",
       " (u'accelerating', 0.796191394329071)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"big\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Attribute features\n",
    "\n",
    "- Difference in publication year\n",
    "- Number of common authors\n",
    "- Self-citation\n",
    "- Same journal\n",
    "- Number of times \"to\" cited (Attraction of the \"to\" paper)\n",
    "\n",
    "### To try\n",
    "- Number of times each author of \"to\" cited [Sum of these number of times] ?\n",
    "- Number of times each journal cited?\n",
    "- Number of same university??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def numb_same_authors(authors1, authors2):\n",
    "    total = 0\n",
    "    for author in authors1:\n",
    "        if author in authors2:\n",
    "            total += 1\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_attribute_features(X):\n",
    "    length = X.shape[0]\n",
    "    difference_publication_year = np.zeros(length)\n",
    "    number_same_authors = np.zeros(length)\n",
    "    self_citation = [False for x in range(length)]\n",
    "    same_journal = [False for x in range(length)]\n",
    "    \n",
    "    ### NOT TO OVERFIT WHEN COUNTING : THE INFORMATION ARE AVAILABLE ONLY FOR X_TRAIN ###\n",
    "    number_times = X_train.groupby(1).count()\n",
    "    \n",
    "    i=-1\n",
    "    for idx, row in X.iterrows():\n",
    "        i += 1\n",
    "        ID1 = int(row[0])\n",
    "        ID2 = int(row[1])\n",
    "        \n",
    "        source = info.loc[ID1]\n",
    "        target = info.loc[ID2]\n",
    "        \n",
    "        ### Difference in publication year\n",
    "        difference_publication_year[i] = source['year'] - target['year']\n",
    "        \n",
    "        ### Number of same authors\n",
    "        common_authors = numb_same_authors(source['authors'], target['authors'])\n",
    "        number_same_authors[i] = common_authors\n",
    "        \n",
    "        ### Self citation ###\n",
    "        if common_authors >= 1:\n",
    "            self_citation[i] = True\n",
    "            \n",
    "        ### Same journal ###\n",
    "        if source['journal'] == target['journal']:\n",
    "            same_journal[i] = True\n",
    "            \n",
    "        \n",
    "    X_ = X.copy()\n",
    "    X_['Diff publication'] = difference_publication_year\n",
    "    X_['Number same authors'] = number_same_authors\n",
    "    X_['Self citation'] = self_citation\n",
    "    X_['Same journal'] = same_journal\n",
    "    return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = create_attribute_features(X_train)\n",
    "X_test = create_attribute_features(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>Betweeness centrality</th>\n",
       "      <th>Number common neighbours</th>\n",
       "      <th>Jaccard coefficienf</th>\n",
       "      <th>Difference in inlinks coefficient</th>\n",
       "      <th>Number of times to cited</th>\n",
       "      <th>Diff publication</th>\n",
       "      <th>Number same authors</th>\n",
       "      <th>Self citation</th>\n",
       "      <th>Same journal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>450485</th>\n",
       "      <td>3299</td>\n",
       "      <td>3256</td>\n",
       "      <td>-0.000648</td>\n",
       "      <td>2</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408211</th>\n",
       "      <td>105023</td>\n",
       "      <td>9601070</td>\n",
       "      <td>0.001837</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-29</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218450</th>\n",
       "      <td>105302</td>\n",
       "      <td>9511204</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20350</th>\n",
       "      <td>202022</td>\n",
       "      <td>9309075</td>\n",
       "      <td>-0.000972</td>\n",
       "      <td>11</td>\n",
       "      <td>0.134146</td>\n",
       "      <td>55</td>\n",
       "      <td>57</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481477</th>\n",
       "      <td>9807077</td>\n",
       "      <td>9510025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0        1  Betweeness centrality  Number common neighbours  \\\n",
       "450485     3299     3256              -0.000648                         2   \n",
       "408211   105023  9601070               0.001837                         0   \n",
       "218450   105302  9511204               0.000216                         0   \n",
       "20350    202022  9309075              -0.000972                        11   \n",
       "481477  9807077  9510025               0.000000                         0   \n",
       "\n",
       "        Jaccard coefficienf  Difference in inlinks coefficient  \\\n",
       "450485             0.040000                                  9   \n",
       "408211             0.000000                                -29   \n",
       "218450             0.000000                                 15   \n",
       "20350              0.134146                                 55   \n",
       "481477             0.000000                                  0   \n",
       "\n",
       "        Number of times to cited  Diff publication  Number same authors  \\\n",
       "450485                        12                 0                    0   \n",
       "408211                         5                 5                    0   \n",
       "218450                        16                 6                    0   \n",
       "20350                         57                 9                    0   \n",
       "481477                         0                 3                    0   \n",
       "\n",
       "       Self citation Same journal  \n",
       "450485         False        False  \n",
       "408211         False        False  \n",
       "218450         False        False  \n",
       "20350          False        False  \n",
       "481477         False        False  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>Betweeness centrality</th>\n",
       "      <th>Number common neighbours</th>\n",
       "      <th>Jaccard coefficienf</th>\n",
       "      <th>Difference in inlinks coefficient</th>\n",
       "      <th>Number of times to cited</th>\n",
       "      <th>Diff publication</th>\n",
       "      <th>Number same authors</th>\n",
       "      <th>Self citation</th>\n",
       "      <th>Same journal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>416513</th>\n",
       "      <td>101064</td>\n",
       "      <td>5163</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>4</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517498</th>\n",
       "      <td>12135</td>\n",
       "      <td>9802109</td>\n",
       "      <td>-0.044258</td>\n",
       "      <td>19</td>\n",
       "      <td>0.014961</td>\n",
       "      <td>1245</td>\n",
       "      <td>1249</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315011</th>\n",
       "      <td>9903196</td>\n",
       "      <td>9806074</td>\n",
       "      <td>-0.002377</td>\n",
       "      <td>37</td>\n",
       "      <td>0.213873</td>\n",
       "      <td>79</td>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614966</th>\n",
       "      <td>9607008</td>\n",
       "      <td>9308136</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185036</th>\n",
       "      <td>2028</td>\n",
       "      <td>9805105</td>\n",
       "      <td>-0.000612</td>\n",
       "      <td>6</td>\n",
       "      <td>0.070588</td>\n",
       "      <td>32</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0        1  Betweeness centrality  Number common neighbours  \\\n",
       "416513   101064     5163               0.000072                         4   \n",
       "517498    12135  9802109              -0.044258                        19   \n",
       "315011  9903196  9806074              -0.002377                        37   \n",
       "614966  9607008  9308136              -0.000036                         1   \n",
       "185036     2028  9805105              -0.000612                         6   \n",
       "\n",
       "        Jaccard coefficienf  Difference in inlinks coefficient  \\\n",
       "416513             0.111111                                 13   \n",
       "517498             0.014961                               1245   \n",
       "315011             0.213873                                 79   \n",
       "614966             0.100000                                  5   \n",
       "185036             0.070588                                 32   \n",
       "\n",
       "        Number of times to cited  Diff publication  Number same authors  \\\n",
       "416513                        13                 1                    1   \n",
       "517498                      1249                 3                    0   \n",
       "315011                       125                 1                    0   \n",
       "614966                         6                 3                    0   \n",
       "185036                        46                 2                    1   \n",
       "\n",
       "       Self citation Same journal  \n",
       "416513          True        False  \n",
       "517498         False        False  \n",
       "315011         False        False  \n",
       "614966         False        False  \n",
       "185036          True         True  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(pred, real):\n",
    "    tot = 0\n",
    "    for i, val in enumerate(real):\n",
    "        if pred[i] == val:\n",
    "            tot += 1\n",
    "    return float(tot)/len(real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.939603421525\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators = 100)\n",
    "rfc.fit(X_train.drop([0,1], axis = 1), y_train)\n",
    "pred = rfc.predict(X_test.drop([0,1], axis = 1))\n",
    "\n",
    "print score(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
