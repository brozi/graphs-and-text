{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sknn.mlp import Layer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sknn.mlp import Classifier as MLP\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "validation = True\n",
    "path =\"\"\n",
    "if(validation):\n",
    "    path = \"validation_set2/\"\n",
    "else:\n",
    "    path = \"train_set/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(path+\"X_train.csv\",index_col=0)\n",
    "X_test = pd.read_csv(path+\"X_test.csv\",index_col=0)\n",
    "y_train = pd.Series.from_csv(path+\"y_train.csv\")\n",
    "if(validation):\n",
    "    y_test = pd.Series.from_csv(path+\"y_test.csv\")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>Betweeness centrality</th>\n",
       "      <th>Number common neighbours</th>\n",
       "      <th>Jaccard coefficienf</th>\n",
       "      <th>Difference in inlinks coefficient</th>\n",
       "      <th>Number of times to cited</th>\n",
       "      <th>Same cluster</th>\n",
       "      <th>CosineD_title_centroid</th>\n",
       "      <th>CosineD_abstract_centroid</th>\n",
       "      <th>...</th>\n",
       "      <th>Self citation</th>\n",
       "      <th>Same journal</th>\n",
       "      <th>Authors betweeness</th>\n",
       "      <th>Authors common neighbors</th>\n",
       "      <th>Authors jaccard</th>\n",
       "      <th>Authors max difference in inlinks</th>\n",
       "      <th>Authors sum difference in inlinks</th>\n",
       "      <th>Authors max of times to cited</th>\n",
       "      <th>Authors sum of times to cited</th>\n",
       "      <th>Authors  of times to cited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3432</th>\n",
       "      <td>9119</td>\n",
       "      <td>9810126</td>\n",
       "      <td>-0.005150</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006098</td>\n",
       "      <td>143</td>\n",
       "      <td>143</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012299</td>\n",
       "      <td>0.001006</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.105209</td>\n",
       "      <td>200</td>\n",
       "      <td>0.130890</td>\n",
       "      <td>720</td>\n",
       "      <td>1910</td>\n",
       "      <td>2258</td>\n",
       "      <td>872</td>\n",
       "      <td>522.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116401</th>\n",
       "      <td>4115</td>\n",
       "      <td>9912072</td>\n",
       "      <td>-0.008391</td>\n",
       "      <td>32</td>\n",
       "      <td>0.105611</td>\n",
       "      <td>254</td>\n",
       "      <td>275</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003831</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.139588</td>\n",
       "      <td>978</td>\n",
       "      <td>0.397399</td>\n",
       "      <td>1270</td>\n",
       "      <td>1749</td>\n",
       "      <td>3376</td>\n",
       "      <td>2095</td>\n",
       "      <td>749.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244993</th>\n",
       "      <td>11079</td>\n",
       "      <td>9802068</td>\n",
       "      <td>-0.005690</td>\n",
       "      <td>7</td>\n",
       "      <td>0.028926</td>\n",
       "      <td>158</td>\n",
       "      <td>186</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003094</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.099575</td>\n",
       "      <td>424</td>\n",
       "      <td>0.268695</td>\n",
       "      <td>685</td>\n",
       "      <td>1137</td>\n",
       "      <td>1688</td>\n",
       "      <td>893</td>\n",
       "      <td>844.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485454</th>\n",
       "      <td>201004</td>\n",
       "      <td>108119</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>10</td>\n",
       "      <td>0.109890</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005318</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.001035</td>\n",
       "      <td>151</td>\n",
       "      <td>0.177022</td>\n",
       "      <td>64</td>\n",
       "      <td>-246</td>\n",
       "      <td>326</td>\n",
       "      <td>258</td>\n",
       "      <td>163.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470066</th>\n",
       "      <td>9210040</td>\n",
       "      <td>112110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007626</td>\n",
       "      <td>0.000821</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.023226</td>\n",
       "      <td>362</td>\n",
       "      <td>0.058125</td>\n",
       "      <td>-4972</td>\n",
       "      <td>-4972</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361065</th>\n",
       "      <td>9510056</td>\n",
       "      <td>9710200</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.017719</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.077613</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>5151</td>\n",
       "      <td>5151</td>\n",
       "      <td>5152</td>\n",
       "      <td>5152</td>\n",
       "      <td>5152.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601163</th>\n",
       "      <td>302125</td>\n",
       "      <td>9608026</td>\n",
       "      <td>0.002665</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004006</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.037829</td>\n",
       "      <td>289</td>\n",
       "      <td>0.046553</td>\n",
       "      <td>5028</td>\n",
       "      <td>5028</td>\n",
       "      <td>5152</td>\n",
       "      <td>5152</td>\n",
       "      <td>5152.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504727</th>\n",
       "      <td>9707222</td>\n",
       "      <td>9611096</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-16</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.016222</td>\n",
       "      <td>0.001247</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.980568</td>\n",
       "      <td>1099</td>\n",
       "      <td>0.176461</td>\n",
       "      <td>4613</td>\n",
       "      <td>3835</td>\n",
       "      <td>5152</td>\n",
       "      <td>5152</td>\n",
       "      <td>5152.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132537</th>\n",
       "      <td>202079</td>\n",
       "      <td>9308075</td>\n",
       "      <td>-0.000756</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009927</td>\n",
       "      <td>0.001407</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.060826</td>\n",
       "      <td>240</td>\n",
       "      <td>0.183066</td>\n",
       "      <td>639</td>\n",
       "      <td>620</td>\n",
       "      <td>892</td>\n",
       "      <td>892</td>\n",
       "      <td>892.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524447</th>\n",
       "      <td>1044</td>\n",
       "      <td>9904207</td>\n",
       "      <td>-0.009435</td>\n",
       "      <td>6</td>\n",
       "      <td>0.021583</td>\n",
       "      <td>213</td>\n",
       "      <td>216</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003948</td>\n",
       "      <td>0.001070</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.012878</td>\n",
       "      <td>453</td>\n",
       "      <td>0.072912</td>\n",
       "      <td>4864</td>\n",
       "      <td>4864</td>\n",
       "      <td>5152</td>\n",
       "      <td>5152</td>\n",
       "      <td>5152.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160759</th>\n",
       "      <td>9506194</td>\n",
       "      <td>9304154</td>\n",
       "      <td>-0.003097</td>\n",
       "      <td>10</td>\n",
       "      <td>0.037879</td>\n",
       "      <td>94</td>\n",
       "      <td>174</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006570</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.046338</td>\n",
       "      <td>744</td>\n",
       "      <td>0.409241</td>\n",
       "      <td>-391</td>\n",
       "      <td>333</td>\n",
       "      <td>1551</td>\n",
       "      <td>827</td>\n",
       "      <td>775.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189489</th>\n",
       "      <td>301247</td>\n",
       "      <td>9803051</td>\n",
       "      <td>-0.002845</td>\n",
       "      <td>4</td>\n",
       "      <td>0.039604</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006173</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.202139</td>\n",
       "      <td>96</td>\n",
       "      <td>0.058860</td>\n",
       "      <td>1129</td>\n",
       "      <td>2660</td>\n",
       "      <td>2685</td>\n",
       "      <td>1154</td>\n",
       "      <td>1121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29661</th>\n",
       "      <td>12054</td>\n",
       "      <td>104100</td>\n",
       "      <td>0.001296</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-26</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.021403</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.284466</td>\n",
       "      <td>553</td>\n",
       "      <td>0.207505</td>\n",
       "      <td>-2244</td>\n",
       "      <td>-1992</td>\n",
       "      <td>564</td>\n",
       "      <td>312</td>\n",
       "      <td>282.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568141</th>\n",
       "      <td>9805082</td>\n",
       "      <td>9711138</td>\n",
       "      <td>-0.002629</td>\n",
       "      <td>4</td>\n",
       "      <td>0.022346</td>\n",
       "      <td>66</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010740</td>\n",
       "      <td>0.001482</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.017592</td>\n",
       "      <td>645</td>\n",
       "      <td>0.388554</td>\n",
       "      <td>-97</td>\n",
       "      <td>-376</td>\n",
       "      <td>1177</td>\n",
       "      <td>675</td>\n",
       "      <td>588.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378160</th>\n",
       "      <td>9411018</td>\n",
       "      <td>9211</td>\n",
       "      <td>-0.002269</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0.021342</td>\n",
       "      <td>0.001102</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.881568</td>\n",
       "      <td>1382</td>\n",
       "      <td>0.221688</td>\n",
       "      <td>-3803</td>\n",
       "      <td>-3698</td>\n",
       "      <td>1454</td>\n",
       "      <td>1349</td>\n",
       "      <td>727.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387454</th>\n",
       "      <td>206161</td>\n",
       "      <td>9405035</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005916</td>\n",
       "      <td>0.001517</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6206</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5152</td>\n",
       "      <td>5152</td>\n",
       "      <td>5152.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505625</th>\n",
       "      <td>205002</td>\n",
       "      <td>9805151</td>\n",
       "      <td>-0.000288</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013235</td>\n",
       "      <td>0.002128</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.126940</td>\n",
       "      <td>81</td>\n",
       "      <td>0.068761</td>\n",
       "      <td>767</td>\n",
       "      <td>1848</td>\n",
       "      <td>1936</td>\n",
       "      <td>812</td>\n",
       "      <td>797.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436380</th>\n",
       "      <td>9511093</td>\n",
       "      <td>9502024</td>\n",
       "      <td>-0.000216</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007382</td>\n",
       "      <td>0.000874</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.063010</td>\n",
       "      <td>179</td>\n",
       "      <td>0.028778</td>\n",
       "      <td>-5072</td>\n",
       "      <td>-4883</td>\n",
       "      <td>269</td>\n",
       "      <td>80</td>\n",
       "      <td>65.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589577</th>\n",
       "      <td>9912210</td>\n",
       "      <td>9908160</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>23</td>\n",
       "      <td>0.242105</td>\n",
       "      <td>19</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009167</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.072669</td>\n",
       "      <td>382</td>\n",
       "      <td>0.238750</td>\n",
       "      <td>530</td>\n",
       "      <td>2175</td>\n",
       "      <td>2524</td>\n",
       "      <td>797</td>\n",
       "      <td>457.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434300</th>\n",
       "      <td>3059</td>\n",
       "      <td>3011</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003787</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.012763</td>\n",
       "      <td>64</td>\n",
       "      <td>0.083442</td>\n",
       "      <td>179</td>\n",
       "      <td>317</td>\n",
       "      <td>468</td>\n",
       "      <td>312</td>\n",
       "      <td>148.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19201</th>\n",
       "      <td>1070</td>\n",
       "      <td>7042</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012861</td>\n",
       "      <td>0.004875</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.005519</td>\n",
       "      <td>2</td>\n",
       "      <td>0.011236</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514134</th>\n",
       "      <td>9804208</td>\n",
       "      <td>9408099</td>\n",
       "      <td>-0.025784</td>\n",
       "      <td>8</td>\n",
       "      <td>0.009685</td>\n",
       "      <td>749</td>\n",
       "      <td>771</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006990</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.728872</td>\n",
       "      <td>2668</td>\n",
       "      <td>0.418970</td>\n",
       "      <td>-2596</td>\n",
       "      <td>-501</td>\n",
       "      <td>4651</td>\n",
       "      <td>2556</td>\n",
       "      <td>2325.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376418</th>\n",
       "      <td>202192</td>\n",
       "      <td>9510226</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010108</td>\n",
       "      <td>0.003184</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008279</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-16</td>\n",
       "      <td>-32</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516222</th>\n",
       "      <td>9712012</td>\n",
       "      <td>9610043</td>\n",
       "      <td>-0.032410</td>\n",
       "      <td>16</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>915</td>\n",
       "      <td>917</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004819</td>\n",
       "      <td>0.001644</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.878119</td>\n",
       "      <td>1753</td>\n",
       "      <td>0.278962</td>\n",
       "      <td>-3624</td>\n",
       "      <td>-817</td>\n",
       "      <td>4335</td>\n",
       "      <td>1528</td>\n",
       "      <td>984.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438925</th>\n",
       "      <td>9607078</td>\n",
       "      <td>9309140</td>\n",
       "      <td>-0.004321</td>\n",
       "      <td>15</td>\n",
       "      <td>0.081081</td>\n",
       "      <td>127</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005481</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.213752</td>\n",
       "      <td>121</td>\n",
       "      <td>0.057592</td>\n",
       "      <td>1394</td>\n",
       "      <td>3524</td>\n",
       "      <td>3585</td>\n",
       "      <td>1455</td>\n",
       "      <td>946.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522810</th>\n",
       "      <td>9804115</td>\n",
       "      <td>9604055</td>\n",
       "      <td>-0.002917</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>81</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010771</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.042428</td>\n",
       "      <td>251</td>\n",
       "      <td>0.040438</td>\n",
       "      <td>5042</td>\n",
       "      <td>5042</td>\n",
       "      <td>5152</td>\n",
       "      <td>5152</td>\n",
       "      <td>5152.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191442</th>\n",
       "      <td>8211</td>\n",
       "      <td>9709066</td>\n",
       "      <td>0.000972</td>\n",
       "      <td>2</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007192</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.084052</td>\n",
       "      <td>220</td>\n",
       "      <td>0.189819</td>\n",
       "      <td>-400</td>\n",
       "      <td>-425</td>\n",
       "      <td>390</td>\n",
       "      <td>381</td>\n",
       "      <td>195.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568456</th>\n",
       "      <td>9407156</td>\n",
       "      <td>9709109</td>\n",
       "      <td>-0.001729</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012085</td>\n",
       "      <td>0.000675</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.018512</td>\n",
       "      <td>3</td>\n",
       "      <td>0.013825</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487146</th>\n",
       "      <td>9611163</td>\n",
       "      <td>9604166</td>\n",
       "      <td>-0.003781</td>\n",
       "      <td>8</td>\n",
       "      <td>0.049080</td>\n",
       "      <td>97</td>\n",
       "      <td>118</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001731</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.860067</td>\n",
       "      <td>1890</td>\n",
       "      <td>0.302303</td>\n",
       "      <td>-3690</td>\n",
       "      <td>-2507</td>\n",
       "      <td>2645</td>\n",
       "      <td>1462</td>\n",
       "      <td>1322.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571510</th>\n",
       "      <td>9805148</td>\n",
       "      <td>9604139</td>\n",
       "      <td>-0.002053</td>\n",
       "      <td>6</td>\n",
       "      <td>0.082192</td>\n",
       "      <td>49</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006958</td>\n",
       "      <td>0.002044</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.052777</td>\n",
       "      <td>273</td>\n",
       "      <td>0.043954</td>\n",
       "      <td>5058</td>\n",
       "      <td>4964</td>\n",
       "      <td>5152</td>\n",
       "      <td>5152</td>\n",
       "      <td>5152.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10651</th>\n",
       "      <td>9609204</td>\n",
       "      <td>9607235</td>\n",
       "      <td>-0.005258</td>\n",
       "      <td>18</td>\n",
       "      <td>0.081818</td>\n",
       "      <td>109</td>\n",
       "      <td>132</td>\n",
       "      <td>1</td>\n",
       "      <td>0.008502</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.803955</td>\n",
       "      <td>2067</td>\n",
       "      <td>0.328721</td>\n",
       "      <td>3365</td>\n",
       "      <td>2277</td>\n",
       "      <td>5152</td>\n",
       "      <td>5152</td>\n",
       "      <td>5152.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590030</th>\n",
       "      <td>9209107</td>\n",
       "      <td>9603123</td>\n",
       "      <td>-0.001693</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017315</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.080488</td>\n",
       "      <td>380</td>\n",
       "      <td>0.341113</td>\n",
       "      <td>538</td>\n",
       "      <td>886</td>\n",
       "      <td>1269</td>\n",
       "      <td>853</td>\n",
       "      <td>634.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462188</th>\n",
       "      <td>9904207</td>\n",
       "      <td>9712028</td>\n",
       "      <td>0.005186</td>\n",
       "      <td>9</td>\n",
       "      <td>0.022901</td>\n",
       "      <td>-110</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6206</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5152</td>\n",
       "      <td>5152</td>\n",
       "      <td>5152.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180544</th>\n",
       "      <td>9411136</td>\n",
       "      <td>111271</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013965</td>\n",
       "      <td>0.001699</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014028</td>\n",
       "      <td>3</td>\n",
       "      <td>0.020979</td>\n",
       "      <td>-20</td>\n",
       "      <td>-64</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289077</th>\n",
       "      <td>301073</td>\n",
       "      <td>9904120</td>\n",
       "      <td>-0.001476</td>\n",
       "      <td>9</td>\n",
       "      <td>0.147541</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0.021053</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.042543</td>\n",
       "      <td>264</td>\n",
       "      <td>0.232190</td>\n",
       "      <td>381</td>\n",
       "      <td>912</td>\n",
       "      <td>1080</td>\n",
       "      <td>549</td>\n",
       "      <td>540.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203418</th>\n",
       "      <td>9701181</td>\n",
       "      <td>208028</td>\n",
       "      <td>-0.001260</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004169</td>\n",
       "      <td>0.002480</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.034150</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003663</td>\n",
       "      <td>92</td>\n",
       "      <td>85</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331881</th>\n",
       "      <td>9811150</td>\n",
       "      <td>9712088</td>\n",
       "      <td>-0.000216</td>\n",
       "      <td>2</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001989</td>\n",
       "      <td>0.003591</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.006554</td>\n",
       "      <td>482</td>\n",
       "      <td>0.077579</td>\n",
       "      <td>-4825</td>\n",
       "      <td>-4825</td>\n",
       "      <td>327</td>\n",
       "      <td>327</td>\n",
       "      <td>327.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169236</th>\n",
       "      <td>209176</td>\n",
       "      <td>204089</td>\n",
       "      <td>-0.006662</td>\n",
       "      <td>9</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>24</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006185</td>\n",
       "      <td>0.001411</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.018742</td>\n",
       "      <td>495</td>\n",
       "      <td>0.079672</td>\n",
       "      <td>4926</td>\n",
       "      <td>4752</td>\n",
       "      <td>5152</td>\n",
       "      <td>5152</td>\n",
       "      <td>5152.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282498</th>\n",
       "      <td>105051</td>\n",
       "      <td>9908134</td>\n",
       "      <td>-0.004033</td>\n",
       "      <td>6</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>122</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004448</td>\n",
       "      <td>0.000686</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.803955</td>\n",
       "      <td>1949</td>\n",
       "      <td>0.310252</td>\n",
       "      <td>-3365</td>\n",
       "      <td>-2663</td>\n",
       "      <td>2489</td>\n",
       "      <td>1787</td>\n",
       "      <td>1244.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373214</th>\n",
       "      <td>207157</td>\n",
       "      <td>10104</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>6</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010955</td>\n",
       "      <td>0.001701</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.050822</td>\n",
       "      <td>183</td>\n",
       "      <td>0.212791</td>\n",
       "      <td>236</td>\n",
       "      <td>214</td>\n",
       "      <td>515</td>\n",
       "      <td>373</td>\n",
       "      <td>257.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3822</th>\n",
       "      <td>209127</td>\n",
       "      <td>110196</td>\n",
       "      <td>-0.000108</td>\n",
       "      <td>4</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009919</td>\n",
       "      <td>0.002892</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.947453</td>\n",
       "      <td>1078</td>\n",
       "      <td>0.173368</td>\n",
       "      <td>-4072</td>\n",
       "      <td>-4072</td>\n",
       "      <td>1080</td>\n",
       "      <td>1080</td>\n",
       "      <td>1080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386846</th>\n",
       "      <td>108011</td>\n",
       "      <td>9907022</td>\n",
       "      <td>-0.000900</td>\n",
       "      <td>1</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014123</td>\n",
       "      <td>0.001395</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.898931</td>\n",
       "      <td>1747</td>\n",
       "      <td>0.278406</td>\n",
       "      <td>-3944</td>\n",
       "      <td>-2923</td>\n",
       "      <td>2229</td>\n",
       "      <td>1208</td>\n",
       "      <td>463.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51275</th>\n",
       "      <td>5151</td>\n",
       "      <td>9410095</td>\n",
       "      <td>0.001765</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-15</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005235</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.076233</td>\n",
       "      <td>180</td>\n",
       "      <td>0.133432</td>\n",
       "      <td>-391</td>\n",
       "      <td>-533</td>\n",
       "      <td>473</td>\n",
       "      <td>381</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71223</th>\n",
       "      <td>301019</td>\n",
       "      <td>10136</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011851</td>\n",
       "      <td>0.003838</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.137864</td>\n",
       "      <td>17</td>\n",
       "      <td>0.014358</td>\n",
       "      <td>678</td>\n",
       "      <td>876</td>\n",
       "      <td>906</td>\n",
       "      <td>708</td>\n",
       "      <td>159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355014</th>\n",
       "      <td>9304034</td>\n",
       "      <td>9601010</td>\n",
       "      <td>-0.000540</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.023293</td>\n",
       "      <td>0.001406</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.917443</td>\n",
       "      <td>1215</td>\n",
       "      <td>0.194245</td>\n",
       "      <td>-4202</td>\n",
       "      <td>-3732</td>\n",
       "      <td>1420</td>\n",
       "      <td>950</td>\n",
       "      <td>710.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479936</th>\n",
       "      <td>9612134</td>\n",
       "      <td>9806181</td>\n",
       "      <td>0.000540</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003737</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003909</td>\n",
       "      <td>90</td>\n",
       "      <td>0.119681</td>\n",
       "      <td>148</td>\n",
       "      <td>148</td>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "      <td>321.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100592</th>\n",
       "      <td>304186</td>\n",
       "      <td>205114</td>\n",
       "      <td>-0.000216</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003917</td>\n",
       "      <td>0.000738</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.009084</td>\n",
       "      <td>9</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>35</td>\n",
       "      <td>50</td>\n",
       "      <td>58</td>\n",
       "      <td>39</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186473</th>\n",
       "      <td>9602114</td>\n",
       "      <td>9504145</td>\n",
       "      <td>0.002701</td>\n",
       "      <td>37</td>\n",
       "      <td>0.108187</td>\n",
       "      <td>-48</td>\n",
       "      <td>150</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006179</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012993</td>\n",
       "      <td>1135</td>\n",
       "      <td>0.517556</td>\n",
       "      <td>-66</td>\n",
       "      <td>453</td>\n",
       "      <td>2509</td>\n",
       "      <td>1389</td>\n",
       "      <td>601.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130510</th>\n",
       "      <td>9909190</td>\n",
       "      <td>9307108</td>\n",
       "      <td>-0.000792</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006324</td>\n",
       "      <td>0.001867</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007014</td>\n",
       "      <td>97</td>\n",
       "      <td>0.142438</td>\n",
       "      <td>109</td>\n",
       "      <td>215</td>\n",
       "      <td>367</td>\n",
       "      <td>221</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182531</th>\n",
       "      <td>4070</td>\n",
       "      <td>9703059</td>\n",
       "      <td>-0.001693</td>\n",
       "      <td>5</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>40</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009565</td>\n",
       "      <td>0.000555</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.096815</td>\n",
       "      <td>75</td>\n",
       "      <td>0.084459</td>\n",
       "      <td>507</td>\n",
       "      <td>878</td>\n",
       "      <td>889</td>\n",
       "      <td>518</td>\n",
       "      <td>444.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414842</th>\n",
       "      <td>9710226</td>\n",
       "      <td>9709108</td>\n",
       "      <td>-0.000396</td>\n",
       "      <td>26</td>\n",
       "      <td>0.320988</td>\n",
       "      <td>18</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>0.000687</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.863286</td>\n",
       "      <td>2112</td>\n",
       "      <td>0.335718</td>\n",
       "      <td>-3624</td>\n",
       "      <td>-2162</td>\n",
       "      <td>2990</td>\n",
       "      <td>1528</td>\n",
       "      <td>1495.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515999</th>\n",
       "      <td>9810156</td>\n",
       "      <td>211079</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>0.001576</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.008279</td>\n",
       "      <td>10</td>\n",
       "      <td>0.028011</td>\n",
       "      <td>-61</td>\n",
       "      <td>-111</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69992</th>\n",
       "      <td>111093</td>\n",
       "      <td>111008</td>\n",
       "      <td>0.000864</td>\n",
       "      <td>45</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>-14</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007021</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.026561</td>\n",
       "      <td>369</td>\n",
       "      <td>0.059411</td>\n",
       "      <td>-5046</td>\n",
       "      <td>-5046</td>\n",
       "      <td>106</td>\n",
       "      <td>106</td>\n",
       "      <td>106.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98261</th>\n",
       "      <td>304207</td>\n",
       "      <td>106016</td>\n",
       "      <td>-0.000360</td>\n",
       "      <td>4</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005815</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.014948</td>\n",
       "      <td>35</td>\n",
       "      <td>0.137795</td>\n",
       "      <td>116</td>\n",
       "      <td>211</td>\n",
       "      <td>231</td>\n",
       "      <td>136</td>\n",
       "      <td>115.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381662</th>\n",
       "      <td>9511195</td>\n",
       "      <td>9309103</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005428</td>\n",
       "      <td>0.001417</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.005864</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>-6</td>\n",
       "      <td>-6</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117530</th>\n",
       "      <td>9909073</td>\n",
       "      <td>9508002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>20</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006721</td>\n",
       "      <td>0.000914</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.035070</td>\n",
       "      <td>397</td>\n",
       "      <td>0.063826</td>\n",
       "      <td>4985</td>\n",
       "      <td>4907</td>\n",
       "      <td>5152</td>\n",
       "      <td>5152</td>\n",
       "      <td>5152.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4419</th>\n",
       "      <td>9710007</td>\n",
       "      <td>9611008</td>\n",
       "      <td>-0.000900</td>\n",
       "      <td>13</td>\n",
       "      <td>0.120370</td>\n",
       "      <td>36</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>0.353091</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.017937</td>\n",
       "      <td>331</td>\n",
       "      <td>0.334343</td>\n",
       "      <td>139</td>\n",
       "      <td>495</td>\n",
       "      <td>898</td>\n",
       "      <td>454</td>\n",
       "      <td>449.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480423</th>\n",
       "      <td>9810095</td>\n",
       "      <td>208033</td>\n",
       "      <td>-0.000252</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008421</td>\n",
       "      <td>0.001014</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.010578</td>\n",
       "      <td>47</td>\n",
       "      <td>0.090385</td>\n",
       "      <td>-10</td>\n",
       "      <td>-6</td>\n",
       "      <td>127</td>\n",
       "      <td>95</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487486</th>\n",
       "      <td>9910160</td>\n",
       "      <td>9711191</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009333</td>\n",
       "      <td>0.001064</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.043233</td>\n",
       "      <td>31</td>\n",
       "      <td>0.045058</td>\n",
       "      <td>-97</td>\n",
       "      <td>-11</td>\n",
       "      <td>144</td>\n",
       "      <td>58</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532541</th>\n",
       "      <td>9705087</td>\n",
       "      <td>9407087</td>\n",
       "      <td>-0.035795</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1001</td>\n",
       "      <td>1011</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014509</td>\n",
       "      <td>0.001311</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.300678</td>\n",
       "      <td>466</td>\n",
       "      <td>0.154356</td>\n",
       "      <td>2341</td>\n",
       "      <td>4167</td>\n",
       "      <td>4651</td>\n",
       "      <td>2556</td>\n",
       "      <td>2325.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>492409 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0        1  Betweeness centrality  Number common neighbours  \\\n",
       "3432       9119  9810126              -0.005150                         1   \n",
       "116401     4115  9912072              -0.008391                        32   \n",
       "244993    11079  9802068              -0.005690                         7   \n",
       "485454   201004   108119               0.000360                        10   \n",
       "470066  9210040   112110               0.000000                         0   \n",
       "361065  9510056  9710200              -0.000072                         0   \n",
       "601163   302125  9608026               0.002665                         0   \n",
       "504727  9707222  9611096               0.001044                         0   \n",
       "132537   202079  9308075              -0.000756                         0   \n",
       "524447     1044  9904207              -0.009435                         6   \n",
       "160759  9506194  9304154              -0.003097                        10   \n",
       "189489   301247  9803051              -0.002845                         4   \n",
       "29661     12054   104100               0.001296                         0   \n",
       "568141  9805082  9711138              -0.002629                         4   \n",
       "378160  9411018     9211              -0.002269                         0   \n",
       "387454   206161  9405035               0.001008                         1   \n",
       "505625   205002  9805151              -0.000288                         0   \n",
       "436380  9511093  9502024              -0.000216                         0   \n",
       "589577  9912210  9908160               0.000036                        23   \n",
       "434300     3059     3011               0.000432                         0   \n",
       "19201      1070     7042               0.000144                         0   \n",
       "514134  9804208  9408099              -0.025784                         8   \n",
       "376418   202192  9510226               0.000324                         0   \n",
       "516222  9712012  9610043              -0.032410                        16   \n",
       "438925  9607078  9309140              -0.004321                        15   \n",
       "522810  9804115  9604055              -0.002917                         1   \n",
       "191442     8211  9709066               0.000972                         2   \n",
       "568456  9407156  9709109              -0.001729                         0   \n",
       "487146  9611163  9604166              -0.003781                         8   \n",
       "571510  9805148  9604139              -0.002053                         6   \n",
       "...         ...      ...                    ...                       ...   \n",
       "10651   9609204  9607235              -0.005258                        18   \n",
       "590030  9209107  9603123              -0.001693                         0   \n",
       "462188  9904207  9712028               0.005186                         9   \n",
       "180544  9411136   111271               0.000252                         0   \n",
       "289077   301073  9904120              -0.001476                         9   \n",
       "203418  9701181   208028              -0.001260                         0   \n",
       "331881  9811150  9712088              -0.000216                         2   \n",
       "169236   209176   204089              -0.006662                         9   \n",
       "282498   105051  9908134              -0.004033                         6   \n",
       "373214   207157    10104               0.000072                         6   \n",
       "3822     209127   110196              -0.000108                         4   \n",
       "386846   108011  9907022              -0.000900                         1   \n",
       "51275      5151  9410095               0.001765                         0   \n",
       "71223    301019    10136               0.000108                         0   \n",
       "355014  9304034  9601010              -0.000540                         0   \n",
       "479936  9612134  9806181               0.000540                         0   \n",
       "100592   304186   205114              -0.000216                         0   \n",
       "186473  9602114  9504145               0.002701                        37   \n",
       "130510  9909190  9307108              -0.000792                         0   \n",
       "182531     4070  9703059              -0.001693                         5   \n",
       "414842  9710226  9709108              -0.000396                        26   \n",
       "515999  9810156   211079              -0.000072                         0   \n",
       "69992    111093   111008               0.000864                        45   \n",
       "98261    304207   106016              -0.000360                         4   \n",
       "381662  9511195  9309103               0.000324                         0   \n",
       "117530  9909073  9508002               0.000000                         4   \n",
       "4419    9710007  9611008              -0.000900                        13   \n",
       "480423  9810095   208033              -0.000252                         0   \n",
       "487486  9910160  9711191               0.000468                         0   \n",
       "532541  9705087  9407087              -0.035795                         0   \n",
       "\n",
       "        Jaccard coefficienf  Difference in inlinks coefficient  \\\n",
       "3432               0.006098                                143   \n",
       "116401             0.105611                                254   \n",
       "244993             0.028926                                158   \n",
       "485454             0.109890                                 10   \n",
       "470066             0.000000                                 -1   \n",
       "361065             0.000000                                 -1   \n",
       "601163             0.000000                                  9   \n",
       "504727             0.000000                                -16   \n",
       "132537             0.000000                                 40   \n",
       "524447             0.021583                                213   \n",
       "160759             0.037879                                 94   \n",
       "189489             0.039604                                 66   \n",
       "29661              0.000000                                -26   \n",
       "568141             0.022346                                 66   \n",
       "378160             0.000000                                 40   \n",
       "387454             0.016949                                  4   \n",
       "505625             0.000000                                 23   \n",
       "436380             0.000000                                  7   \n",
       "589577             0.242105                                 19   \n",
       "434300             0.000000                                  3   \n",
       "19201              0.000000                                 -1   \n",
       "514134             0.009685                                749   \n",
       "376418             0.000000                                  2   \n",
       "516222             0.016949                                915   \n",
       "438925             0.081081                                127   \n",
       "522810             0.006667                                 81   \n",
       "191442             0.048780                                  1   \n",
       "568456             0.000000                                 40   \n",
       "487146             0.049080                                 97   \n",
       "571510             0.082192                                 49   \n",
       "...                     ...                                ...   \n",
       "10651              0.081818                                109   \n",
       "590030             0.000000                                 43   \n",
       "462188             0.022901                               -110   \n",
       "180544             0.000000                                 -6   \n",
       "289077             0.147541                                 40   \n",
       "203418             0.000000                                  8   \n",
       "331881             0.090909                                  8   \n",
       "169236             0.036000                                 24   \n",
       "282498             0.038961                                122   \n",
       "373214             0.115385                                  7   \n",
       "3822               0.067797                                 27   \n",
       "386846             0.020000                                 13   \n",
       "51275              0.000000                                -15   \n",
       "71223              0.000000                                  0   \n",
       "355014             0.000000                                  2   \n",
       "479936             0.000000                                  1   \n",
       "100592             0.000000                                  2   \n",
       "186473             0.108187                                -48   \n",
       "130510             0.000000                                 23   \n",
       "182531             0.059524                                 40   \n",
       "414842             0.320988                                 18   \n",
       "515999             0.000000                                  2   \n",
       "69992              0.384615                                -14   \n",
       "98261              0.200000                                 12   \n",
       "381662             0.000000                                 -4   \n",
       "117530             0.060606                                 20   \n",
       "4419               0.120370                                 36   \n",
       "480423             0.000000                                 -4   \n",
       "487486             0.000000                                  2   \n",
       "532541             0.000000                               1001   \n",
       "\n",
       "        Number of times to cited  Same cluster  CosineD_title_centroid  \\\n",
       "3432                         143             1                0.012299   \n",
       "116401                       275             1                0.003831   \n",
       "244993                       186             1                0.003094   \n",
       "485454                        25             1                0.005318   \n",
       "470066                         4             0                0.007626   \n",
       "361065                         0             0                0.017719   \n",
       "601163                         9             0                0.004006   \n",
       "504727                         2             0                0.016222   \n",
       "132537                        49             0                0.009927   \n",
       "524447                       216             1                0.003948   \n",
       "160759                       174             1                0.006570   \n",
       "189489                        66             1                0.006173   \n",
       "29661                          8             0                0.021403   \n",
       "568141                        94             0                0.010740   \n",
       "378160                        45             0                0.021342   \n",
       "387454                        16             1                0.005916   \n",
       "505625                        25             0                0.013235   \n",
       "436380                         8             0                0.007382   \n",
       "589577                        45             1                0.009167   \n",
       "434300                         8             1                0.003787   \n",
       "19201                          0             0                0.012861   \n",
       "514134                       771             0                0.006990   \n",
       "376418                         2             0                0.010108   \n",
       "516222                       917             1                0.004819   \n",
       "438925                       145             1                0.005481   \n",
       "522810                        92             1                0.010771   \n",
       "191442                         6             1                0.007192   \n",
       "568456                        41             0                0.012085   \n",
       "487146                       118             1                0.001731   \n",
       "571510                        51             1                0.006958   \n",
       "...                          ...           ...                     ...   \n",
       "10651                        132             1                0.008502   \n",
       "590030                        44             1                0.017315   \n",
       "462188                       106             0                0.011225   \n",
       "180544                         0             0                0.013965   \n",
       "289077                        40             1                0.021053   \n",
       "203418                         8             0                0.004169   \n",
       "331881                         9             1                0.001989   \n",
       "169236                        29             1                0.006185   \n",
       "282498                       122             1                0.004448   \n",
       "373214                        10             0                0.010955   \n",
       "3822                          27             0                0.009919   \n",
       "386846                        14             1                0.014123   \n",
       "51275                          4             0                0.005235   \n",
       "71223                          0             0                0.011851   \n",
       "355014                         7             0                0.023293   \n",
       "479936                         2             0                0.003737   \n",
       "100592                         2             1                0.003917   \n",
       "186473                       150             1                0.006179   \n",
       "130510                        23             0                0.006324   \n",
       "182531                        42             1                0.009565   \n",
       "414842                        51             1                0.003187   \n",
       "515999                         3             0                0.015500   \n",
       "69992                         28             1                0.007021   \n",
       "98261                         12             1                0.005815   \n",
       "381662                         1             0                0.005428   \n",
       "117530                        28             1                0.006721   \n",
       "4419                          66             1                0.353091   \n",
       "480423                         0             0                0.008421   \n",
       "487486                         7             0                0.009333   \n",
       "532541                      1011             0                0.014509   \n",
       "\n",
       "        CosineD_abstract_centroid             ...              Self citation  \\\n",
       "3432                     0.001006             ...                          0   \n",
       "116401                   0.000732             ...                          0   \n",
       "244993                   0.000562             ...                          0   \n",
       "485454                   0.000895             ...                          0   \n",
       "470066                   0.000821             ...                          0   \n",
       "361065                   0.000676             ...                          0   \n",
       "601163                   0.000907             ...                          0   \n",
       "504727                   0.001247             ...                          0   \n",
       "132537                   0.001407             ...                          0   \n",
       "524447                   0.001070             ...                          0   \n",
       "160759                   0.000707             ...                          0   \n",
       "189489                   0.000855             ...                          0   \n",
       "29661                    0.002634             ...                          0   \n",
       "568141                   0.001482             ...                          0   \n",
       "378160                   0.001102             ...                          0   \n",
       "387454                   0.001517             ...                          1   \n",
       "505625                   0.002128             ...                          0   \n",
       "436380                   0.000874             ...                          0   \n",
       "589577                   0.000322             ...                          0   \n",
       "434300                   0.000433             ...                          0   \n",
       "19201                    0.004875             ...                          0   \n",
       "514134                   0.001271             ...                          0   \n",
       "376418                   0.003184             ...                          0   \n",
       "516222                   0.001644             ...                          0   \n",
       "438925                   0.000401             ...                          0   \n",
       "522810                   0.000537             ...                          0   \n",
       "191442                   0.000753             ...                          0   \n",
       "568456                   0.000675             ...                          0   \n",
       "487146                   0.000990             ...                          0   \n",
       "571510                   0.002044             ...                          0   \n",
       "...                           ...             ...                        ...   \n",
       "10651                    0.000726             ...                          0   \n",
       "590030                   0.001587             ...                          0   \n",
       "462188                   0.001049             ...                          1   \n",
       "180544                   0.001699             ...                          0   \n",
       "289077                   0.000440             ...                          0   \n",
       "203418                   0.002480             ...                          0   \n",
       "331881                   0.003591             ...                          0   \n",
       "169236                   0.001411             ...                          0   \n",
       "282498                   0.000686             ...                          0   \n",
       "373214                   0.001701             ...                          0   \n",
       "3822                     0.002892             ...                          0   \n",
       "386846                   0.001395             ...                          0   \n",
       "51275                    0.000944             ...                          0   \n",
       "71223                    0.003838             ...                          0   \n",
       "355014                   0.001406             ...                          0   \n",
       "479936                   0.000543             ...                          0   \n",
       "100592                   0.000738             ...                          0   \n",
       "186473                   0.001094             ...                          1   \n",
       "130510                   0.001867             ...                          0   \n",
       "182531                   0.000555             ...                          0   \n",
       "414842                   0.000687             ...                          0   \n",
       "515999                   0.001576             ...                          0   \n",
       "69992                    0.000621             ...                          0   \n",
       "98261                    0.002036             ...                          0   \n",
       "381662                   0.001417             ...                          0   \n",
       "117530                   0.000914             ...                          0   \n",
       "4419                     0.000799             ...                          0   \n",
       "480423                   0.001014             ...                          0   \n",
       "487486                   0.001064             ...                          0   \n",
       "532541                   0.001311             ...                          0   \n",
       "\n",
       "        Same journal  Authors betweeness  Authors common neighbors  \\\n",
       "3432               0           -0.105209                       200   \n",
       "116401             0           -0.139588                       978   \n",
       "244993             0           -0.099575                       424   \n",
       "485454             0           -0.001035                       151   \n",
       "470066             0            1.023226                       362   \n",
       "361065             0           -1.077613                         1   \n",
       "601163             0           -1.037829                       289   \n",
       "504727             0           -0.980568                      1099   \n",
       "132537             0           -0.060826                       240   \n",
       "524447             0           -1.012878                       453   \n",
       "160759             0            0.046338                       744   \n",
       "189489             0           -0.202139                        96   \n",
       "29661              1            0.284466                       553   \n",
       "568141             0            0.017592                       645   \n",
       "378160             1            0.881568                      1382   \n",
       "387454             0            0.000000                      6206   \n",
       "505625             0           -0.126940                        81   \n",
       "436380             0            1.063010                       179   \n",
       "589577             0           -0.072669                       382   \n",
       "434300             0           -0.012763                        64   \n",
       "19201              0           -0.005519                         2   \n",
       "514134             0            0.728872                      2668   \n",
       "376418             0            0.008279                         0   \n",
       "516222             0            0.878119                      1753   \n",
       "438925             0           -0.213752                       121   \n",
       "522810             1           -1.042428                       251   \n",
       "191442             1            0.084052                       220   \n",
       "568456             0           -0.018512                         3   \n",
       "487146             0            0.860067                      1890   \n",
       "571510             0           -1.052777                       273   \n",
       "...              ...                 ...                       ...   \n",
       "10651              0           -0.803955                      2067   \n",
       "590030             0           -0.080488                       380   \n",
       "462188             0            0.000000                      6206   \n",
       "180544             0            0.014028                         3   \n",
       "289077             0           -0.042543                       264   \n",
       "203418             0           -0.034150                         1   \n",
       "331881             1            1.006554                       482   \n",
       "169236             0           -1.018742                       495   \n",
       "282498             0            0.803955                      1949   \n",
       "373214             0           -0.050822                       183   \n",
       "3822               0            0.947453                      1078   \n",
       "386846             0            0.898931                      1747   \n",
       "51275              0            0.076233                       180   \n",
       "71223              0           -0.137864                        17   \n",
       "355014             0            0.917443                      1215   \n",
       "479936             0            0.003909                        90   \n",
       "100592             0           -0.009084                         9   \n",
       "186473             1            0.012993                      1135   \n",
       "130510             0            0.007014                        97   \n",
       "182531             1           -0.096815                        75   \n",
       "414842             0            0.863286                      2112   \n",
       "515999             1           -0.008279                        10   \n",
       "69992              0            1.026561                       369   \n",
       "98261              0           -0.014948                        35   \n",
       "381662             0           -0.005864                         2   \n",
       "117530             0           -1.035070                       397   \n",
       "4419               0           -0.017937                       331   \n",
       "480423             0           -0.010578                        47   \n",
       "487486             0            0.043233                        31   \n",
       "532541             1           -0.300678                       466   \n",
       "\n",
       "        Authors jaccard  Authors max difference in inlinks  \\\n",
       "3432           0.130890                                720   \n",
       "116401         0.397399                               1270   \n",
       "244993         0.268695                                685   \n",
       "485454         0.177022                                 64   \n",
       "470066         0.058125                              -4972   \n",
       "361065         0.000161                               5151   \n",
       "601163         0.046553                               5028   \n",
       "504727         0.176461                               4613   \n",
       "132537         0.183066                                639   \n",
       "524447         0.072912                               4864   \n",
       "160759         0.409241                               -391   \n",
       "189489         0.058860                               1129   \n",
       "29661          0.207505                              -2244   \n",
       "568141         0.388554                                -97   \n",
       "378160         0.221688                              -3803   \n",
       "387454         1.000000                                  0   \n",
       "505625         0.068761                                767   \n",
       "436380         0.028778                              -5072   \n",
       "589577         0.238750                                530   \n",
       "434300         0.083442                                179   \n",
       "19201          0.011236                                 21   \n",
       "514134         0.418970                              -2596   \n",
       "376418         0.000000                                -16   \n",
       "516222         0.278962                              -3624   \n",
       "438925         0.057592                               1394   \n",
       "522810         0.040438                               5042   \n",
       "191442         0.189819                               -400   \n",
       "568456         0.013825                                 70   \n",
       "487146         0.302303                              -3690   \n",
       "571510         0.043954                               5058   \n",
       "...                 ...                                ...   \n",
       "10651          0.328721                               3365   \n",
       "590030         0.341113                                538   \n",
       "462188         1.000000                                  0   \n",
       "180544         0.020979                                -20   \n",
       "289077         0.232190                                381   \n",
       "203418         0.003663                                 92   \n",
       "331881         0.077579                              -4825   \n",
       "169236         0.079672                               4926   \n",
       "282498         0.310252                              -3365   \n",
       "373214         0.212791                                236   \n",
       "3822           0.173368                              -4072   \n",
       "386846         0.278406                              -3944   \n",
       "51275          0.133432                               -391   \n",
       "71223          0.014358                                678   \n",
       "355014         0.194245                              -4202   \n",
       "479936         0.119681                                148   \n",
       "100592         0.097826                                 35   \n",
       "186473         0.517556                                -66   \n",
       "130510         0.142438                                109   \n",
       "182531         0.084459                                507   \n",
       "414842         0.335718                              -3624   \n",
       "515999         0.028011                                -61   \n",
       "69992          0.059411                              -5046   \n",
       "98261          0.137795                                116   \n",
       "381662         0.012987                                 -6   \n",
       "117530         0.063826                               4985   \n",
       "4419           0.334343                                139   \n",
       "480423         0.090385                                -10   \n",
       "487486         0.045058                                -97   \n",
       "532541         0.154356                               2341   \n",
       "\n",
       "        Authors sum difference in inlinks  Authors max of times to cited  \\\n",
       "3432                                 1910                           2258   \n",
       "116401                               1749                           3376   \n",
       "244993                               1137                           1688   \n",
       "485454                               -246                            326   \n",
       "470066                              -4972                            180   \n",
       "361065                               5151                           5152   \n",
       "601163                               5028                           5152   \n",
       "504727                               3835                           5152   \n",
       "132537                                620                            892   \n",
       "524447                               4864                           5152   \n",
       "160759                                333                           1551   \n",
       "189489                               2660                           2685   \n",
       "29661                               -1992                            564   \n",
       "568141                               -376                           1177   \n",
       "378160                              -3698                           1454   \n",
       "387454                                  0                           5152   \n",
       "505625                               1848                           1936   \n",
       "436380                              -4883                            269   \n",
       "589577                               2175                           2524   \n",
       "434300                                317                            468   \n",
       "19201                                  21                             70   \n",
       "514134                               -501                           4651   \n",
       "376418                                -32                              3   \n",
       "516222                               -817                           4335   \n",
       "438925                               3524                           3585   \n",
       "522810                               5042                           5152   \n",
       "191442                               -425                            390   \n",
       "568456                                 70                             82   \n",
       "487146                              -2507                           2645   \n",
       "571510                               4964                           5152   \n",
       "...                                   ...                            ...   \n",
       "10651                                2277                           5152   \n",
       "590030                                886                           1269   \n",
       "462188                                  0                           5152   \n",
       "180544                                -64                              6   \n",
       "289077                                912                           1080   \n",
       "203418                                 85                             99   \n",
       "331881                              -4825                            327   \n",
       "169236                               4752                           5152   \n",
       "282498                              -2663                           2489   \n",
       "373214                                214                            515   \n",
       "3822                                -4072                           1080   \n",
       "386846                              -2923                           2229   \n",
       "51275                                -533                            473   \n",
       "71223                                 876                            906   \n",
       "355014                              -3732                           1420   \n",
       "479936                                148                            321   \n",
       "100592                                 50                             58   \n",
       "186473                                453                           2509   \n",
       "130510                                215                            367   \n",
       "182531                                878                            889   \n",
       "414842                              -2162                           2990   \n",
       "515999                               -111                             40   \n",
       "69992                               -5046                            106   \n",
       "98261                                 211                            231   \n",
       "381662                                 -6                             30   \n",
       "117530                               4907                           5152   \n",
       "4419                                  495                            898   \n",
       "480423                                 -6                            127   \n",
       "487486                                -11                            144   \n",
       "532541                               4167                           4651   \n",
       "\n",
       "        Authors sum of times to cited  Authors  of times to cited  \n",
       "3432                              872                       522.0  \n",
       "116401                           2095                       749.0  \n",
       "244993                            893                       844.0  \n",
       "485454                            258                       163.0  \n",
       "470066                            180                       180.0  \n",
       "361065                           5152                      5152.0  \n",
       "601163                           5152                      5152.0  \n",
       "504727                           5152                      5152.0  \n",
       "132537                            892                       892.0  \n",
       "524447                           5152                      5152.0  \n",
       "160759                            827                       775.5  \n",
       "189489                           1154                      1121.0  \n",
       "29661                             312                       282.0  \n",
       "568141                            675                       588.5  \n",
       "378160                           1349                       727.0  \n",
       "387454                           5152                      5152.0  \n",
       "505625                            812                       797.0  \n",
       "436380                             80                        65.5  \n",
       "589577                            797                       457.0  \n",
       "434300                            312                       148.0  \n",
       "19201                              70                        70.0  \n",
       "514134                           2556                      2325.5  \n",
       "376418                              3                         3.0  \n",
       "516222                           1528                       984.0  \n",
       "438925                           1455                       946.0  \n",
       "522810                           5152                      5152.0  \n",
       "191442                            381                       195.0  \n",
       "568456                             82                        82.0  \n",
       "487146                           1462                      1322.5  \n",
       "571510                           5152                      5152.0  \n",
       "...                               ...                         ...  \n",
       "10651                            5152                      5152.0  \n",
       "590030                            853                       634.5  \n",
       "462188                           5152                      5152.0  \n",
       "180544                              6                         6.0  \n",
       "289077                            549                       540.0  \n",
       "203418                             99                        99.0  \n",
       "331881                            327                       327.0  \n",
       "169236                           5152                      5152.0  \n",
       "282498                           1787                      1244.5  \n",
       "373214                            373                       257.5  \n",
       "3822                             1080                      1080.0  \n",
       "386846                           1208                       463.0  \n",
       "51275                             381                        55.0  \n",
       "71223                             708                       159.0  \n",
       "355014                            950                       710.0  \n",
       "479936                            321                       321.0  \n",
       "100592                             39                        14.0  \n",
       "186473                           1389                       601.0  \n",
       "130510                            221                        56.0  \n",
       "182531                            518                       444.5  \n",
       "414842                           1528                      1495.0  \n",
       "515999                             40                        40.0  \n",
       "69992                             106                       106.0  \n",
       "98261                             136                       115.5  \n",
       "381662                             30                        30.0  \n",
       "117530                           5152                      5152.0  \n",
       "4419                              454                       449.0  \n",
       "480423                             95                        13.0  \n",
       "487486                             58                        52.0  \n",
       "532541                           2556                      2325.5  \n",
       "\n",
       "[492409 rows x 23 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.949895615866 0.954842417584\n",
      "Betweeness centrality score gained: 2.43698366409e-05\n",
      "score gained on train: 0.000142158246498\n",
      "Number common neighbours score gained: -0.0333054434092\n",
      "score gained on train: -0.0295222061335\n",
      "Jaccard coefficienf score gained: -0.00163277905494\n",
      "score gained on train: -0.00223188447002\n",
      "Difference in inlinks coefficient score gained: -0.00541010373427\n",
      "score gained on train: -0.00530656425857\n",
      "Number of times to cited score gained: -0.00334679089868\n",
      "score gained on train: -0.00321277637086\n",
      "Same cluster score gained: -0.0040372696035\n",
      "score gained on train: -0.00793649181879\n",
      "CosineD_title_centroid score gained: -0.00129160134197\n",
      "score gained on train: -0.000840764486433\n",
      "CosineD_abstract_centroid score gained: -0.00282690105034\n",
      "score gained on train: -0.00246746099279\n",
      "CosineD_abstract_bag_centroids score gained: -0.00790395035052\n",
      "score gained on train: -0.0069698157426\n",
      "Diff publication score gained: -0.00422410501775\n",
      "score gained on train: -0.00497147696326\n",
      "Number same authors score gained: -0.00311933909003\n",
      "score gained on train: -0.00238216604489\n",
      "Self citation score gained: -0.0028837640025\n",
      "score gained on train: -0.00202473959655\n",
      "Same journal score gained: -0.00430533780655\n",
      "score gained on train: -0.00307671062064\n",
      "Authors betweeness score gained: -0.0039966532091\n",
      "score gained on train: -0.0027192841723\n",
      "Authors common neighbors score gained: -0.00518265192562\n",
      "score gained on train: -0.00437644316006\n",
      "Authors jaccard score gained: -0.00183586102694\n",
      "score gained on train: -0.00200036961144\n",
      "Authors max difference in inlinks score gained: -0.00446780338416\n",
      "score gained on train: -0.00310311143785\n",
      "Authors sum difference in inlinks score gained: -0.00209580595111\n",
      "score gained on train: -0.0014723532673\n",
      "Authors max of times to cited score gained: -0.000268068203049\n",
      "score gained on train: -8.5294947899e-05\n",
      "Authors sum of times to cited score gained: -0.0023882439908\n",
      "score gained on train: -0.00160841901752\n",
      "Authors  of times to cited score gained: -0.00251009317401\n",
      "score gained on train: -0.00176479308867\n",
      "CPU times: user 7min 51s, sys: 1.7 s, total: 7min 53s\n",
      "Wall time: 7min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C=1000,max_iter = 10000)\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "base_score = accuracy_score(pred, y_test)\n",
    "base_train_score = accuracy_score(clf.predict(X_train),y_train)\n",
    "print base_score, base_train_score\n",
    "for i in X_train.columns:\n",
    "    clf = LogisticRegression(C=1000,max_iter = 10000)\n",
    "    clf.fit(X_train.drop([i], axis = 1), y_train)\n",
    "    pred = clf.predict(X_test.drop([i], axis = 1))\n",
    "    print i,\"score gained:\",accuracy_score(pred, y_test)- base_score\n",
    "    print \"score gained on train:\",accuracy_score(clf.predict(X_train.drop([i], axis = 1)),y_train) - base_train_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.959602934128 1.0\n",
      "0 score gained: 0.000203081972007\n",
      "1 score gained: 0.000219328529768\n",
      "Betweeness centrality score gained: 0.000186835414247\n",
      "Number common neighbours score gained: -9.74793465635e-05\n",
      "Jaccard coefficienf score gained: 0.00028431476081\n",
      "Difference in inlinks coefficient score gained: 0.000414287222895\n",
      "Number of times to cited score gained: -0.00472774830833\n",
      "Same cluster score gained: 0.00249384661625\n",
      "CosineD_title_centroid score gained: 0.000178712135366\n",
      "CosineD_abstract_centroid score gained: -0.000194958693127\n",
      "CosineD_abstract_bag_centroids score gained: 0.00225827152872\n",
      "Diff publication score gained: 0.00286751744474\n",
      "Number same authors score gained: 0.000129972462085\n",
      "Self citation score gained: 5.6862952162e-05\n",
      "Same journal score gained: 0.000162465577606\n",
      "Authors betweeness score gained: 1.62465577606e-05\n",
      "Authors common neighbors score gained: 3.24931155211e-05\n",
      "Authors jaccard score gained: 0.00056050624274\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-444cd883e5c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu'from sklearn.ensemble import RandomForestClassifier\\nclf = RandomForestClassifier(n_estimators = 200,n_jobs=3)\\nclf.fit(X_train, y_train)\\npred = clf.predict(X_test)\\nbase_score = accuracy_score(pred, y_test)\\nbase_train_score = accuracy_score(clf.predict(X_train),y_train)\\nprint base_score, base_train_score\\nfor i in X_train.columns:\\n    clf = RandomForestClassifier(n_estimators = 200,n_jobs=3)\\n    clf.fit(X_train.drop([i], axis = 1), y_train)\\n    pred = clf.predict(X_test.drop([i], axis = 1))\\n    print i,\"score gained:\",-(accuracy_score(pred, y_test)- base_score)\\n    #print \"score gained on train:\",accuracy_score(clf.predict(X_train.drop([i], axis = 1)),y_train) - base_train_score'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/bat/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2291\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2292\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2293\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2294\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bat/anaconda2/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32m/home/bat/anaconda2/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bat/anaconda2/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1165\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1166\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1167\u001b[1;33m             \u001b[1;32mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1168\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1169\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m/home/bat/anaconda2/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    288\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[1;32m--> 290\u001b[1;33m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[0;32m    291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m             \u001b[1;31m# Collect newly grown trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bat/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    810\u001b[0m                 \u001b[1;31m# consumption.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 812\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    813\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    814\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bat/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    729\u001b[0m                 \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 731\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    732\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    733\u001b[0m                 \u001b[1;31m# Stop dispatching any new job in the async callback thread\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bat/anaconda2/lib/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ready\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bat/anaconda2/lib/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ready\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 556\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    557\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bat/anaconda2/lib/python2.7/threading.pyc\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_note\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s.wait(): got it\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators = 200,n_jobs=3)\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "base_score = accuracy_score(pred, y_test)\n",
    "base_train_score = accuracy_score(clf.predict(X_train),y_train)\n",
    "print base_score, base_`train_score\n",
    "for i in X_train.columns:\n",
    "    clf = RandomForestClassifier(n_estimators = 200,n_jobs=3)\n",
    "    clf.fit(X_train.drop([i], axis = 1), y_train)\n",
    "    pred = clf.predict(X_test.drop([i], axis = 1))\n",
    "    print i,\"score gained:\",-(accuracy_score(pred, y_test)- base_score)\n",
    "    #print \"score gained on train:\",accuracy_score(clf.predict(X_train.drop([i], axis = 1)),y_train) - base_train_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Betweeness centrality score 0.958376319017\n",
      "0.999987815007\n",
      "Number common neighbours score 0.958920578702\n",
      "0.999977660847\n",
      "Jaccard coefficienf score 0.95864438722\n",
      "0.999991876672\n",
      "Difference in inlinks coefficient score 0.958701250173\n",
      "0.999985784175\n",
      "Number of times to cited score 0.962730396497\n",
      "0.99998984584\n",
      "Same cluster score 0.954631487454\n",
      "0.999991876672\n",
      "CosineD_title_centroid score 0.958628140663\n",
      "0.999985784175\n",
      "CosineD_abstract_centroid score 0.958774359683\n",
      "0.99998984584\n",
      "CosineD_abstract_bag_centroids score 0.955622527477\n",
      "0.99998984584\n",
      "Diff publication score 0.953965378585\n",
      "0.999985784175\n",
      "Number same authors score 0.958481921643\n",
      "0.999987815007\n",
      "Self citation score 0.958514414758\n",
      "0.999995938336\n",
      "Same journal score 0.959034304607\n",
      "0.999983753343\n",
      "Authors betweeness score 0.959123660674\n",
      "0.999991876672\n",
      "Authors common neighbors score 0.958839345914\n",
      "0.999993907504\n",
      "Authors jaccard score 0.958571277711\n",
      "0.99998984584\n",
      "Authors max difference in inlinks score 0.959034304607\n",
      "0.999991876672\n",
      "Authors sum difference in inlinks score 0.958701250173\n",
      "0.999991876672\n",
      "Authors max of times to cited score 0.958563154432\n",
      "0.999995938336\n",
      "Authors sum of times to cited score 0.958733743288\n",
      "0.999987815007\n",
      "Authors  of times to cited score 0.958839345914\n",
      "0.999995938336\n",
      "CPU times: user 1h 31min 39s, sys: 12.8 s, total: 1h 31min 52s\n",
      "Wall time: 31min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "for i in X_train.columns:\n",
    "    rfc = RandomForestClassifier(n_estimators = 100,n_jobs=3)\n",
    "    rfc.fit(X_train.drop([i], axis = 1), y_train)\n",
    "    pred = rfc.predict(X_test.drop([i], axis = 1))\n",
    "    print i,\"score\",accuracy_score(pred, y_test)\n",
    "    print accuracy_score(rfc.predict(X_train.drop([i], axis = 1)),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.964241326369 0.970008671653\n",
      "0 score gained: -0.00027619148193\n",
      "1 score gained: 0.000186835414247\n",
      "Betweeness centrality score gained: -4.06163944014e-05\n",
      "Number common neighbours score gained: 0.000389917386254\n",
      "Jaccard coefficienf score gained: -0.000178712135366\n",
      "Difference in inlinks coefficient score gained: 8.12327888028e-05\n",
      "Number of times to cited score gained: -0.000316807876331\n",
      "Same cluster score gained: 0.00263194235721\n",
      "CosineD_title_centroid score gained: -7.31095099227e-05\n",
      "CosineD_abstract_centroid score gained: 0.000170588856486\n",
      "CosineD_abstract_bag_centroids score gained: 0.00318432532107\n",
      "Diff publication score gained: 0.000300561318571\n",
      "Number same authors score gained: -0.000316807876331\n",
      "Self citation score gained: 5.6862952162e-05\n",
      "Same journal score gained: -0.000300561318571\n",
      "Authors betweeness score gained: 0.000146219019845\n",
      "Authors common neighbors score gained: -0.000211205250887\n",
      "Authors jaccard score gained: 0.000536136406099\n",
      "Authors max difference in inlinks score gained: -8.12327888029e-05\n",
      "Authors sum difference in inlinks score gained: -8.12327888029e-05\n",
      "Authors max of times to cited score gained: -8.12327888033e-06\n",
      "Authors sum of times to cited score gained: -4.06163944014e-05\n",
      "Authors  of times to cited score gained: -0.000316807876331\n",
      "CPU times: user 2h 37min 18s, sys: 2min 34s, total: 2h 39min 53s\n",
      "Wall time: 1h 20min 27s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time \n",
    "scaler1=StandardScaler()\n",
    "scaler = MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "\n",
    "nn = MLP(\n",
    "layers=[\n",
    "    Layer(\"Rectifier\", units=100),\n",
    "    Layer(\"Rectifier\", units=100),\n",
    "        Layer(\"Softmax\")],\n",
    "        learning_rule = 'momentum', learning_rate=0.02, batch_size = 100,dropout_rate =0.1,\n",
    "        n_iter=100,\n",
    "        verbose = 1, \n",
    "        valid_size = 0.1, \n",
    "        n_stable = 15,\n",
    "        debug = True,\n",
    "        #    regularize = 'L2'\n",
    "        random_state = 42\n",
    ")\n",
    "\n",
    "#nn = MLP(layers=[\n",
    "#        Layer(\"Rectifier\", units=100, pieces=2),\n",
    "#        Layer(\"Softmax\")],\n",
    "#    learning_rate=0.001,    n_iter=25)\n",
    "\n",
    "\n",
    "clf = Pipeline([\n",
    "    (\"scaler\", scaler1),\n",
    "    ('neural network', nn)\n",
    "    ])\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "base_score = accuracy_score(pred, y_test)\n",
    "base_train_score = accuracy_score(clf.predict(X_train),y_train)\n",
    "print base_score, base_train_score\n",
    "for i in X_train.columns:\n",
    "    scaler1=StandardScaler()\n",
    "    scaler = MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "\n",
    "    nn = MLP(\n",
    "    layers=[\n",
    "        Layer(\"Rectifier\", units=100),\n",
    "        Layer(\"Rectifier\", units=100),\n",
    "        Layer(\"Softmax\")],\n",
    "        learning_rule = 'momentum', learning_rate=0.02, batch_size = 100,dropout_rate =0.1,\n",
    "        n_iter=100,\n",
    "        verbose = 1, \n",
    "        valid_size = 0.1, \n",
    "        n_stable = 15,\n",
    "        debug = True,\n",
    "        #    regularize = 'L2'\n",
    "        random_state = 42\n",
    "    )\n",
    "\n",
    "    clf = Pipeline([\n",
    "    (\"scaler\", scaler1),\n",
    "    ('neural network', nn)\n",
    "    ])\n",
    "    clf.fit(X_train.drop([i], axis = 1), y_train)\n",
    "    pred = clf.predict(X_test.drop([i], axis = 1))\n",
    "    print i,\"score gained:\",-(accuracy_score(pred, y_test)- base_score)\n",
    "    #print \"score gained on train:\",accuracy_score(clf.predict(X_train.drop([i], axis = 1)),y_train) - base_train_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_light2 = X_train.drop([\"0\",\"1\",\"Number common neighbours\",\"Self citation\",\"Same journal\",\"Authors max difference in inlinks\",\n",
    "                                   \"Number of times to cited\",\"Authors  of times to cited\"], axis = 1)\n",
    "X_test_light2 = X_test.drop([\"0\",\"1\",\"Number common neighbours\",\"Self citation\",\"Same journal\",\"Authors max difference in inlinks\",\n",
    "                             \"Number of times to cited\",\"Authors  of times to cited\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_light = X_train.drop([\"0\",\"1\",\"Number of times to cited\",\"Authors  of times to cited\"], axis = 1)\n",
    "X_test_light = X_test.drop([\"0\",\"1\",\"Number of times to cited\",\"Authors  of times to cited\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authors  of times to cited score gained: 0.000828574445789\n",
      "0.963412751923\n"
     ]
    }
   ],
   "source": [
    "scaler1=StandardScaler()\n",
    "scaler = MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "\n",
    "nn = MLP(\n",
    "layers=[\n",
    "    Layer(\"Rectifier\", units=100),\n",
    "    Layer(\"Rectifier\", units=100),\n",
    "    Layer(\"Softmax\")],\n",
    "    learning_rule = 'momentum', learning_rate=0.02, batch_size = 100,dropout_rate =0.1,\n",
    "    n_iter=100,\n",
    "    verbose = 1, \n",
    "    valid_size = 0.1, \n",
    "    n_stable = 15,\n",
    "    debug = True,\n",
    "    #    regularize = 'L2'\n",
    "    random_state = 42\n",
    "    )\n",
    "\n",
    "clfnn = Pipeline([\n",
    "(\"scaler\", scaler1),\n",
    "('neural network', nn)\n",
    "])\n",
    "clfnn.fit(X_train_light2, y_train)\n",
    "pred = clfnn.predict(X_test_light2)\n",
    "print i,\"score gained:\",-(accuracy_score(pred, y_test)- base_score)\n",
    "print accuracy_score(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 score gained: 0.000536136406099\n",
      "0.963705189963\n"
     ]
    }
   ],
   "source": [
    "scaler1=StandardScaler()\n",
    "scaler = MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "\n",
    "nn = MLP(\n",
    "layers=[\n",
    "    Layer(\"Rectifier\", units=100),\n",
    "    Layer(\"Rectifier\", units=100),\n",
    "    Layer(\"Softmax\")],\n",
    "    learning_rule = 'momentum', learning_rate=0.05, batch_size = 100,dropout_rate =0.1,\n",
    "    n_iter=100,\n",
    "    verbose = 1, \n",
    "    valid_size = 0.1, \n",
    "    n_stable = 20,\n",
    "    debug = True,\n",
    "    #    regularize = 'L2'\n",
    "    random_state = 42\n",
    "    )\n",
    "\n",
    "clfnn = Pipeline([\n",
    "(\"scaler\", scaler1),\n",
    "('neural network', nn)\n",
    "])\n",
    "clfnn.fit(X_train_light, y_train)\n",
    "pred = clfnn.predict(X_test_light)\n",
    "print i,\"score gained:\",-(accuracy_score(pred, y_test)- base_score)\n",
    "print accuracy_score(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96351023127\n",
      "0.969033872248\n"
     ]
    }
   ],
   "source": [
    "nn2 = MLP(\n",
    "layers=[\n",
    "    Layer(\"Rectifier\", units=100),\n",
    "    Layer(\"Rectifier\", units=100),\n",
    "           Layer(\"Softmax\")],\n",
    "        learning_rule = 'momentum', learning_rate=0.05, batch_size = 100,dropout_rate =0.1,\n",
    "        n_iter=100,\n",
    "        verbose = 1, \n",
    "        valid_size = 0.1, \n",
    "        n_stable = 25,\n",
    "        debug = True,\n",
    "        #    regularize = 'L2'\n",
    ")\n",
    "\n",
    "clfnn2 = Pipeline([\n",
    "    (\"scaler\", scaler1),\n",
    "    ('neural network', nn2)\n",
    "    ])\n",
    "\n",
    "clfnn2.fit(X_train_light.values, y_train.values)\n",
    "pred = clfnn2.predict(X_test_light)\n",
    "print accuracy_score(pred, y_test)\n",
    "print accuracy_score(clfnn2.predict(X_train_light),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.960472124969\n",
      "0.986044121858\n",
      "CPU times: user 1h 49min 25s, sys: 11.7 s, total: 1h 49min 37s\n",
      "Wall time: 1h 49min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    " \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf =  GradientBoostingClassifier(n_estimators = 800, max_depth= 7)\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "print accuracy_score(pred, y_test)\n",
    "print accuracy_score(clf.predict(X_train),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.963721436521\n",
      "0.983702572455\n",
      "CPU times: user 1h 24min, sys: 9.34 s, total: 1h 24min 10s\n",
      "Wall time: 1h 24min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    " \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf =  GradientBoostingClassifier(n_estimators = 800, max_depth= 7)\n",
    "clf.fit(X_train_light, y_train)\n",
    "pred = clf.predict(X_test_light)\n",
    "print accuracy_score(pred, y_test)\n",
    "print accuracy_score(clf.predict(X_train_light),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.963989504724\n",
      "0.983379670152\n",
      "CPU times: user 1h 22min 57s, sys: 5.71 s, total: 1h 23min 3s\n",
      "Wall time: 1h 23min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    " \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf =  GradientBoostingClassifier(n_estimators = 800, max_depth= 7)\n",
    "clf.fit(X_train_light2, y_train)\n",
    "pred = clf.predict(X_test_light2)\n",
    "print accuracy_score(pred, y_test)\n",
    "print accuracy_score(clf.predict(X_train_light2),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.964030121118\n",
      "0.977181570605\n",
      "CPU times: user 7.46 s, sys: 4.05 ms, total: 7.47 s\n",
      "Wall time: 7.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    " \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "#gbc400 =  GradientBoostingClassifier(n_estimators = 400, max_depth= 7)\n",
    "#gbc400.fit(X_train_light2, y_train)\n",
    "pred = gbc400.predict(X_test_light2)\n",
    "print accuracy_score(pred, y_test)\n",
    "print accuracy_score(gbc400.predict(X_train_light2),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.963039081095\n",
      "CPU times: user 2min 7s, sys: 456 ms, total: 2min 7s\n",
      "Wall time: 2min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "clf = ExtraTreesClassifier(n_estimators = 200,n_jobs=1)\n",
    "clf.fit(X_train_light, y_train)\n",
    "pred = clf.predict(X_test_light)\n",
    "print accuracy_score(pred, y_test)\n",
    "#print score(clf.predict(X_train.drop([0,1], axis = 1)),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.958156990488\n",
      "0.999983753343\n",
      "CPU times: user 4min 9s, sys: 486 ms, total: 4min 10s\n",
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators = 100,n_jobs=3)\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "print accuracy_score(pred, y_test)\n",
    "print accuracy_score(clf.predict(X_train),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.963095944047\n",
      "CPU times: user 19min 21s, sys: 3.21 s, total: 19min 24s\n",
      "Wall time: 7min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators = 400,n_jobs=3)\n",
    "rfc.fit(X_train_light, y_train)\n",
    "pred = rfc.predict(X_test_light)\n",
    "print accuracy_score(pred, y_test)\n",
    "#print accuracy_score(clf.predict(X_train_light),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.961 (+/-0.002) for {'max_depth': 5}\n",
      "0.967 (+/-0.001) for {'max_depth': 10}\n",
      "0.968 (+/-0.001) for {'max_depth': 15}\n",
      "0.968 (+/-0.001) for {'max_depth': 20}\n",
      "0.968 (+/-0.001) for {'max_depth': 50}\n",
      "0.959091167559\n",
      "0.99998984584\n",
      "CPU times: user 1h 8min 23s, sys: 15.4 s, total: 1h 8min 38s\n",
      "Wall time: 23min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "depths = [5, 10, 15, 20, 50]\n",
    " \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "for depth in depths:\n",
    "    clf = RandomForestClassifier(n_estimators = 100,n_jobs=3,max_depth = depth)\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "    print accuracy_score(pred, y_test)\n",
    "    print accuracy_score(clf.predict(X_train),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth 3 : 0.962324232553\n",
      "0.967216277525\n",
      "depth 5 : 0.963388382087\n",
      "0.968934361476\n",
      "depth 7 : 0.96394076505\n",
      "0.971420099958\n",
      "depth 10 : 0.963518354549\n",
      "0.979472349206\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-0aa9e1afc8d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu'depths = [3,5,7, 10, 15]\\n \\nfrom sklearn.ensemble import GradientBoostingClassifier\\nfor depth in depths:\\n    clf =  GradientBoostingClassifier(n_estimators = 100, max_depth= depth)\\n    clf.fit(X_train_light, y_train)\\n    pred = clf.predict(X_test_light)\\n    print \"depth\",depth,\":\",accuracy_score(pred, y_test)\\n    print accuracy_score(clf.predict(X_train_light),y_train)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/bat/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2291\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2292\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2293\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2294\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bat/anaconda2/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32m/home/bat/anaconda2/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bat/anaconda2/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1165\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1166\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1167\u001b[1;33m             \u001b[1;32mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1168\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1169\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m/home/bat/anaconda2/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[1;31m# fit the boosting stages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m         n_stages = self._fit_stages(X, y, y_pred, sample_weight, random_state,\n\u001b[1;32m-> 1025\u001b[1;33m                                     begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[0;32m   1026\u001b[0m         \u001b[1;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_stages\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bat/anaconda2/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36m_fit_stages\u001b[1;34m(self, X, y, y_pred, sample_weight, random_state, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1078\u001b[0m             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n\u001b[0;32m   1079\u001b[0m                                      \u001b[0msample_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1080\u001b[1;33m                                      X_csc, X_csr)\n\u001b[0m\u001b[0;32m   1081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1082\u001b[0m             \u001b[1;31m# track deviance (= loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bat/anaconda2/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36m_fit_stage\u001b[1;34m(self, i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    782\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m                 tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[1;32m--> 784\u001b[1;33m                          check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    785\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m             \u001b[1;31m# update tree leaves\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bat/anaconda2/lib/python2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    348\u001b[0m                                            max_leaf_nodes)\n\u001b[0;32m    349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time \n",
    "depths = [3,5,7, 10]\n",
    " \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "for depth in depths:\n",
    "    clf =  GradientBoostingClassifier(n_estimators = 100, max_depth= depth)\n",
    "    clf.fit(X_train_light, y_train)\n",
    "    pred = clf.predict(X_test_light)\n",
    "    print \"depth\",depth,\":\",accuracy_score(pred, y_test)\n",
    "    print accuracy_score(clf.predict(X_train_light),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth 6 : 0.963924518493\n",
      "0.970033041638\n",
      "depth 8 : 0.963916395214\n",
      "0.973329082125\n",
      "depth 9 : 0.963932641772\n",
      "0.976093044603\n",
      "CPU times: user 2h 8min 53s, sys: 30.7 s, total: 2h 9min 24s\n",
      "Wall time: 2h 10min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "depths = [6,8,9]\n",
    " \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "for depth in depths:\n",
    "    clf =  GradientBoostingClassifier(n_estimators = 100, max_depth= depth)\n",
    "    clf.fit(X_train_light, y_train)\n",
    "    pred = clf.predict(X_test_light)\n",
    "    print \"depth\",depth,\":\",accuracy_score(pred, y_test)\n",
    "    print accuracy_score(clf.predict(X_train_light),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95320991365\n",
      "CPU times: user 47.4 s, sys: 72.3 ms, total: 47.5 s\n",
      "Wall time: 47.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg =  Pipeline([\n",
    "    (\"scaler\", MinMaxScaler()),\n",
    "    (\"regression\",LogisticRegression(C=200000,max_iter = 1000))\n",
    "    ])\n",
    "logreg.fit(X_train, y_train)\n",
    "pred = logreg.predict(X_test)\n",
    "print accuracy_score(pred, y_test)\n",
    "#print accuracy_score(logreg.predict(X_train_light),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.962535437804\n",
      "0.968008302042\n",
      "CPU times: user 4.01 s, sys: 16 ms, total: 4.02 s\n",
      "Wall time: 4.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier(n_estimators = 200)\n",
    "gbc.fit(X_train_light, y_train)\n",
    "pred = gbc.predict(X_test_light)\n",
    "print score(pred, y_test)\n",
    "print score(gbc.predict(X_train_light),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bat/anaconda2/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:5: UserWarning: downsample module has been moved to the pool module.\n",
      "  warnings.warn(\"downsample module has been moved to the pool module.\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of features of the model must  match the input. Model n_features is 20 and  input n_features is 21 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-e75c4a12a852>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mnnclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/bat/anaconda2/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    496\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \"\"\"\n\u001b[1;32m--> 498\u001b[1;33m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bat/anaconda2/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    535\u001b[0m         \"\"\"\n\u001b[0;32m    536\u001b[0m         \u001b[1;31m# Check data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 537\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m         \u001b[1;31m# Assign chunk of trees to jobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bat/anaconda2/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    317\u001b[0m                                  \"call `fit` before exploiting the model.\")\n\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bat/anaconda2/lib/python2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    374\u001b[0m                              \u001b[1;34m\" match the input. Model n_features is %s and \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m                              \u001b[1;34m\" input n_features is %s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m                              % (self.n_features_, n_features))\n\u001b[0m\u001b[0;32m    377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Number of features of the model must  match the input. Model n_features is 20 and  input n_features is 21 "
     ]
    }
   ],
   "source": [
    "scaler1=StandardScaler()\n",
    "scaler = MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "\n",
    "nn = MLP(\n",
    "layers=[\n",
    "    Layer(\"Rectifier\", units=100),\n",
    "    Layer(\"Rectifier\", units=100),\n",
    "        Layer(\"Softmax\")],\n",
    "        learning_rule = 'momentum', learning_rate=0.02, batch_size = 100,dropout_rate =0.1,\n",
    "        n_iter=100,\n",
    "        verbose = 1, \n",
    "        valid_size = 0.1, \n",
    "        n_stable = 30,\n",
    "        debug = True,\n",
    "        #    regularize = 'L2'\n",
    ")\n",
    "\n",
    "#nn = MLP(layers=[\n",
    "#        Layer(\"Rectifier\", units=100, pieces=2),\n",
    "#        Layer(\"Softmax\")],\n",
    "#    learning_rate=0.001,    n_iter=25)\n",
    "\n",
    "nnclf = Pipeline([\n",
    "    (\"scaler\", scaler1),\n",
    "    ('neural network', nn)\n",
    "    ])\n",
    "\n",
    "nnclf.fit(X_train.values, y_train.values)\n",
    "#pred = clf.predict(X_test.values).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.99212589,  0.02555739,  0.17214367, ...,  0.99767432,\n",
       "        0.99681896,  0.00357826])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc.predict_proba(X_test_light)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('neural network', Classifier(batch_size=100, callback=None, debug=True, dropout_rate=0.1,\n",
       "      f_stable=0.001,\n",
       "      hidden0=<sknn.nn.Layer `Rectifier`: units=100, name=u'hidden0', frozen=False>,\n",
       "      hidden1=<sknn.nn.La...  [1, 0]])),\n",
       "      valid_size=0.1, verbose=1, warning=None, weight_decay=None,\n",
       "      weights=None))])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnclf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_array = [gbc.predict_proba(X_test_light)[:,1],rfc.predict_proba(X_test_light)[:,1], logreg.predict_proba(X_test)[:,1],nnclf.predict_proba(X_test)[:,1].flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_array = [gbc400.predict_proba(X_test_light2)[:,1],\n",
    "                nnclf.predict_proba(X_test)[:,1].flatten(),clfnn2.predict_proba(X_test_light)[:,1].flatten(),clf.predict_proba(X_test_light)[:,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rounds(predict):\n",
    "    return [0 if pr<=0.5 else 1 for pr in predict]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96403012111808806, 0.96405449095472895, 0.96351023126974977, 0.96303908109469305]\n",
      "step: 0 validation error: 0.964054490955\n",
      "[0.9652486129501312, 0.96405449095472895, 0.96468810670739136, 0.96443628506210244]\n",
      "step: 1 validation error: 0.96524861295\n",
      "[0.96479370933283515, 0.96518362671908886, 0.9651673801613283, 0.96542732508549756]\n",
      "step: 2 validation error: 0.965427325085\n",
      "[0.96507802409364518, 0.96528110606565232, 0.96497242146820139, 0.96506990081476485]\n",
      "step: 3 validation error: 0.965281106066\n",
      "[0.96539483196997633, 0.96495617491044083, 0.96515113360356775, 0.96505365425700429]\n",
      "step: 4 validation error: 0.96539483197\n",
      "[0.96511864048804663, 0.96537046213333555, 0.96519987327684942, 0.96542732508549756]\n",
      "step: 5 validation error: 0.965427325085\n",
      "[0.96553292771094124, 0.96528922934453265, 0.96525673622901143, 0.96530547590229321]\n",
      "step: 6 validation error: 0.965532927711\n",
      "[0.96507802409364518, 0.96549231131653979, 0.96525673622901143, 0.96548418803765956]\n",
      "step: 7 validation error: 0.965492311317\n",
      "[0.96543544836437778, 0.96537858541221577, 0.96525673622901143, 0.96542732508549756]\n",
      "step: 8 validation error: 0.965435448364\n",
      "[0.96510239393028607, 0.96539483196997633, 0.96532172246005377, 0.96546794147989889]\n",
      "step: 9 validation error: 0.96546794148\n",
      "[4, 3, 0, 3]\n",
      "0.96546794148\n"
     ]
    }
   ],
   "source": [
    "max_iter = 10\n",
    "\n",
    "weights = [0 for algo in y_pred_array]\n",
    "prediction = np.array([0. for i in y_pred_array[0]])\n",
    "for i in range(max_iter):\n",
    "    possible_predictions = [(i*prediction + pred)/(i+1) for pred in y_pred_array]\n",
    "    #print possible_predictions[0].shape, validation[1].shape\n",
    "    #print possible_predictions \n",
    "    scores = [accuracy_score(y_test,rounds(pred)) for pred in possible_predictions ]\n",
    "    print scores\n",
    "    best = max(enumerate(scores), key = lambda p: p[1])\n",
    "    weights[best[0]]+=1\n",
    "    prediction = possible_predictions[best[0]]\n",
    "    print \"step:\", i, \"validation error:\", best[1]\n",
    "       \n",
    "print weights\n",
    "score = accuracy_score(rounds(1./sum(weights)*sum([weights[i]*y for i,y in enumerate(y_pred_array)])),y_test)\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98940724434010541"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred_array[3],y_pred_array[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################################\n",
    "scaler1=StandardScaler()\n",
    "scaler = MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "val_percent = 0.1\n",
    "nn = MLP(\n",
    "layers=[\n",
    "    Layer(\"Rectifier\", units=200),\n",
    "        Layer(\"Softmax\")],\n",
    "        learning_rule = 'momentum', learning_rate=0.01, batch_size = 100,dropout_rate =0.,\n",
    "        n_iter=20,\n",
    "        verbose = 1, \n",
    "        valid_size = val_percent, \n",
    "        n_stable = 30,\n",
    "        debug = True,\n",
    "        #    regularize = 'L2'\n",
    ")\n",
    "\n",
    "#nn = MLP(layers=[\n",
    "#        Layer(\"Rectifier\", units=100, pieces=2),\n",
    "#        Layer(\"Softmax\")],\n",
    "#    learning_rate=0.001,    n_iter=25)\n",
    "\n",
    "clf = Pipeline([\n",
    "    (\"scaler\", scaler1),\n",
    "    ('neural network', nn)\n",
    "    ])\n",
    "\n",
    "clf.fit(X_train.values, y_train.values)\n",
    "pred = clf.predict(X_test.values).flatten()\n",
    "if(validation):\n",
    "    print \"hold out validation:\",accuracy_score(pred, y_test)\n",
    "if val_percent>0:\n",
    "    valid = clf.named_steps[\"neural network\"].valid_set\n",
    "    print \"nn validation:\",accuracy_score(clf.named_steps[\"neural network\"].predict(valid[0]),valid[1][:,1])\n",
    "\n",
    "#print accuracy_score(clf.predict(X_train),y_train)\n",
    "\n",
    "def make_submission(predicted_label, name = 'submit.csv'):\n",
    "    submit_d = d = {'id' : pd.Series(np.arange(predicted_label.shape[0]).astype(int)),\n",
    "                    'category' : pd.Series(predicted_label).astype(int)}\n",
    "    submit = pd.DataFrame(submit_d, columns=[\"id\",\"category\"])\n",
    "    submit.to_csv(name,index=False)\n",
    "    return submit\n",
    "\n",
    "\n",
    "if(not(validation)):\n",
    "    submit = make_submission(pred,name = \"submit_nn.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
